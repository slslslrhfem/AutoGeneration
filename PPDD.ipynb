{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPDD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1vCXwudXUZpUmzLdlfBoJ0LU7XlCbKpgG",
      "authorship_tag": "ABX9TyN4rTU1JmLthSr3XRDieQUs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slslslrhfem/AutoGeneration/blob/master/PPDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxEabHuzyjFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "15fa809a-b9dd-4fbd-bf55-c9a7176ef2f1"
      },
      "source": [
        "pip install mido"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mido\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/0a/81beb587b1ae832ea6a1901dc7c6faa380e8dd154e0a862f0a9f3d2afab9/mido-1.2.9-py2.py3-none-any.whl (52kB)\n",
            "\r\u001b[K     |██████▎                         | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: mido\n",
            "Successfully installed mido-1.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ3x-FF_4XGK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "033722bf-7994-4797-86c7-25d800304302"
      },
      "source": [
        "pip install pretty_midi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pretty_midi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/8e/63c6e39a7a64623a9cd6aec530070c70827f6f8f40deec938f323d7b1e15/pretty_midi-0.2.9.tar.gz (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.18.5)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.15.0)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-cp36-none-any.whl size=5591953 sha256=5532dfdd01ed24f55fc454252a274a9f5e959dbc669706f94a06006b7e9aa769\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/a1/c6/b5697841db1112c6e5866d75a6b6bf1bef73b874782556ba66\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: pretty-midi\n",
            "Successfully installed pretty-midi-0.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kljETrpXyFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "outputId": "88c3331f-8a45-4f48-e6fe-0c267f846922"
      },
      "source": [
        "pip install tensorflow==1.15.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 37kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.31.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 73.9MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 56.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.34.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.18.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (49.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=b98ca3a0255d01e4c5c488a46f3cf770fdadfdb6a522d47c01d41b63d981c00b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, tensorboard, tensorflow-estimator, gast, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP5JOsojTLIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "1f9a0392-693d-4508-abae-22a89beeb489"
      },
      "source": [
        "pip install keras==2.3.1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\r\u001b[K     |▉                               | 10kB 21.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upkZgrqruxem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b39e905-210f-4601-99b0-8bc1091596bf"
      },
      "source": [
        "import keras\n",
        "import os\n",
        "import json\n",
        "import pandas\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.layers import Dense\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "import pickle\n",
        "import pretty_midi\n",
        "import mido\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import copy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt3ElGb47kOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4c142e37-646c-477a-f3e5-166cc6f46728"
      },
      "source": [
        "\"\"\"\n",
        "midifilenames=sorted(midifilenames)\n",
        "csvfilenames=sorted(csvfilenames)\n",
        "jsonfilenames=sorted(jsonfilenames)\n",
        "\"\"\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmidifilenames=sorted(midifilenames)\\ncsvfilenames=sorted(csvfilenames)\\njsonfilenames=sorted(jsonfilenames)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOjmQZTO0TtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_meta(filename):\n",
        "  with open('/content/drive/My Drive/MARG/PPDD-Sep2018_sym_mono_large/PPDD-Sep2018_sym_mono_large/descriptor/'+filename) as json_file:\n",
        "    meta_data = json.load(json_file)\n",
        "  return meta_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdprhNM2Y2qy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b832b60c-ef9b-4d9f-ee10-6157bfa2cd08"
      },
      "source": [
        "\"\"\"\n",
        "csvfilenames=os.listdir('/content/drive/My Drive/MARG/PPDD-Sep2018_sym_mono_large/PPDD-Sep2018_sym_mono_large/prime_csv')\n",
        "jsonfilenames=os.listdir('/content/drive/My Drive/MARG/PPDD-Sep2018_sym_mono_large/PPDD-Sep2018_sym_mono_large/descriptor')\n",
        "midifilenames=os.listdir('/content/drive/My Drive/MARG/PPDD-Sep2018_sym_mono_large/PPDD-Sep2018_sym_mono_large/prime_midi')\n",
        "\"\"\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ncsvfilenames=os.listdir('/content/drive/My Drive/MARG/PPDD-Sep2018_sym_mono_large/PPDD-Sep2018_sym_mono_large/prime_csv')\\njsonfilenames=os.listdir('/content/drive/My Drive/MARG/PPDD-Sep2018_sym_mono_large/PPDD-Sep2018_sym_mono_large/descriptor')\\nmidifilenames=os.listdir('/content/drive/My Drive/MARG/PPDD-Sep2018_sym_mono_large/PPDD-Sep2018_sym_mono_large/prime_midi')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lpgz0bAYHsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "16958c3a-62c8-4a40-e0dd-c38d1d28308c"
      },
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "with open('/content/drive/My Drive/MARG/PPDDlist/csvnamelist.txt', 'wb') as f:\n",
        "  pickle.dump(csvfilenames, f)\n",
        "with open('/content/drive/My Drive/MARG/PPDDlist/jsonnamelist.txt', 'wb') as f:\n",
        "  pickle.dump(jsonfilenames, f)\n",
        "with open('/content/drive/My Drive/MARG/PPDDlist/midinamelist.txt', 'wb') as f:\n",
        "  pickle.dump(midifilenames, f)\n",
        "\"\"\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nwith open('/content/drive/My Drive/MARG/PPDDlist/csvnamelist.txt', 'wb') as f:\\n  pickle.dump(csvfilenames, f)\\nwith open('/content/drive/My Drive/MARG/PPDDlist/jsonnamelist.txt', 'wb') as f:\\n  pickle.dump(jsonfilenames, f)\\nwith open('/content/drive/My Drive/MARG/PPDDlist/midinamelist.txt', 'wb') as f:\\n  pickle.dump(midifilenames, f)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoAxt13seNi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with open('/content/drive/My Drive/MARG/PPDDlist/midilist.txt', 'rb') as f:\n",
        "  midilist=pickle.load(f)\n",
        "with open('/content/drive/My Drive/MARG/PPDDlist/csvlist.txt', 'rb') as f2:\n",
        "  csvlist=pickle.load(f2)\n",
        "with open('/content/drive/My Drive/MARG/PPDDlist/jsonlist.txt', 'rb') as f3:\n",
        "  jsonlist=pickle.load(f3)\n",
        "with open('/content/drive/My Drive/MARG/PPDDlist/prettymidilist.txt', 'rb') as f4:\n",
        "  prettymidilist=pickle.load(f4)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc57uuAq2gx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "958f3431-d341-4095-ab2f-b76fe116a475"
      },
      "source": [
        "\"\"\"\n",
        "import pretty_midi\n",
        "import mido\n",
        "from tqdm import tqdm\n",
        "midilist=[]\n",
        "csvlist=[]\n",
        "jsonlist=[]\n",
        "prettymidilist=[]\n",
        "for filenames in tqdm(midifilenames,position=0):\n",
        "  midi_path='/content/drive/My Drive/MARG/PPDD-Sep2018_sym_mono_large/PPDD-Sep2018_sym_mono_large/prime_midi/'+filenames\n",
        "  mid = mido.MidiFile(midi_path, clip=True)\n",
        "  midilist.append(mid)\n",
        "  prettymid=pretty_midi.PrettyMIDI(midi_path)\n",
        "  prettymidilist.append(prettymid)\n",
        "\n",
        "for filenames in tqdm(csvfilenames,position=0):\n",
        "  csv_path='/content/drive/My Drive/MARG/PPDD-Sep2018_sym_mono_large/PPDD-Sep2018_sym_mono_large/prime_csv/'+filenames\n",
        "  csv = pandas.read_csv(csv_path)\n",
        "  csvlist.append(csv)\n",
        "for filenames in tqdm(jsonfilenames,position=0):\n",
        "  jsonlist.append(get_meta(filenames))\n",
        "\n",
        "#midifilenames와 list들의 순서는 같다고 생각하고 코드 작성\n",
        "\"\"\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nimport pretty_midi\\nimport mido\\nfrom tqdm import tqdm\\nmidilist=[]\\ncsvlist=[]\\njsonlist=[]\\nprettymidilist=[]\\nfor filenames in tqdm(midifilenames,position=0):\\n  midi_path='/content/drive/My Drive/MARG/PPDD-Sep2018_sym_mono_large/PPDD-Sep2018_sym_mono_large/prime_midi/'+filenames\\n  mid = mido.MidiFile(midi_path, clip=True)\\n  midilist.append(mid)\\n  prettymid=pretty_midi.PrettyMIDI(midi_path)\\n  prettymidilist.append(prettymid)\\n\\nfor filenames in tqdm(csvfilenames,position=0):\\n  csv_path='/content/drive/My Drive/MARG/PPDD-Sep2018_sym_mono_large/PPDD-Sep2018_sym_mono_large/prime_csv/'+filenames\\n  csv = pandas.read_csv(csv_path)\\n  csvlist.append(csv)\\nfor filenames in tqdm(jsonfilenames,position=0):\\n  jsonlist.append(get_meta(filenames))\\n\\n#midifilenames와 list들의 순서는 같다고 생각하고 코드 작성\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czIsThWnDhqF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "395fa9f4-4433-4f1b-b287-87ebe69e1417"
      },
      "source": [
        "\"\"\"\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/MARG/PPDDlist/midilist.txt', 'wb') as f:\n",
        "  pickle.dump(midilist, f)\n",
        "with open('/content/drive/My Drive/MARG/PPDDlist/csvlist.txt', 'wb') as f:\n",
        "  pickle.dump(csvlist, f)\n",
        "with open('/content/drive/My Drive/MARG/PPDDlist/jsonlist.txt', 'wb') as f:\n",
        "  pickle.dump(jsonlist, f)\n",
        "with open('/content/drive/My Drive/MARG/PPDDlist/prettymidilist.txt', 'wb') as f:\n",
        "  pickle.dump(prettymidilist, f)\n",
        "\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nimport pickle\\nwith open('/content/drive/My Drive/MARG/PPDDlist/midilist.txt', 'wb') as f:\\n  pickle.dump(midilist, f)\\nwith open('/content/drive/My Drive/MARG/PPDDlist/csvlist.txt', 'wb') as f:\\n  pickle.dump(csvlist, f)\\nwith open('/content/drive/My Drive/MARG/PPDDlist/jsonlist.txt', 'wb') as f:\\n  pickle.dump(jsonlist, f)\\nwith open('/content/drive/My Drive/MARG/PPDDlist/prettymidilist.txt', 'wb') as f:\\n  pickle.dump(prettymidilist, f)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOttPVfF_yuk",
        "colab_type": "text"
      },
      "source": [
        "아래의 코드는 csv기반의 Processing을 진행한다. \n",
        "MIDI 기반의 Processing은 다음 블록에 작성되어 있으며, 둘 중 하나만 굴리면 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_pieqXw_Yq0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "805256f6-4c5f-4558-be4f-1963e4e6fd7d"
      },
      "source": [
        "#현재 CSV dataset에 문제가 있어보인다..\n",
        "\"\"\"\n",
        "bar_list=[]\n",
        "one_bar_number_list=[]\n",
        "starting_number_list=[]\n",
        "import numpy as np\n",
        "for i,csvs in enumerate(tqdm(csvlist)):\n",
        "  a=np.array([list(map(float,csvs.columns))])#column에도 숫자가 들어가 있어서.. 경우에 따라 조절한다\n",
        "  b=np.array(csvs.values)\n",
        "  csvarray=np.concatenate((a,b),axis=0)\n",
        "  if('timeSignature' not in jsonlist[i]):\n",
        "    jsonlist[i]['timeSignature']=[4,4]\n",
        "  one_bar_number=jsonlist[i]['timeSignature'][0]\n",
        "  bar_number=(csvarray[-1][0]-csvarray[0][0])//one_bar_number+1\n",
        "  bar_info_list=[]\n",
        "  for i in range(int(bar_number)):\n",
        "    starting_bar_time=csvarray[0][0]+i*one_bar_number\n",
        "    bar_info_list.append(csvarray[np.where( (starting_bar_time<=csvarray[:,0]) & (csvarray[:,0]<starting_bar_time+one_bar_number) )])\n",
        "  bar_list.append(bar_info_list)\n",
        "  one_bar_number_list.append(one_bar_number)\n",
        "  starting_number_list.append(csvarray[0][0])\n",
        "\"\"\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nbar_list=[]\\none_bar_number_list=[]\\nstarting_number_list=[]\\nimport numpy as np\\nfor i,csvs in enumerate(tqdm(csvlist)):\\n  a=np.array([list(map(float,csvs.columns))])#column에도 숫자가 들어가 있어서.. 경우에 따라 조절한다\\n  b=np.array(csvs.values)\\n  csvarray=np.concatenate((a,b),axis=0)\\n  if('timeSignature' not in jsonlist[i]):\\n    jsonlist[i]['timeSignature']=[4,4]\\n  one_bar_number=jsonlist[i]['timeSignature'][0]\\n  bar_number=(csvarray[-1][0]-csvarray[0][0])//one_bar_number+1\\n  bar_info_list=[]\\n  for i in range(int(bar_number)):\\n    starting_bar_time=csvarray[0][0]+i*one_bar_number\\n    bar_info_list.append(csvarray[np.where( (starting_bar_time<=csvarray[:,0]) & (csvarray[:,0]<starting_bar_time+one_bar_number) )])\\n  bar_list.append(bar_info_list)\\n  one_bar_number_list.append(one_bar_number)\\n  starting_number_list.append(csvarray[0][0])\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeRQyawg_8tO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#이게 이제 input을 midi로 받는 코드.\n",
        "bar_list=[]\n",
        "one_bar_number_list=[]\n",
        "starting_number_list=[]\n",
        "for i,songs in enumerate(prettymidilist):#곡마다.\n",
        "  for instrument in songs.instruments: #2. 어차피 instrument하나\n",
        "    csvarray=[]\n",
        "    for note in instrument.notes: #3\n",
        "      row=[note.start*2, note.pitch, note.pitch, (note.end - note.start)*2, 0] #*2를 해줘야 제대로 하나의 bar가 하나의 단위가 된다.\n",
        "      csvarray.append(row)\n",
        "  csvarray=np.array(csvarray)\n",
        "  if('timeSignature' not in jsonlist[i]):\n",
        "    jsonlist[i]['timeSignature']=[4,4]\n",
        "  one_bar_number=jsonlist[i]['timeSignature'][0]\n",
        "  bar_number=(csvarray[-1][0]-csvarray[0][0])//one_bar_number+1\n",
        "  bar_info_list=[]\n",
        "  for i in range(int(bar_number)):\n",
        "    starting_bar_time=csvarray[0][0]+i*one_bar_number\n",
        "    bar_info_list.append(csvarray[np.where( (starting_bar_time<=csvarray[:,0]) & (csvarray[:,0]<starting_bar_time+one_bar_number) )])\n",
        "  bar_list.append(bar_info_list)# bar info list가 bar마다 csv내용들 담겨져 있는거다.\n",
        "  one_bar_number_list.append(one_bar_number)\n",
        "  starting_number_list.append(csvarray[0][0])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ubPcKmPBXQH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d9ecb6bc-5bf2-4c96-dc10-587199c401ec"
      },
      "source": [
        "\"\"\"\n",
        "for bars in bar_list:\n",
        "  #print(len(bars))결과 보면 다 다르다. 따라서 정해진 Shape가 없음\n",
        "  for matrix in bars:\n",
        "    for lists in matrix:\n",
        "      if (lists[1]<24):\n",
        "        print(lists[1])#Small Data기준 95가 최대, 25가 Minimum. Big Data로 바꾸면 추가적인 조절 가능할듯\n",
        "\"\"\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfor bars in bar_list:\\n  #print(len(bars))결과 보면 다 다르다. 따라서 정해진 Shape가 없음\\n  for matrix in bars:\\n    for lists in matrix:\\n      if (lists[1]<24):\\n        print(lists[1])#Small Data기준 95가 최대, 25가 Minimum. Big Data로 바꾸면 추가적인 조절 가능할듯\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioRp4dJDCtUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "d632a15b-72f4-4eda-915d-205a1b16ab93"
      },
      "source": [
        "print(bar_list[0][0])#1번째 곡의 1번째 bar.\n",
        "print(jsonlist[0])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.  48.  48.   0.5  0. ]\n",
            " [ 0.5 55.  55.   0.5  0. ]\n",
            " [ 1.5 52.  52.   0.5  0. ]\n",
            " [ 2.  50.  50.   0.5  0. ]\n",
            " [ 2.5 48.  48.   0.5  0. ]\n",
            " [ 3.  46.  46.   1.   0. ]\n",
            " [ 4.  53.  53.   0.5  0. ]]\n",
            "{'id': '0000ee22-8e62-47c3-aca3-49b7999f8cf7', 'idLakh': '86545d15b5b9dda20268c47d827f072d', 'bpm': 145, 'timeSignature': [5, 4], 'keyEstimate': 'C major'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK7G2wiA4fgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nearest_time(time,minimum_size):\n",
        "  #혹시나 값이 조금 벗어나는 엇박 음을 가까운 최소단위로 Shifting한다.\n",
        "  #다만 엇박 관련 Skill Detecting을 따로 고려할시 코드를 수정할 수 있다.\n",
        "  num_to_multiply=time/minimum_size\n",
        "  num_to_multiply=int(num_to_multiply)\n",
        "  left_time=num_to_multiply*minimum_size\n",
        "  right_time=left_time+minimum_size\n",
        "  if (time-left_time>=right_time-time):\n",
        "    return right_time\n",
        "  return left_time\n",
        "def bar_to_matrix1(bar,one_bar_number,starting_number,i):\n",
        "  #그냥 점만 남긴다.\n",
        "  #8/6박이면 one_bar_number가 8이다. 그러면 바 1개당 무조건 12개 처리하는거로 한다.\n",
        "  #lists[0]은 시간, lists[1]은 Note 높이, lists[3]은 Duration. \n",
        "  init=np.zeros((112,96))#112는 Note의 수(감으로 써둠.. 나중에 전체 데이터로 할때 수정 가능성 있음 그런데 Shift를 잘 이용하면 96*96도 가능해보임.)\n",
        "  minimum_size=one_bar_number/96\n",
        "  zero_time=starting_number+one_bar_number*i    \n",
        "  for j,lists in enumerate(bar):\n",
        "    \n",
        "    point=int((nearest_time(lists[0],minimum_size)-zero_time)/minimum_size)\n",
        "    if(point==96):\n",
        "      point=92\n",
        "    \n",
        "    init[111-int(lists[1])][point]=lists[3]#111-int(list[1])형태로 해야 직관적인 PianoRoll 형태가 ㅏ온다.\n",
        "  return init\n",
        "def bar_to_matrix2(bar,one_bar_number,starting_number,i):\n",
        "  #Duration에 따라 Ploting한다.\n",
        "  #8/6박이면 one_bar_number가 8이다. 그러면 바 1개당 무조건 12개 처리하는거로 한다.\n",
        "\n",
        "  init=np.zeros((112,96))#112는 Note의 수(감으로 써둠.. 나중에 전체 데이터로 할때 수정 가능성 있음 그런데 Shift를 잘 이용하면 96*96도 가능해보임.)\n",
        "  minimum_size=one_bar_number/96\n",
        "  zero_time=starting_number+one_bar_number*i    \n",
        "  for j,lists in enumerate(bar):\n",
        "    #lists[0]은 시간, lists[1]은 Note 높이, lists[3]은 Duration. \n",
        "    point=int((nearest_time(lists[0],minimum_size)-zero_time)/minimum_size)\n",
        "    length=int(round(lists[3]/minimum_size))\n",
        "    if (length>3):\n",
        "      length=length-1#여러번 두두두 치는 음을 구별하기 위함\n",
        "    if (point+length>95):\n",
        "      length=95-point # 한 음이 2Bar에 걸쳐있는 경우 Bar 뒤쪽의 음을 무시한다.\n",
        "    init[111-int(lists[1])][point:point+length]+=1\n",
        "  return init\n",
        "\n",
        "def bar_to_matrix3(bar,one_bar_number,starting_number,i):\n",
        "  #Duration에 따라 Ploting한다.\n",
        "  #size를 상당히 작게 잡는다.\n",
        "  #8/6박이면 one_bar_number가 8이다. 그러면 바 1개당 무조건 12개 처리하는거로 한다.\n",
        "\n",
        "  init=np.zeros((24,24))#112는 Note의 수(감으로 써둠.. 나중에 전체 데이터로 할때 수정 가능성 있음 그런데 Shift를 잘 이용하면 96*96도 가능해보임.)\n",
        "  minimum_size=one_bar_number/24\n",
        "  zero_time=starting_number+one_bar_number*i\n",
        "  min_height=500\n",
        "  for lists in bar:\n",
        "    if min_height>lists[1]:\n",
        "      min_height=lists[1]    \n",
        "  for j,lists in enumerate(bar):\n",
        "    #lists[0]은 시간, lists[1]은 Note 높이, lists[3]은 Duration. \n",
        "    point=int((nearest_time(lists[0],minimum_size)-zero_time)/minimum_size)\n",
        "    length=int(round(lists[3]/minimum_size))\n",
        "    if (length>3):\n",
        "      length=length-1#여러번 두두두 치는 음을 구별하기 위함\n",
        "    if (point+length>23):\n",
        "      length=23-point # 한 음이 2Bar에 걸쳐있는 경우 Bar 뒤쪽의 음을 무시한다.\n",
        "    height=lists[1]-min_height\n",
        "    while(height>23):\n",
        "      height=height-12\n",
        "    init[23-int(height)][point:point+length]+=1\n",
        "  return init"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t34eVSFIT6M-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bar_list to bar_matrix_list\n",
        "#bar_matrix_list=copy.deepcopy(bar_list)\n",
        "#bar_matrix_list2=copy.deepcopy(bar_list)\n",
        "bar_matrix_list3=copy.deepcopy(bar_list)\n",
        "for i,songs in enumerate(bar_matrix_list3):\n",
        "  for j,bar in enumerate(songs):\n",
        "    #print(one_bar_number_list[i],starting_number_list[i])\n",
        "    #matrix=bar_to_matrix1(bar,one_bar_number_list[i],starting_number_list[i],j)\n",
        "    #matrix2=bar_to_matrix2(bar,one_bar_number_list[i],starting_number_list[i],j)\n",
        "    matrix3=bar_to_matrix3(bar,one_bar_number_list[i],starting_number_list[i],j)\n",
        "    #bar_matrix_list[i][j]=matrix\n",
        "    #bar_matrix_list2[i][j]=matrix2\n",
        "    bar_matrix_list3[i][j]=matrix3"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJUSzyDlbQ3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bar_updown_list=copy.deepcopy(bar_list)\n",
        "for i,songs in enumerate(bar_list):\n",
        "  for j,bar in enumerate(songs):\n",
        "    if (j==len(songs)-1):\n",
        "      updown_label='final'\n",
        "    elif(len(bar_list[i][j])==0 or len(bar_list[i][j+1])==0):\n",
        "      updown_label='meanless'\n",
        "    else:\n",
        "      if(bar_list[i][j][len(bar_list[i][j])-1][1]<=bar_list[i][j+1][0][1]):\n",
        "        updown_label='up'\n",
        "      else:\n",
        "        updown_label='down'\n",
        "    bar_updown_list[i][j]=updown_label"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO6-SYCu29qF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "04c51a24-b97a-4fab-ca15-b71a0574cb72"
      },
      "source": [
        "#print(bar_list[0][0],bar_matrix_list[0][0],bar_matrix_list2[0][0]) 큰 의미 없다\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "H = bar_matrix_list3[0][0]\n",
        "\n",
        "fig = plt.figure(figsize=(6, 3.2))\n",
        "\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_title('colorMap')\n",
        "plt.imshow(H)\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
        "cax.get_xaxis().set_visible(False)\n",
        "cax.get_yaxis().set_visible(False)\n",
        "cax.patch.set_alpha(0)\n",
        "cax.set_frame_on(False)\n",
        "plt.colorbar(orientation='vertical')\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUwAAADdCAYAAADQI0sNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUMklEQVR4nO3de5BedX3H8feHEEi5eMFoCiQIanAa0YKTAWdwNBQvgbFEp5YSa4GWIf5BrBfqiNQCQ0fFtqI4UssKKReVS1EkY1OjUBlqq5hYHSRQNEUiCYEQiJKKQLL76R/nrD552D3P2d3sc052P6+ZM3suv+d3vjzAd3+3c1a2iYiI3vZqOoCIiD1FEmZERE1JmBERNSVhRkTUlIQZEVFTEmZERE1JmNEakg6XZEl7Nx1LxEiSMGPKkHRRmXDf13X+feX5ixoKLaaIJMyYEjpapT8BTu+6fEZ5PmJCkjBj0kiaJ+mrkh6T9Likz0naS9JHJW2QtEXStZKeP8rnD5G0UtITktZLOrvj2kWSbpb0RUlPAmeWl9YA+0l6VVnuVcCs8vzwZ18o6etlXNvK/bkd1++Q9AlJ35f0pKRbJR20+7+h2NMkYcakkDQD+DqwATgcOBS4gSKxnQmcALwMOAD43CjV3ABsBA4B3gl8XNIfdFxfAtwMvAD4Usf56/htK/OM8rjTXsA/Ay8FDgN+PUIMpwN/ARwM7AQ+W/XPG9NDEmZMlmMpEt2HbP/K9tO2vwP8KXCp7Qds/x/wEeC07okeSfOA44EPl5/9EXAlu3a3v2v7a7aHbP+64/wXgaWSZgKnlce/Yftx21+x/ZTt7cDHgDd2xX+d7Xts/wr4G+DU8pdATGNJmDFZ5gEbbO/sOn8IRatz2AZgb2DOCOWeKBNaZ9lDO44fGunGtn8OrAc+DvzU9i7lJO0n6YpyWOBJ4E7gBV0JsfMzG4CZwOyR7hfTRxJmTJaHgMNGWCL0MEVXeNhhFF3eR0cod5CkA7vKbuo4rnrV1rXAueXPbucCrwSOs/084A3leXWUmdd13x3A1or7xTSQhBmT5fvAZuASSftLmiXpeOB64AOSjpB0AEUr8MbulmjZKvwv4BPlZ18DnEVX97rCjcBbgJtGuHYgxbjlL8rJnAtHKPNuSQsk7QdcDNxse7DmvaMFJK0oJxbvGeW6JH22nFC8W9Jre9WZhBmTokwufwi8Avg5xeTNnwArKCZh7gR+BjwNvHeUapZSTBg9DNwCXGj7tpr3/7Xt27rGNod9Bvgdihbj94BvjFDmOuBq4BGKWfa/rHPfaJWrgcUV108C5pfbMuDzvSpUXiAcsStJdwBftH1l07HExEg6HPi67aNGuHYFcIft68vj+4FFtjePVl8eQYuI1nnrCfv78SeqR0B+cPcz6yh6KMMGbA+M4TaHsuvk3sbyXBJmROw5tj4xyF2r51aWmXnw/z5te2GfQgKSMCOew/aipmMIM+ihyb7JJnZdDTGXXVdhPEcmfSKidQwM4cptN1gJnF7Olr8O+GXV+CWkhRkRLWTMjgmu4pJ0PbAImC1pI8XysZkAtv8JWAWcTPGQw1PAn/eqMwlznCQtBi4DZgBX2r6kqvw+2tez2L8vsUXsLtvZttX2i5u490RbkbaX9rhu4Jyx1JmEOQ7lI3SXA2+mmFlbI2ml7XtH+8ws9uc4ndivECN2i9t884bepXY/AzuY9DHMMcsY5vgcC6wvXyDxLMVbdZY0HFPElGFg0K7cmpCEOT6jrd/ahaRlktZKWruDZ/oWXMRUMNRja0K65JOoXEQ7APA8HZRHqiJqss2zLXwKMQlzfMa8fisi6iuWFbVPuuTjswaYX75xZx+Kl9SubDimiClEDPbYmpAW5jjY3ilpObCaYlnRCtvrGg4rYsowsMPNJMUqSZjjZHsVxcLXiNjNDI21IqskYUZE6xQtzPaNGCZhRkTrGDHYwimWJMyIaKWhjGFGRPRmxLNu3181TsKMiNYp1mGmSx4R0ZOdFmZERG1DWVYUEdFbsQ4zXfKIiJ6M2OH2paf2RRQRAQxmWVFERG9pYUZE1JQxzIiImozSJY+IqMMmXfKIiHqUdZgREXUUfzUyY5gRET0Vs+R5NDIiopbMkkdE1NDWFmb7UnhETHsGhrxX5daLpMWS7pe0XtJ5I1w/TNK3Jf1Q0t2STu5VZxJmRLTSRP7MrqQZwOXAScACYKmkBV3FPgrcZPsYij+V/Y+9YkqXPCJaxxY7hiaUno4F1tt+AEDSDcAS4N7O2wDPK/efDzzcq9IkzIhoneKN6xNah3ko8FDH8UbguK4yFwHflPReYH/gTb0qTZc8IlrHiB1DMyo3YLaktR3bsjHeZilwte25wMnAdZIqc2JamBHRSjWWFW21vXCUa5uAeR3Hc8tznc4CFgPY/q6kWcBsYMtoN0wLMyJax4ghV289rAHmSzpC0j4Ukzoru8r8HDgRQNLvAbOAx6oqTQszIlqnePnG+Ndh2t4paTmwGpgBrLC9TtLFwFrbK4FzgS9I+gDFsOmZtl1VbxLmOEl6ENgODAI7K7oGETEONVqRlWyvAlZ1nbugY/9e4Pix1JmEOTEn2N7adBARU01bn/RJwoyI1ime9Gnf690y6TN+pljD9YNxLGeIiEqa8KORkyEtzPF7ve1Nkl4CfEvS/9i+s7NAmUiXAcxivyZijNgjFZM+7WvPtS+iPYTtTeXPLcAtFI9idZcZsL3Q9sKZ7NvvECP2aG1sYSZhjoOk/SUdOLwPvAW4p9moIqaO3bAOc1KkSz4+c4BbJEHxHX7Z9jeaDSli6jCws4Vd8iTMcSjfgPL7TccRMZU11e2ukoQZEa1jKy3MiIi62rgOMwkzIlqnrQvXkzAjonWM2DmULnlERC0TfOP6pEjCjIjWsUkLMyKiroxhRkTUMPykT9skYUZEKw1mHWZERG92uuQRETWJwUz6RETU47QwIyJ6y5M+ERF1GQaTMCMiejPpkkdE1JR1mBERtQ0NJWFGRPRkp0seEVHbYFqYERH1tLGF2b6l9BEx7RlhV2+9SFos6X5J6yWdN0qZUyXdK2mdpC/3qjMtzIhonwk+Sy5pBnA58GZgI7BG0krb93aUmQ98BDje9jZJL+lVb1qYEdFO7rFVOxZYb/sB288CNwBLusqcDVxuexuA7S29Kk3CjIhWGhpS5dbDocBDHccby3OdjgSOlPSfkr4naXGvStMlj4jWqfmkz2xJazuOB2wPjOE2ewPzgUXAXOBOSa+2/YuqD0REtIuB3glzq+2Fo1zbBMzrOJ5bnuu0EbjL9g7gZ5J+QpFA14x2w3TJI6KVPFS99bAGmC/pCEn7AKcBK7vKfI2idYmk2RRd9AeqKk3CrCBphaQtku7pOHeQpG9J+mn584VNxhgxNU1sWZHtncByYDVwH3CT7XWSLpZ0SllsNfC4pHuBbwMfsv14Vb1JmNWuBroHgs8Dbrc9H7i9PI6I3cngIVVuPauwV9k+0vbLbX+sPHeB7ZXlvm1/0PYC26+2fUOvOpMwK9i+E3ii6/QS4Jpy/xrg7X0NKmK6mNiyokmRSZ+xm2N7c7n/CDCnyWAipq72PRqZhDkBti1p1N91kpYBywBmsV/f4oqYEnpP7PRduuRj96ikgwHKn6M+HWB7wPZC2wtnsm/fAozY4w0vK6raGpCEOXYrgTPK/TOAWxuMJWLKKt6JOfrWhCTMCpKuB74LvFLSRklnAZcAb5b0U+BN5XFE7G5Dqt4akDHMCraXjnLpxL4GEjENjT470JwkzIhoHzfXiqyShBkR7ZQWZkRETUmYERE1mHTJIyLqyqRPRERdSZgREfWkhRkRUVcL/y55EmZEtI9p5cs3kjAjopXSJY+IqCstzIiI3uS0MCMi6sukT0REPUqXPCKipnTJIyJqyBhmRMQYpEseEVFPWpgREXUlYUZE1JAxzBiP1Q//aNyffeshR+/GSCL6rIUJM39mNyJaRxTrMKu2nnVIiyXdL2m9pPMqyv2RJEta2KvOJMyIaCf32CpImgFcDpwELACWSlowQrkDgfcBd9UJKQkzItrHE25hHgust/2A7WeBG4AlI5T7W+CTwNN1wkrCjIh2mkALEzgUeKjjeGN57jckvRaYZ/tf64aUSZ+IaKUas+SzJa3tOB6wPVCrbmkv4FLgzLHElIQZEe1T743rW22PNlGzCZjXcTy3PDfsQOAo4A5JAL8LrJR0iu3OJLyLJMyIaKUJrsNcA8yXdARFojwNeNfwRdu/BGb/5l7SHcBfVSVLSMKsJGkF8DZgi+2jynMXAWcDj5XFzre9arJiyFrKmK4m8no32zslLQdWAzOAFbbXSboYWGt75XjqTcKsdjXwOeDarvOftv0P/Q8nYhqZ4ML1siGzquvcBaOUXVSnzsySV7B9J/BE03FETDu9ZsgbegooCXN8lku6W9IKSS8crZCkZZLWSlq7g2f6GV/EHk389u/6jLY1IQlz7D4PvBw4GtgMfGq0grYHbC+0vXAm+/YrvogpIQlzCrD9qO1B20PAFyieKIiI3S1d8j2fpIM7Dt8B3NNULBFT1sQfjZwUmSWvIOl6YBHFEwUbgQuBRZKOpvgd9yDwnsYCjClnvK/zm5LLz1r4erckzAq2l45w+qq+BxIxDeXP7EZE1JQ3rkdE1NHgxE6VJMyIaJ3hN663TRJmRLRTWpgRETUYNNS+jJmEGWOWpS+TJ9/Rb2XSJyKiriTMiIh6MukTEVFHgy/YqJKEGRGtk2VFERFj4fY1MZMwI6KV0iWPKSFLX2LSGTTYdBDPlYQZEe2UFmZERD3pkkdE1JFHIyMixqB9+TIJMyLaR3ZamBERdWUMMyKiriTMiJGN95VxkHWhU5JBg+3LmPm75BHRTu6x9SBpsaT7Ja2XdN4I1z8o6V5Jd0u6XdJLe9WZhBkRraQhV26Vn5VmAJcDJwELgKWSFnQV+yGw0PZrgJuBv+sVUxJmRLSSXL31cCyw3vYDtp8FbgCWdBaw/W3bT5WH3wPm9qo0CTMi2qdXd7x3wjwUeKjjeGN5bjRnAf/Wq9JM+kRE64hakz6zJa3tOB6wPTDme0nvBhYCb+xVNgkzIlpJvd+HudX2wlGubQLmdRzPLc/teg/pTcBfA2+0/UyvGyZhVpA0D7gWmEPRCRiwfZmkg4AbgcOBB4FTbW9rKs6pIEuDYhc2TOxJnzXAfElHUCTK04B3dRaQdAxwBbDY9pY6lWYMs9pO4FzbC4DXAeeUM23nAbfbng/cXh5HxG40kUkf2zuB5cBq4D7gJtvrJF0s6ZSy2N8DBwD/IulHklb2iiktzAq2NwOby/3tku6jGDheAiwqi10D3AF8uIEQI6auCf6JCturgFVd5y7o2H/TWOtMwqxJ0uHAMcBdwJwymQI8QtFlH+kzy4BlALPYb/KDjJgq8qTPnkvSAcBXgPfbfrLzmu1RFznYHrC90PbCmezbh0gjppAJPukzGZIwe5A0kyJZfsn2V8vTj0o6uLx+MFBrwDgi6pNduTUhCbOCJAFXAffZvrTj0krgjHL/DODWfscWMaUZGHT11oCMYVY7Hvgz4MeShl+ncz5wCXCTpLOADcCpvSo68jVPsXr12N/Ik+U2vY33TUf5bttLNNeKrJKEWcH2dygeOhjJif2MJWLaGRpqOoLnSMKMiPYx0L58mYQZEe2ULnlERC1OlzwiohYz4Sd9JkMSZkS0Uhuf9EnC7JOf3L1flrFMknyvU1RamBERNZiJvt5tUiRhRkQLZdInIqK+dMkjImqwYXCw6SieIwkzItopLcyIiBoy6RMRMQaZ9Iloj/G+Fm4isma0LqdLHhFRi0kLMyKitiTMiIg6nEmfiIhaDM46zIiImjLpExFRg/Ms+bS2nW1bb/PNG8rD2cDWJuMZQdtimvR4Zhw8puK7KZ71E6+i0K9/Xy/twz1GlC75NGb7xcP7ktbaXthkPN3aFlPiqda2eHa/dq7D3KvpACIinsMUL9+o2nqQtFjS/ZLWSzpvhOv7SrqxvH6XpMN71ZmEGRGtY8BDrtyqSJoBXA6cBCwAlkpa0FXsLGCb7VcAnwY+2SuuJMxmDDQdwAjaFlPiqda2eHYvGzxUvVU7Flhv+wHbzwI3AEu6yiwBrin3bwZOlKSqSpMwG2C7df+xty2mxFOtbfFMBg8OVm49HAo81HG8sTw3YhnbO4FfAi+qqjSTPhHROtvZtvo23zy7R7FZktZ2HA9M9i+SJMw+k7QYuAyYAVxp+5KG43kQ2A4MAjubmHmVtAJ4G7DF9lHluYOAG4HDgQeBU21vazCei4CzgcfKYufbXtWneOYB1wJzKIb3Bmxf1uR3NNlsL55gFZuAeR3Hc8tzI5XZKGlv4PnA41WVpkveRzUHoptwgu2jG1ymcjXQ/T/IecDttucDt5fHTcYD8Onyezq6X8mytBM41/YC4HXAOeV/N01+R223Bpgv6QhJ+wCnASu7yqwEzij33wn8u129likJs7/qDERPO7bvBJ7oOt05IH8N8PaG42mM7c22/7vc3w7cRzH+1th31HblmORyYDXF93WT7XWSLpZ0SlnsKuBFktYDH6TGL5x0yftrpIHo4xqKZZiBb0oycEWLJhPm2N5c7j9C0R1t2nJJpwNrKVp8fe/+lmsFjwHuop3fUWuUvYBVXecu6Nh/GvjjsdSZFma83vZrKYYJzpH0hqYD6lZ2k5p+7OPzwMuBo4HNwKf6HYCkA4CvAO+3/WTntZZ8R1NeEmZ/1RmI7ivbm8qfW4BbKIYN2uBRSQcDlD+3NBmM7UdtD9oeAr5An78nSTMpkuWXbH+1PN2q72g6SMLsrzoD0X0jaX9JBw7vA28B7mkqni6dA/JnALc2GMtwQhr2Dvr4PZWLqa8C7rN9acelVn1H04F6TArFbibpZOAzFMuKVtj+WIOxvIyiVQnFePaXm4hH0vXAIoo38DwKXAh8DbgJOAzYQLFkpi8TMaPEs4iiO26KJTzv6Rg/nOx4Xg/8B/BjYPgRl/MpxjEb+Y6mqyTMiIia0iWPiKgpCTMioqYkzIiImpIwIyJqSsKMiKgpCTMioqYkzIiImpIwIyJq+n8jPeV21mqw2wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x230.4 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV-em-Jo4I8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ab4badc4-070f-489a-f9e6-4ae5867fc200"
      },
      "source": [
        "tot=bar_matrix_list3[2]\n",
        "H=bar_matrix_list3[2][0]\n",
        "for i in range(1,len(tot)):\n",
        "  a=bar_matrix_list3[2][i]\n",
        "  H=np.concatenate((H,a),axis=1)\n",
        "  \n",
        "fig = plt.figure(figsize=(30, 3.2))\n",
        "\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_title('colorMap')\n",
        "plt.imshow(H)\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
        "cax.get_xaxis().set_visible(False)\n",
        "cax.get_yaxis().set_visible(False)\n",
        "cax.patch.set_alpha(0)\n",
        "cax.set_frame_on(False)\n",
        "plt.colorbar(orientation='vertical')\n",
        "plt.show()\n",
        "#위 for문 없이 돌리면 1 bar만 나옴"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x230.4 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhslYmbJdSba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bar_to_contour(bar,one_bar_number,starting_number,j):\n",
        "  contour=[]\n",
        "  pitch_change_list=[]\n",
        "  duration_list=[]\n",
        "  real_pitch_list=[]\n",
        "  real_time_list=[]\n",
        "  real_duration_list=[]\n",
        "  now_pitch=1000\n",
        "  first_time=starting_number+one_bar_number*j\n",
        "  a=0\n",
        "  for lists in bar:\n",
        "    if(a!=0): \n",
        "      real_time_list.append(lists[0]-now_rhythm)\n",
        "    now_rhythm=lists[0]\n",
        "    a+=1\n",
        "    if (first_time*1.001<lists[0]):#smoothing for case like first time=5.00001, lists[0]=5.0000..\n",
        "      resting_time=lists[0]-first_time\n",
        "      duration_list.append(resting_time)\n",
        "      first_time=lists[0]\n",
        "      pitch_change_list.append('Rest')\n",
        "    if (now_pitch==1000):\n",
        "      pitch_change_list.append('Starting_Point')\n",
        "      real_pitch_list.append('Starting_Point')\n",
        "      real_duration_list.append(lists[3])\n",
        "      duration_list.append(lists[3])\n",
        "      first_time=first_time+lists[3]\n",
        "      now_pitch=lists[1]\n",
        "      a+=1\n",
        "    else:\n",
        "      pitch_change=lists[1]-now_pitch\n",
        "      pitch_change_list.append(str(pitch_change))#나중에 int로 바꿔쓸 것. 자료형 터지는거 때문에 우선 스트링.\n",
        "      duration_list.append(lists[3])\n",
        "      real_duration_list.append(lists[3])\n",
        "      first_time=first_time+lists[3]\n",
        "      now_pitch=lists[1]\n",
        "      real_pitch_list.append(str(pitch_change))\n",
        "  if (first_time*1.001<starting_number+one_bar_number*(j+1)):\n",
        "    pitch_change_list.append('Rest')#마지막 Rest\n",
        "    duration_list.append(starting_number+one_bar_number*(j+1)-first_time)\n",
        "  if(len(bar)!=0):\n",
        "    real_time_list.append(starting_number+one_bar_number*(j+1)-now_rhythm)\n",
        "  contour.append(pitch_change_list)\n",
        "  contour.append(duration_list)\n",
        "  contour.append(real_pitch_list)\n",
        "  contour.append(real_time_list)\n",
        "  contour.append(real_duration_list)\n",
        "  #something\n",
        "  return contour\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkt16alDXVFp",
        "colab_type": "text"
      },
      "source": [
        "Plot에서 중요한것은 Y-Axis의 0~112의 숫자가 Note의 반대 성향을 가진다는 것이다.(숫자가 커질수록 Note의 높이가 낮아진다.) 이는 PianoRoll을 이미지 처럼 사용하는 과정에서, 직관적인 학습이 가능하도록 이와 같이 구현한 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DB6np0r4pr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bar_contour_list=copy.deepcopy(bar_list)\n",
        "for i,songs in enumerate(bar_list):\n",
        "  for j,bar in enumerate(songs):\n",
        "    contour=bar_to_contour(bar,one_bar_number_list[i],starting_number_list[i],j)\n",
        "    bar_contour_list[i][j]=contour\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFxmPmNCVpBD",
        "colab_type": "text"
      },
      "source": [
        "bar_contour_list : 중간과정\n",
        "bar_matrix_list2 : 아마 학습에 사용하게 될 Matrix의 List\n",
        "bar_label_list : 학습에 사용하게 될 Label의 List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO49MM20Lth1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5f44ab77-ab7d-4a15-f4e0-2374cd51fee8"
      },
      "source": [
        "print(bar_contour_list[0][0])\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Starting_Point', '7.0', 'Rest', '-3.0', '-2.0', '-2.0', '-2.0', '7.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5], ['Starting_Point', '7.0', '-3.0', '-2.0', '-2.0', '-2.0', '7.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], [0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqJXy8Wqv1od",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "96e3fed8-9b99-4934-bd1e-9c4ca5ee127f"
      },
      "source": [
        "for contours in bar_contour_list[11]:\n",
        "  print(contours)\n",
        "print(jsonlist[11])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Starting_Point', 'Rest', '-8.0', '3.0', 'Rest', '5.0', 'Rest'], [0.3333333333333333, 0.16666666666666669, 0.25, 0.16666666666666663, 0.08333333333333337, 0.33333333333333326, 2.666666666666667], ['Starting_Point', '-8.0', '3.0', '5.0'], [0.5, 0.25, 0.25, 3.0], [0.3333333333333333, 0.25, 0.16666666666666663, 0.33333333333333326]]\n",
            "[['Rest', 'Starting_Point', '1.0'], [3.5, 0.25, 0.33333333333333215], ['Starting_Point', '1.0'], [0.25, 0.25], [0.25, 0.33333333333333215]]\n",
            "[['Starting_Point', 'Rest'], [0.25, 3.75], ['Starting_Point'], [4.0], [0.25]]\n",
            "[['Rest'], [4.0], [], [], []]\n",
            "[['Starting_Point', '-9.0', '1.0', '8.0', '-12.0', 'Rest'], [0.25, 0.25, 0.33333333333333215, 0.25, 0.33333333333333215, 2.5833333333333357], ['Starting_Point', '-9.0', '1.0', '8.0', '-12.0'], [0.25, 0.25, 0.25, 0.25, 3.0], [0.25, 0.25, 0.33333333333333215, 0.25, 0.33333333333333215]]\n",
            "[['Rest'], [4.0], [], [], []]\n",
            "[['Starting_Point', '-9.0', '1.0', '3.0', 'Rest', '5.0', 'Rest'], [0.25, 0.25, 0.25, 0.1666666666666643, 0.0833333333333357, 0.25, 2.75], ['Starting_Point', '-9.0', '1.0', '3.0', '5.0'], [0.25, 0.25, 0.25, 0.25, 3.0], [0.25, 0.25, 0.25, 0.1666666666666643, 0.25]]\n",
            "[['Rest', 'Starting_Point', 'Rest', '-3.0', 'Rest'], [3.0, 0.25, 0.25, 0.1666666666666643, 0.3333333333333357], ['Starting_Point', '-3.0'], [0.5, 0.5], [0.25, 0.1666666666666643]]\n",
            "[['Starting_Point', 'Rest', '2.0', 'Rest'], [0.1666666666666643, 0.0833333333333357, 0.75, 3.0], ['Starting_Point', '2.0'], [0.25, 3.75], [0.1666666666666643, 0.75]]\n",
            "[['Rest'], [4.0], [], [], []]\n",
            "[['Starting_Point', '1.0', '8.0', 'Rest'], [0.25, 0.25, 0.25, 3.25], ['Starting_Point', '1.0', '8.0'], [0.25, 0.25, 3.5], [0.25, 0.25, 0.25]]\n",
            "[['Rest'], [4.0], [], [], []]\n",
            "[['Starting_Point', 'Rest', '-8.0', '8.0', '-9.0', '1.0', '8.0', '-9.0', '1.0', '8.0', '-9.0', '1.0', '8.0', '-8.0', '8.0'], [0.3333333333333286, 0.1666666666666714, 0.3333333333333286, 0.3333333333333286, 0.25, 0.3333333333333286, 0.3333333333333286, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3333333333333286, 0.3333333333333286, 0.25], ['Starting_Point', '-8.0', '8.0', '-9.0', '1.0', '8.0', '-9.0', '1.0', '8.0', '-9.0', '1.0', '8.0', '-8.0', '8.0'], [0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.25, 0.25], [0.3333333333333286, 0.3333333333333286, 0.3333333333333286, 0.25, 0.3333333333333286, 0.3333333333333286, 0.25, 0.25, 0.25, 0.25, 0.25, 0.3333333333333286, 0.3333333333333286, 0.25]]\n",
            "[['Starting_Point', 'Rest', '12.0', 'Rest', '0.0', 'Rest', '-3.0', 'Rest'], [0.25, 2.75, 0.1666666666666643, 0.0833333333333357, 0.1666666666666643, 0.0833333333333357, 0.1666666666666643, 0.3333333333333357], ['Starting_Point', '12.0', '0.0', '-3.0'], [3.0, 0.25, 0.25, 0.5], [0.25, 0.1666666666666643, 0.1666666666666643, 0.1666666666666643]]\n",
            "[['Starting_Point', 'Rest'], [0.1666666666666643, 3.8333333333333357], ['Starting_Point'], [4.0], [0.1666666666666643]]\n",
            "{'id': '00218695-5fc3-47eb-871f-16f0955840ae', 'idLakh': 'aca4cae9b1320fa31141492ed99c3bc0', 'bpm': 110, 'timeSignature': [4, 4], 'keyEstimate': 'A major'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MThJLfY0V6F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#최대한 많고 깔끔한 조건문을 사용하여 Skill들을 정의해볼 것. Multilabel Classification의 가능성이 있다.\n",
        "\"\"\"\n",
        "Skill들의 음악학적인 특성 & 계산적인 특성을 적는 곳\n",
        "'resting' : 포함하는 음이 0 또는 1개인 경우 resting으로 정의. 다른 Skill들은 겹칠 수 있으나 이 skill이 Label될 경우 그냥 resting 고정이다.\n",
        "즉, Skilling Labeling은 'resting'이 아닌 경우에 진행된다.\n",
        "'repeating' : 전체 음 중 n% 이상 또는 n개를 제외한 경우가 전부 같은 음일 경우 repeating으로 정의\n",
        "'up_steping' : 전체 음 중 n% 이상 또는 n개를 제외한 경우가 steping up 또는 같은 음, 즉 반음기준 3Note 이하로 상승하는 형태일 경우 up_steping으로 정의\n",
        "'down_steping' : 전체 음 중 n% 이상 또는 n개를 제외한 경우가 steping down 또는 같은 음, 즉 반음기준 3Note 이하로 하강하는 형태일 경우 down_steping으로 정의\n",
        "'up_leaping' : 전체 음 중 n% 이상 또는 n개를 제외한 경우가 leaping up, 즉 반음기준 3Note 이상으로 상승하는 형태일 경우 up_leaping으로 정의\n",
        "'down_leaping' : 전체 음 중 n% 이상 또는 n개를 제외한 경우가 leaping down, 즉 반음기준 3Note 이상으로 하강하는 형태일 경우 down_leaping으로 정의\n",
        "3Note에서 겹치는게 맞다. Multilabel Classification을 고안 중이기 때문.\n",
        "'steping_twisting' : 음이 4개 이상이고, n개를 제외한 경우가 2Note 이하의 상승과 하강을 반복하는 형태일 경우 steping_twisting으로 정의\n",
        "'leaping_twisting' : 음이 4개 이상이고, n개를 제외한 경우가 3Note 이상의 상승과 하강을 반복하는 형태일 경우 leaping_twisting으로 정의\n",
        "'fast_rhythm' : 1 bar 내에 음이 9개 이상인 경우 fast_rhythm으로 정의.\n",
        "'One_rhythm' :  모든 음이 지닌 연주의 real_time, 즉 음이 울리고 다음 음이 나올때 까지의 시간이 같으면 One_rhythm으로 정의\n",
        "'triplet' : real_time기반해서 triplet이 존재하면(Note 3개) triplet으로 정의\n",
        "'Staccato' : real_Duration_Time 기반해서 n% 이상의 음의 Duration이 0.16667(최소단위*4임)보다 작으면(매우 짧으면) Staccato로 정의\n",
        "'continuing_rhythm' : pitch_change_list에서 'Rest'의 비율이 25퍼센트 이하면 continuing_rhythm으로 정의\n",
        "첫 음 제외 실 음의 75%를 기준으로 잡는다.\n",
        "5개 이상의 음이 있다면 1개를 제외하고 전부 조건에 맞아야하고,\n",
        "9개 이상의 음이 있다면 2개를 제외하고 전부 조건에 맞아야하고...\n",
        "4개 이하는 다 맞아야 한다.\n",
        "ex) CDEF -> up_steping, CDED -> None, CDEFD-> up_steping.\n",
        "다만 Leaping에 대해서는 많이 후해질 것 같다. 거의 50%가까이..?  \n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "예시로는..\n",
        "contour[0] = ['Starting_Point', 'Rest', '4.0', '1.0', 'Rest', '2.0', 'Rest', '-7.0', 'Rest'] Note pitch의 변화를 쉼표를 포함하여 의미한다.\n",
        "contour[1] = [0.83333, 0.16666999999999987, 0.5, 0.33333, 0.16666999999999987, 0.8333299999999999, 0.16666999999999987, 0.75, 0.25] Duration을 쉼표를 포함하여 의미한다.\n",
        "contour[2] = ['Starting_Point', '4.0', '1.0', '2.0', '-7.0'] note pitch의 변화를 의미한다.\n",
        "contour[3] = [1.0, 0.5, 0.5, 1.0, 1.0] 한 음의 실 연주시간을 의미한다.(다음 음이 나올때 까지의 시간)\n",
        "contour[4] = [0.83333, 0.5, 0.33333, 0.8333299999999999, 0.75] Duration을 의미한다.\n",
        "\"\"\"\n",
        "def is_repeating(contour_list,exception_range):\n",
        "  boolean_repeating=0\n",
        "  non_repeat=0\n",
        "  for elements in contour_list:\n",
        "    if (elements is not 'Starting_Point'):\n",
        "      if(elements != '0.0'):\n",
        "        non_repeat+=1\n",
        "  if (non_repeat<=exception_range):\n",
        "    boolean_repeating=1\n",
        "  return boolean_repeating\n",
        "\n",
        "def is_up_steping(contour_list,exception_range):\n",
        "  balancing_param=1\n",
        "  boolean_up_steping=0\n",
        "  non_step_up=0\n",
        "  now_step_up=0\n",
        "  for elements in contour_list:\n",
        "    if (elements is not 'Starting_Point'):\n",
        "      if (now_step_up==0):\n",
        "        if (float(elements)<0.5 or float(elements)>4.5):\n",
        "          now_step_up=0\n",
        "          non_step_up+=1\n",
        "        else:\n",
        "          now_step_up=1\n",
        "      else:\n",
        "        if(float(elements)<-0.5 or float(elements)>4.5):\n",
        "          now_step_up=0\n",
        "          non_step_up+=1\n",
        "        else:\n",
        "          now_step_up=1\n",
        "  if(non_step_up<=exception_range+balancing_param):\n",
        "    boolean_up_steping=1\n",
        "  return boolean_up_steping\n",
        "\n",
        "def is_down_steping(contour_list,exception_range):\n",
        "  boolean_down_steping=0\n",
        "  balancing_param=0\n",
        "  non_step_down=0\n",
        "  now_step_down=0\n",
        "  for elements in contour_list:\n",
        "    if (elements is not 'Starting_Point'):\n",
        "      if (now_step_down==0):\n",
        "        if (float(elements)>-0.5 or float(elements)<-4.5):\n",
        "          now_step_down=0\n",
        "          non_step_down+=1\n",
        "        else:\n",
        "          now_step_down=1\n",
        "      else:\n",
        "        if(float(elements)>0.5 or float(elements)<-4.5):\n",
        "          now_step_down=0\n",
        "          non_step_down+=1\n",
        "        else:\n",
        "          now_step_down=1\n",
        "  if(non_step_down<=exception_range+balancing_param):\n",
        "    boolean_down_steping=1\n",
        "  return boolean_down_steping\n",
        "\n",
        "def is_up_leaping(contour_list,exception_range):\n",
        "  boolean_up_leaping=0\n",
        "  non_leap_up=0\n",
        "  for elements in contour_list:\n",
        "    if (elements is not 'Starting_Point'):\n",
        "      if (float(elements)<3.5):\n",
        "        non_leap_up+=1\n",
        "  if (non_leap_up<=exception_range+1):\n",
        "    boolean_up_leaping=1\n",
        "  return boolean_up_leaping\n",
        "\n",
        "def is_down_leaping(contour_list,exception_range):\n",
        "  boolean_down_leaping=0\n",
        "  non_leap_down=0\n",
        "  for elements in contour_list:\n",
        "    if (elements is not 'Starting_Point'):\n",
        "      if (float(elements)>-3.5):\n",
        "        non_leap_down+=1\n",
        "  if (non_leap_down<=exception_range+1):\n",
        "    boolean_down_leaping=1\n",
        "  return boolean_down_leaping\n",
        "\n",
        "def is_leaping_twisting(contour_list,exception_range):\n",
        "  boolean_leaping_twisting=0\n",
        "  non_leap_twist=0\n",
        "  balancing_param=1\n",
        "  now_dir=0 #1for up, -1 for down\n",
        "  for elements in contour_list:\n",
        "    if (elements is not 'Starting_Point'):\n",
        "      if (now_dir==0):\n",
        "        if (3.5<float(elements)):\n",
        "          now_dir=1\n",
        "        elif (float(elements) <-3.5):\n",
        "          now_dir=-1\n",
        "        else:\n",
        "          non_leap_twist+=1\n",
        "      elif (now_dir==1):\n",
        "        if (float(elements) <-3.5):\n",
        "          now_dir=-1\n",
        "        else:\n",
        "          now_dir=0\n",
        "          non_leap_twist+=1\n",
        "      elif (now_dir==-1):\n",
        "        if (3.5<float(elements)):\n",
        "          now_dir=1\n",
        "        else:\n",
        "          now_dir=0\n",
        "          non_leap_twist+=1\n",
        "  if(non_leap_twist<=exception_range+balancing_param):\n",
        "    boolean_leaping_twisting=1\n",
        "  return boolean_leaping_twisting\n",
        "\n",
        "def is_steping_twisting(contour_list,exception_range):\n",
        "  boolean_steping_twisting=0\n",
        "  non_step_twist=0\n",
        "  now_dir=0 #1for up, -1 for down\n",
        "  for elements in contour_list:\n",
        "    if (elements is not 'Starting_Point'):\n",
        "      if (now_dir==0):\n",
        "        if (0<float(elements) and float(elements) <2.5):\n",
        "          now_dir=1\n",
        "        elif (-2.5<float(elements) and float(elements) <0):\n",
        "          now_dir=-1\n",
        "        else:\n",
        "          non_step_twist+=1\n",
        "      elif (now_dir==1):\n",
        "        if (-2.5<float(elements) and float(elements) <0):\n",
        "          now_dir=-1\n",
        "        else:\n",
        "          now_dir=0\n",
        "          non_step_twist+=1\n",
        "      elif (now_dir==-1):\n",
        "        if (0<float(elements) and float(elements) <2.5):\n",
        "          now_dir=1\n",
        "        else:\n",
        "          now_dir=0\n",
        "          non_step_twist+=1\n",
        "  if(non_step_twist<=exception_range):\n",
        "    boolean_steping_twisting=1\n",
        "  return boolean_steping_twisting\n",
        "\n",
        "def is_one_rhythm(contour_list,exception_range):\n",
        "  boolean_one_rhythm=0\n",
        "  non_same_rhythm=0\n",
        "\n",
        "  first_rhythm=contour_list[0]\n",
        "  for rhythms in contour_list:\n",
        "    if (rhythms != first_rhythm):\n",
        "      non_same_rhythm=1\n",
        "  boolean_one_rhythm=1-non_same_rhythm\n",
        "  return boolean_one_rhythm\n",
        "\n",
        "def is_triplet(contour_list,exception_range):\n",
        "  boolean_triplet=0\n",
        "  now_triplet=0\n",
        "  for rhythms in contour_list:\n",
        "    rhythms=float(rhythms)\n",
        "    if (rhythms%0.015625>0.001):\n",
        "      if(now_triplet==1):\n",
        "        boolean_triplet=1\n",
        "      now_triplet+=1\n",
        "    else:\n",
        "      now_triplet=0\n",
        "      \n",
        "  return boolean_triplet\n",
        "\n",
        "def is_staccato(contour_list,exception_range):\n",
        "  boolean_staccato=0\n",
        "  ranges=len(contour_list)//2\n",
        "  staccato_num=0\n",
        "  for times in contour_list:\n",
        "    if (times<0.2):\n",
        "      staccato_num+=1\n",
        "  if (staccato_num>=ranges):\n",
        "    boolean_staccato=1\n",
        "  return boolean_staccato\n",
        "\n",
        "def is_continuing_rhythm(contour_list):\n",
        "  boolean_continuing_rhythm=0\n",
        "  length=len(contour_list)\n",
        "  rest_num=0\n",
        "  for elements in contour_list:\n",
        "    if (elements=='Rest'):\n",
        "      rest_num+=1\n",
        "  if (rest_num<=0.5):\n",
        "    boolean_continuing_rhythm=1\n",
        "  return boolean_continuing_rhythm\n",
        "\n",
        "def contour_to_label(contour):\n",
        "  labels=[]\n",
        "  totnum=len(contour[2]) #실 음의 갯수이다.\n",
        "  exception_range=(totnum-1)//4\n",
        "  exception_range2=(totnum-1)//3\n",
        "  if (len(contour[2])<2.5):\n",
        "    labels.append('resting')\n",
        "    return labels\n",
        "  else:\n",
        "    if (is_repeating(contour[2],exception_range2)):\n",
        "      labels.append('repeating')\n",
        "\n",
        "    if (is_up_steping(contour[2],exception_range)):\n",
        "      if (len(contour[2])>3):\n",
        "        labels.append('up_steping')\n",
        "\n",
        "    if (is_down_steping(contour[2],exception_range)):\n",
        "      if (len(contour[2])>3):\n",
        "        labels.append('down_steping')\n",
        "\n",
        "    if (is_up_leaping(contour[2],exception_range2)):\n",
        "      labels.append('up_leaping')\n",
        "    \n",
        "    if (is_down_leaping(contour[2],exception_range2)):\n",
        "      labels.append('down_leaping')\n",
        "\n",
        "    if (is_steping_twisting(contour[2],exception_range2)):\n",
        "      if (len(contour[2])>3):\n",
        "        labels.append('steping_twisting')\n",
        "\n",
        "    if (is_leaping_twisting(contour[2],exception_range2)):\n",
        "      if (len(contour[2])>3):\n",
        "        labels.append('leaping_twisting')\n",
        "\n",
        "    if (len(contour[2])>8.5):\n",
        "      labels.append('fast_rhythm')\n",
        "\n",
        "    if (is_one_rhythm(contour[3],exception_range)):\n",
        "      labels.append('One_rhythm')\n",
        "\n",
        "    if (is_triplet(contour[3],exception_range2)):\n",
        "      labels.append('triplet')\n",
        "\n",
        "    if (is_staccato(contour[4],exception_range2)):\n",
        "      labels.append('staccato')  \n",
        "\n",
        "    if (is_continuing_rhythm(contour[0])):\n",
        "      labels.append('continuing_rhythm')  \n",
        "\n",
        "  \"\"\"\n",
        "  if (len(labels)==0):\n",
        "    labels.append('no skills')\n",
        "    meanless.\n",
        "  \"\"\"\n",
        "\n",
        "  return labels"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez9WMZruqLA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bar_label_list=copy.deepcopy(bar_contour_list)\n",
        "for i,songs in enumerate(bar_contour_list):\n",
        "  for j,contour in enumerate(songs):\n",
        "    label=contour_to_label(contour) \n",
        "    bar_label_list[i][j]=label"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqUWmjlproso",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "496b53e9-a8dc-426e-a466-6dab477d23ca"
      },
      "source": [
        "for j in range(len(bar_label_list[0])):\n",
        "  print(bar_label_list[0][j])\n",
        "  print(bar_contour_list[0][j])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[['Starting_Point', '7.0', 'Rest', '-3.0', '-2.0', '-2.0', '-2.0', '7.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5], ['Starting_Point', '7.0', '-3.0', '-2.0', '-2.0', '-2.0', '7.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], [0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]]\n",
            "[]\n",
            "[['Starting_Point', '7.0', 'Rest', '-3.0', '-2.0', '-2.0', '-2.0', '7.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5], ['Starting_Point', '7.0', '-3.0', '-2.0', '-2.0', '-2.0', '7.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], [0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]]\n",
            "['repeating']\n",
            "[['Starting_Point', '7.0', 'Rest', '-7.0', '0.0', '0.0', '0.0', '0.0'], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0], ['Starting_Point', '7.0', '-7.0', '0.0', '0.0', '0.0', '0.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], [0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0]]\n",
            "['leaping_twisting', 'continuing_rhythm']\n",
            "[['Starting_Point', '7.0', '-7.0', '4.0', '3.0', '3.0', '-5.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], ['Starting_Point', '7.0', '-7.0', '4.0', '3.0', '3.0', '-5.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0]]\n",
            "['leaping_twisting']\n",
            "[['Starting_Point', '7.0', '-7.0', '7.0', '3.0', '-5.0', 'Rest'], [0.5, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5], ['Starting_Point', '7.0', '-7.0', '7.0', '3.0', '-5.0'], [0.5, 1.0, 0.5, 0.5, 1.0, 1.5], [0.5, 1.0, 0.5, 0.5, 1.0, 1.0]]\n",
            "['leaping_twisting']\n",
            "[['Starting_Point', '7.0', '-7.0', '7.0', '3.0', '-5.0', 'Rest'], [0.5, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5], ['Starting_Point', '7.0', '-7.0', '7.0', '3.0', '-5.0'], [0.5, 1.0, 0.5, 0.5, 1.0, 1.5], [0.5, 1.0, 0.5, 0.5, 1.0, 1.0]]\n",
            "['leaping_twisting']\n",
            "[['Starting_Point', '7.0', '-7.0', '7.0', '3.0', '-5.0', 'Rest'], [0.5, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5], ['Starting_Point', '7.0', '-7.0', '7.0', '3.0', '-5.0'], [0.5, 1.0, 0.5, 0.5, 1.0, 1.5], [0.5, 1.0, 0.5, 0.5, 1.0, 1.0]]\n",
            "[]\n",
            "[['Starting_Point', '0.0', 'Rest', '-5.0', '5.0', '-2.0', '-1.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5], ['Starting_Point', '0.0', '-5.0', '5.0', '-2.0', '-1.0'], [0.5, 1.0, 0.5, 1.0, 1.0, 1.0], [0.5, 0.5, 0.5, 1.0, 1.0, 0.5]]\n",
            "[]\n",
            "[['Starting_Point', '0.0', 'Rest', '-5.0', '5.0', '-2.0', '-1.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5], ['Starting_Point', '0.0', '-5.0', '5.0', '-2.0', '-1.0'], [0.5, 1.0, 0.5, 1.0, 1.0, 1.0], [0.5, 0.5, 0.5, 1.0, 1.0, 0.5]]\n",
            "[]\n",
            "[['Starting_Point', '0.0', 'Rest', '-5.0', '5.0', '-2.0', '-1.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5], ['Starting_Point', '0.0', '-5.0', '5.0', '-2.0', '-1.0'], [0.5, 1.0, 0.5, 1.0, 1.0, 1.0], [0.5, 0.5, 0.5, 1.0, 1.0, 0.5]]\n",
            "[]\n",
            "[['Starting_Point', '0.0', 'Rest', '-5.0', '5.0', '0.0', 'Rest', '0.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 0.5], ['Starting_Point', '0.0', '-5.0', '5.0', '0.0', '0.0'], [0.5, 1.0, 0.5, 1.0, 1.0, 1.0], [0.5, 0.5, 0.5, 1.0, 0.5, 0.5]]\n",
            "['leaping_twisting']\n",
            "[['Starting_Point', '-5.0', 'Rest', '-7.0', '3.0', '4.0', '-2.0', '7.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5], ['Starting_Point', '-5.0', '-7.0', '3.0', '4.0', '-2.0', '7.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], [0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]]\n",
            "['leaping_twisting']\n",
            "[['Starting_Point', '-5.0', 'Rest', '-7.0', '3.0', '4.0', '-2.0', '7.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5], ['Starting_Point', '-5.0', '-7.0', '3.0', '4.0', '-2.0', '7.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], [0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]]\n",
            "['leaping_twisting']\n",
            "[['Starting_Point', '-5.0', 'Rest', '-7.0', '3.0', '4.0', '-2.0', '7.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5], ['Starting_Point', '-5.0', '-7.0', '3.0', '4.0', '-2.0', '7.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], [0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]]\n",
            "['leaping_twisting']\n",
            "[['Starting_Point', '-5.0', 'Rest', '-7.0', '3.0', '4.0', '-2.0', '7.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5], ['Starting_Point', '-5.0', '-7.0', '3.0', '4.0', '-2.0', '7.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], [0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]]\n",
            "['leaping_twisting']\n",
            "[['Starting_Point', '0.0', 'Rest', '0.0', '7.0', '-7.0', '10.0', '-5.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5], ['Starting_Point', '0.0', '0.0', '7.0', '-7.0', '10.0', '-5.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], [0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]]\n",
            "['down_leaping', 'leaping_twisting']\n",
            "[['Starting_Point', '-5.0', 'Rest', '-7.0', '3.0', '4.0', '5.0', '-5.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5], ['Starting_Point', '-5.0', '-7.0', '3.0', '4.0', '5.0', '-5.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], [0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eCyCmgXzbSr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "b1732c53-6a04-4b71-edec-70f9eb47e82d"
      },
      "source": [
        "#Summary\n",
        "print(bar_contour_list[0][0])\n",
        "print(bar_label_list[0][0])\n",
        "print(bar_list[0][0])\n",
        "\n",
        "H=bar_matrix_list3[0][0]\n",
        "fig = plt.figure(figsize=(6, 3.2))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_title('colorMap')\n",
        "plt.imshow(H)\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
        "cax.get_xaxis().set_visible(False)\n",
        "cax.get_yaxis().set_visible(False)\n",
        "cax.patch.set_alpha(0)\n",
        "cax.set_frame_on(False)\n",
        "plt.colorbar(orientation='vertical')\n",
        "plt.show()\n",
        "#위 for문 없이 돌리면 1 bar만 나옴\n",
        "#matrix_list2를 CNN하되 라벨을 label로 가져가면 될듯 bar_label_list[i][j]는 i번째곡의 j번째 bar를 의미하고, 나머지도 같다."
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Starting_Point', '7.0', 'Rest', '-3.0', '-2.0', '-2.0', '-2.0', '7.0', 'Rest'], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5], ['Starting_Point', '7.0', '-3.0', '-2.0', '-2.0', '-2.0', '7.0'], [0.5, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0], [0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]]\n",
            "[]\n",
            "[[ 0.  48.  48.   0.5  0. ]\n",
            " [ 0.5 55.  55.   0.5  0. ]\n",
            " [ 1.5 52.  52.   0.5  0. ]\n",
            " [ 2.  50.  50.   0.5  0. ]\n",
            " [ 2.5 48.  48.   0.5  0. ]\n",
            " [ 3.  46.  46.   1.   0. ]\n",
            " [ 4.  53.  53.   0.5  0. ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x230.4 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndQru9Cv1XIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_matrix=[]\n",
        "all_labels=[]\n",
        "all_updown_labels=[]\n",
        "for songs in bar_label_list:\n",
        "  for label in songs:\n",
        "    label=np.array(label)\n",
        "    all_labels.append(label)\n",
        "bar_label_list=[]#램 터짐\n",
        "for songs in bar_matrix_list3:\n",
        "  for matrix in songs:\n",
        "    matrix=matrix.reshape(24,24,1)\n",
        "    all_matrix.append(matrix)\n",
        "for songs in bar_updown_list:\n",
        "  for label in songs:\n",
        "    all_updown_labels.append(label)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLD6jvcH2thx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "980e2e10-e766-42fa-b1d5-0713eaf56ab3"
      },
      "source": [
        "print(len(all_matrix),len(all_labels),len(all_updown_labels))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "175857 175857 175857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxnqzorC35O4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2e0654d-0b16-40e3-b4d0-b5921b6424fe"
      },
      "source": [
        "import keras.backend.tensorflow_backend as tfback\n",
        "from tensorflow.python.client import device_lib\n",
        "def _get_available_gpus():\n",
        "  if tfback._LOCAL_DEVICES is None:\n",
        "    devices = device_lib.list_local_devices()\n",
        "    tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
        "  return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
        "tfback._get_available_gpus = _get_available_gpus\n",
        "tfback._get_available_gpus()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt2exWUggwA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "train_matrix=np.array(all_matrix[:150000])\n",
        "train_label=np.array(all_labels[:150000])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc4SuOnd9wC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_labels():\n",
        "  labels=[]\n",
        "  label_tuple=[]\n",
        "  skills_pitch=['repeating','up_steping','down_steping','up_leaping','down_leaping','steping_twisting','leaping_twisting','dummy']\n",
        "  skills_timing=['resting','fast_rhythm','dummy']\n",
        "  skills_triplet=['triplet','dummy']\n",
        "  skills_one_rhythm=['One_rhythm','dummy']\n",
        "  skills_staccato=['staccato','continuing_rhythm','dummy']\n",
        "  for pitch in skills_pitch:\n",
        "    for timing in skills_timing:\n",
        "      for triplet in skills_triplet:\n",
        "        for one_rhythm in skills_one_rhythm:\n",
        "          for staccato in skills_staccato:\n",
        "            label_tuple=[]\n",
        "            if pitch is not 'dummy':\n",
        "              label_tuple.append(pitch)\n",
        "            if timing is not 'dummy':\n",
        "              label_tuple.append(timing)\n",
        "            if triplet is not 'dummy':\n",
        "              label_tuple.append(triplet)\n",
        "            if one_rhythm is not 'dummy':\n",
        "              label_tuple.append(one_rhythm)\n",
        "            if staccato is not 'dummy':\n",
        "              label_tuple.append(staccato)\n",
        "            label_tuple=tuple(label_tuple)\n",
        "            labels.append(label_tuple)\n",
        "  return labels"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYOhSlmHA1ka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ab172d5b-49dc-440a-f65b-6a7f91883779"
      },
      "source": [
        "label=set_labels()\n",
        "print(label)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('repeating', 'resting', 'triplet', 'One_rhythm', 'staccato'), ('repeating', 'resting', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('repeating', 'resting', 'triplet', 'One_rhythm'), ('repeating', 'resting', 'triplet', 'staccato'), ('repeating', 'resting', 'triplet', 'continuing_rhythm'), ('repeating', 'resting', 'triplet'), ('repeating', 'resting', 'One_rhythm', 'staccato'), ('repeating', 'resting', 'One_rhythm', 'continuing_rhythm'), ('repeating', 'resting', 'One_rhythm'), ('repeating', 'resting', 'staccato'), ('repeating', 'resting', 'continuing_rhythm'), ('repeating', 'resting'), ('repeating', 'fast_rhythm', 'triplet', 'One_rhythm', 'staccato'), ('repeating', 'fast_rhythm', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('repeating', 'fast_rhythm', 'triplet', 'One_rhythm'), ('repeating', 'fast_rhythm', 'triplet', 'staccato'), ('repeating', 'fast_rhythm', 'triplet', 'continuing_rhythm'), ('repeating', 'fast_rhythm', 'triplet'), ('repeating', 'fast_rhythm', 'One_rhythm', 'staccato'), ('repeating', 'fast_rhythm', 'One_rhythm', 'continuing_rhythm'), ('repeating', 'fast_rhythm', 'One_rhythm'), ('repeating', 'fast_rhythm', 'staccato'), ('repeating', 'fast_rhythm', 'continuing_rhythm'), ('repeating', 'fast_rhythm'), ('repeating', 'triplet', 'One_rhythm', 'staccato'), ('repeating', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('repeating', 'triplet', 'One_rhythm'), ('repeating', 'triplet', 'staccato'), ('repeating', 'triplet', 'continuing_rhythm'), ('repeating', 'triplet'), ('repeating', 'One_rhythm', 'staccato'), ('repeating', 'One_rhythm', 'continuing_rhythm'), ('repeating', 'One_rhythm'), ('repeating', 'staccato'), ('repeating', 'continuing_rhythm'), ('repeating',), ('up_steping', 'resting', 'triplet', 'One_rhythm', 'staccato'), ('up_steping', 'resting', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('up_steping', 'resting', 'triplet', 'One_rhythm'), ('up_steping', 'resting', 'triplet', 'staccato'), ('up_steping', 'resting', 'triplet', 'continuing_rhythm'), ('up_steping', 'resting', 'triplet'), ('up_steping', 'resting', 'One_rhythm', 'staccato'), ('up_steping', 'resting', 'One_rhythm', 'continuing_rhythm'), ('up_steping', 'resting', 'One_rhythm'), ('up_steping', 'resting', 'staccato'), ('up_steping', 'resting', 'continuing_rhythm'), ('up_steping', 'resting'), ('up_steping', 'fast_rhythm', 'triplet', 'One_rhythm', 'staccato'), ('up_steping', 'fast_rhythm', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('up_steping', 'fast_rhythm', 'triplet', 'One_rhythm'), ('up_steping', 'fast_rhythm', 'triplet', 'staccato'), ('up_steping', 'fast_rhythm', 'triplet', 'continuing_rhythm'), ('up_steping', 'fast_rhythm', 'triplet'), ('up_steping', 'fast_rhythm', 'One_rhythm', 'staccato'), ('up_steping', 'fast_rhythm', 'One_rhythm', 'continuing_rhythm'), ('up_steping', 'fast_rhythm', 'One_rhythm'), ('up_steping', 'fast_rhythm', 'staccato'), ('up_steping', 'fast_rhythm', 'continuing_rhythm'), ('up_steping', 'fast_rhythm'), ('up_steping', 'triplet', 'One_rhythm', 'staccato'), ('up_steping', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('up_steping', 'triplet', 'One_rhythm'), ('up_steping', 'triplet', 'staccato'), ('up_steping', 'triplet', 'continuing_rhythm'), ('up_steping', 'triplet'), ('up_steping', 'One_rhythm', 'staccato'), ('up_steping', 'One_rhythm', 'continuing_rhythm'), ('up_steping', 'One_rhythm'), ('up_steping', 'staccato'), ('up_steping', 'continuing_rhythm'), ('up_steping',), ('down_steping', 'resting', 'triplet', 'One_rhythm', 'staccato'), ('down_steping', 'resting', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('down_steping', 'resting', 'triplet', 'One_rhythm'), ('down_steping', 'resting', 'triplet', 'staccato'), ('down_steping', 'resting', 'triplet', 'continuing_rhythm'), ('down_steping', 'resting', 'triplet'), ('down_steping', 'resting', 'One_rhythm', 'staccato'), ('down_steping', 'resting', 'One_rhythm', 'continuing_rhythm'), ('down_steping', 'resting', 'One_rhythm'), ('down_steping', 'resting', 'staccato'), ('down_steping', 'resting', 'continuing_rhythm'), ('down_steping', 'resting'), ('down_steping', 'fast_rhythm', 'triplet', 'One_rhythm', 'staccato'), ('down_steping', 'fast_rhythm', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('down_steping', 'fast_rhythm', 'triplet', 'One_rhythm'), ('down_steping', 'fast_rhythm', 'triplet', 'staccato'), ('down_steping', 'fast_rhythm', 'triplet', 'continuing_rhythm'), ('down_steping', 'fast_rhythm', 'triplet'), ('down_steping', 'fast_rhythm', 'One_rhythm', 'staccato'), ('down_steping', 'fast_rhythm', 'One_rhythm', 'continuing_rhythm'), ('down_steping', 'fast_rhythm', 'One_rhythm'), ('down_steping', 'fast_rhythm', 'staccato'), ('down_steping', 'fast_rhythm', 'continuing_rhythm'), ('down_steping', 'fast_rhythm'), ('down_steping', 'triplet', 'One_rhythm', 'staccato'), ('down_steping', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('down_steping', 'triplet', 'One_rhythm'), ('down_steping', 'triplet', 'staccato'), ('down_steping', 'triplet', 'continuing_rhythm'), ('down_steping', 'triplet'), ('down_steping', 'One_rhythm', 'staccato'), ('down_steping', 'One_rhythm', 'continuing_rhythm'), ('down_steping', 'One_rhythm'), ('down_steping', 'staccato'), ('down_steping', 'continuing_rhythm'), ('down_steping',), ('up_leaping', 'resting', 'triplet', 'One_rhythm', 'staccato'), ('up_leaping', 'resting', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('up_leaping', 'resting', 'triplet', 'One_rhythm'), ('up_leaping', 'resting', 'triplet', 'staccato'), ('up_leaping', 'resting', 'triplet', 'continuing_rhythm'), ('up_leaping', 'resting', 'triplet'), ('up_leaping', 'resting', 'One_rhythm', 'staccato'), ('up_leaping', 'resting', 'One_rhythm', 'continuing_rhythm'), ('up_leaping', 'resting', 'One_rhythm'), ('up_leaping', 'resting', 'staccato'), ('up_leaping', 'resting', 'continuing_rhythm'), ('up_leaping', 'resting'), ('up_leaping', 'fast_rhythm', 'triplet', 'One_rhythm', 'staccato'), ('up_leaping', 'fast_rhythm', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('up_leaping', 'fast_rhythm', 'triplet', 'One_rhythm'), ('up_leaping', 'fast_rhythm', 'triplet', 'staccato'), ('up_leaping', 'fast_rhythm', 'triplet', 'continuing_rhythm'), ('up_leaping', 'fast_rhythm', 'triplet'), ('up_leaping', 'fast_rhythm', 'One_rhythm', 'staccato'), ('up_leaping', 'fast_rhythm', 'One_rhythm', 'continuing_rhythm'), ('up_leaping', 'fast_rhythm', 'One_rhythm'), ('up_leaping', 'fast_rhythm', 'staccato'), ('up_leaping', 'fast_rhythm', 'continuing_rhythm'), ('up_leaping', 'fast_rhythm'), ('up_leaping', 'triplet', 'One_rhythm', 'staccato'), ('up_leaping', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('up_leaping', 'triplet', 'One_rhythm'), ('up_leaping', 'triplet', 'staccato'), ('up_leaping', 'triplet', 'continuing_rhythm'), ('up_leaping', 'triplet'), ('up_leaping', 'One_rhythm', 'staccato'), ('up_leaping', 'One_rhythm', 'continuing_rhythm'), ('up_leaping', 'One_rhythm'), ('up_leaping', 'staccato'), ('up_leaping', 'continuing_rhythm'), ('up_leaping',), ('down_leaping', 'resting', 'triplet', 'One_rhythm', 'staccato'), ('down_leaping', 'resting', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('down_leaping', 'resting', 'triplet', 'One_rhythm'), ('down_leaping', 'resting', 'triplet', 'staccato'), ('down_leaping', 'resting', 'triplet', 'continuing_rhythm'), ('down_leaping', 'resting', 'triplet'), ('down_leaping', 'resting', 'One_rhythm', 'staccato'), ('down_leaping', 'resting', 'One_rhythm', 'continuing_rhythm'), ('down_leaping', 'resting', 'One_rhythm'), ('down_leaping', 'resting', 'staccato'), ('down_leaping', 'resting', 'continuing_rhythm'), ('down_leaping', 'resting'), ('down_leaping', 'fast_rhythm', 'triplet', 'One_rhythm', 'staccato'), ('down_leaping', 'fast_rhythm', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('down_leaping', 'fast_rhythm', 'triplet', 'One_rhythm'), ('down_leaping', 'fast_rhythm', 'triplet', 'staccato'), ('down_leaping', 'fast_rhythm', 'triplet', 'continuing_rhythm'), ('down_leaping', 'fast_rhythm', 'triplet'), ('down_leaping', 'fast_rhythm', 'One_rhythm', 'staccato'), ('down_leaping', 'fast_rhythm', 'One_rhythm', 'continuing_rhythm'), ('down_leaping', 'fast_rhythm', 'One_rhythm'), ('down_leaping', 'fast_rhythm', 'staccato'), ('down_leaping', 'fast_rhythm', 'continuing_rhythm'), ('down_leaping', 'fast_rhythm'), ('down_leaping', 'triplet', 'One_rhythm', 'staccato'), ('down_leaping', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('down_leaping', 'triplet', 'One_rhythm'), ('down_leaping', 'triplet', 'staccato'), ('down_leaping', 'triplet', 'continuing_rhythm'), ('down_leaping', 'triplet'), ('down_leaping', 'One_rhythm', 'staccato'), ('down_leaping', 'One_rhythm', 'continuing_rhythm'), ('down_leaping', 'One_rhythm'), ('down_leaping', 'staccato'), ('down_leaping', 'continuing_rhythm'), ('down_leaping',), ('steping_twisting', 'resting', 'triplet', 'One_rhythm', 'staccato'), ('steping_twisting', 'resting', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('steping_twisting', 'resting', 'triplet', 'One_rhythm'), ('steping_twisting', 'resting', 'triplet', 'staccato'), ('steping_twisting', 'resting', 'triplet', 'continuing_rhythm'), ('steping_twisting', 'resting', 'triplet'), ('steping_twisting', 'resting', 'One_rhythm', 'staccato'), ('steping_twisting', 'resting', 'One_rhythm', 'continuing_rhythm'), ('steping_twisting', 'resting', 'One_rhythm'), ('steping_twisting', 'resting', 'staccato'), ('steping_twisting', 'resting', 'continuing_rhythm'), ('steping_twisting', 'resting'), ('steping_twisting', 'fast_rhythm', 'triplet', 'One_rhythm', 'staccato'), ('steping_twisting', 'fast_rhythm', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('steping_twisting', 'fast_rhythm', 'triplet', 'One_rhythm'), ('steping_twisting', 'fast_rhythm', 'triplet', 'staccato'), ('steping_twisting', 'fast_rhythm', 'triplet', 'continuing_rhythm'), ('steping_twisting', 'fast_rhythm', 'triplet'), ('steping_twisting', 'fast_rhythm', 'One_rhythm', 'staccato'), ('steping_twisting', 'fast_rhythm', 'One_rhythm', 'continuing_rhythm'), ('steping_twisting', 'fast_rhythm', 'One_rhythm'), ('steping_twisting', 'fast_rhythm', 'staccato'), ('steping_twisting', 'fast_rhythm', 'continuing_rhythm'), ('steping_twisting', 'fast_rhythm'), ('steping_twisting', 'triplet', 'One_rhythm', 'staccato'), ('steping_twisting', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('steping_twisting', 'triplet', 'One_rhythm'), ('steping_twisting', 'triplet', 'staccato'), ('steping_twisting', 'triplet', 'continuing_rhythm'), ('steping_twisting', 'triplet'), ('steping_twisting', 'One_rhythm', 'staccato'), ('steping_twisting', 'One_rhythm', 'continuing_rhythm'), ('steping_twisting', 'One_rhythm'), ('steping_twisting', 'staccato'), ('steping_twisting', 'continuing_rhythm'), ('steping_twisting',), ('leaping_twisting', 'resting', 'triplet', 'One_rhythm', 'staccato'), ('leaping_twisting', 'resting', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('leaping_twisting', 'resting', 'triplet', 'One_rhythm'), ('leaping_twisting', 'resting', 'triplet', 'staccato'), ('leaping_twisting', 'resting', 'triplet', 'continuing_rhythm'), ('leaping_twisting', 'resting', 'triplet'), ('leaping_twisting', 'resting', 'One_rhythm', 'staccato'), ('leaping_twisting', 'resting', 'One_rhythm', 'continuing_rhythm'), ('leaping_twisting', 'resting', 'One_rhythm'), ('leaping_twisting', 'resting', 'staccato'), ('leaping_twisting', 'resting', 'continuing_rhythm'), ('leaping_twisting', 'resting'), ('leaping_twisting', 'fast_rhythm', 'triplet', 'One_rhythm', 'staccato'), ('leaping_twisting', 'fast_rhythm', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('leaping_twisting', 'fast_rhythm', 'triplet', 'One_rhythm'), ('leaping_twisting', 'fast_rhythm', 'triplet', 'staccato'), ('leaping_twisting', 'fast_rhythm', 'triplet', 'continuing_rhythm'), ('leaping_twisting', 'fast_rhythm', 'triplet'), ('leaping_twisting', 'fast_rhythm', 'One_rhythm', 'staccato'), ('leaping_twisting', 'fast_rhythm', 'One_rhythm', 'continuing_rhythm'), ('leaping_twisting', 'fast_rhythm', 'One_rhythm'), ('leaping_twisting', 'fast_rhythm', 'staccato'), ('leaping_twisting', 'fast_rhythm', 'continuing_rhythm'), ('leaping_twisting', 'fast_rhythm'), ('leaping_twisting', 'triplet', 'One_rhythm', 'staccato'), ('leaping_twisting', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('leaping_twisting', 'triplet', 'One_rhythm'), ('leaping_twisting', 'triplet', 'staccato'), ('leaping_twisting', 'triplet', 'continuing_rhythm'), ('leaping_twisting', 'triplet'), ('leaping_twisting', 'One_rhythm', 'staccato'), ('leaping_twisting', 'One_rhythm', 'continuing_rhythm'), ('leaping_twisting', 'One_rhythm'), ('leaping_twisting', 'staccato'), ('leaping_twisting', 'continuing_rhythm'), ('leaping_twisting',), ('resting', 'triplet', 'One_rhythm', 'staccato'), ('resting', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('resting', 'triplet', 'One_rhythm'), ('resting', 'triplet', 'staccato'), ('resting', 'triplet', 'continuing_rhythm'), ('resting', 'triplet'), ('resting', 'One_rhythm', 'staccato'), ('resting', 'One_rhythm', 'continuing_rhythm'), ('resting', 'One_rhythm'), ('resting', 'staccato'), ('resting', 'continuing_rhythm'), ('resting',), ('fast_rhythm', 'triplet', 'One_rhythm', 'staccato'), ('fast_rhythm', 'triplet', 'One_rhythm', 'continuing_rhythm'), ('fast_rhythm', 'triplet', 'One_rhythm'), ('fast_rhythm', 'triplet', 'staccato'), ('fast_rhythm', 'triplet', 'continuing_rhythm'), ('fast_rhythm', 'triplet'), ('fast_rhythm', 'One_rhythm', 'staccato'), ('fast_rhythm', 'One_rhythm', 'continuing_rhythm'), ('fast_rhythm', 'One_rhythm'), ('fast_rhythm', 'staccato'), ('fast_rhythm', 'continuing_rhythm'), ('fast_rhythm',), ('triplet', 'One_rhythm', 'staccato'), ('triplet', 'One_rhythm', 'continuing_rhythm'), ('triplet', 'One_rhythm'), ('triplet', 'staccato'), ('triplet', 'continuing_rhythm'), ('triplet',), ('One_rhythm', 'staccato'), ('One_rhythm', 'continuing_rhythm'), ('One_rhythm',), ('staccato',), ('continuing_rhythm',), ()]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93yFDiw86QyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_matrix=np.array(all_matrix[150000:170000])\n",
        "valid_label=np.array(all_labels[150000:170000])\n",
        "test_matrix=np.array(all_matrix[170000:])\n",
        "test_label=np.array(all_labels[170000:])#어쩌면 쓸수도?\n",
        "mlb=MultiLabelBinarizer()\n",
        "labels=set_labels()\n",
        "mlb.fit(labels)\n",
        "train_label2=mlb.transform(train_label)\n",
        "valid_label2=mlb.transform(valid_label)\n",
        "test_label2=mlb.transform(test_label)\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPUsl9deYgNl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0726e23-8391-4b65-c1c9-0bf4fe2e13f7"
      },
      "source": [
        "print(train_label2.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150000, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MoUdnPf62cX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "updownle=LabelBinarizer()\n",
        "updownle.fit(['up','down','final','meanless'])\n",
        "updown_label=updownle.transform(np.array(all_updown_labels))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfUeYxwX8eVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_updown_label=updown_label[:150000]\n",
        "valid_updown_label=updown_label[150000:170000]\n",
        "test_updown_label=updown_label[170000:]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flwJh4SmmzuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_matrix=[]\n",
        "all_labels=[]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb3xLrR99Rk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "aced445c-3a6a-410d-a10b-c1a71def29cb"
      },
      "source": [
        "print(train_matrix.shape)\n",
        "print(train_label2)\n",
        "print(valid_matrix.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150000, 24, 24, 1)\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "(20000, 24, 24, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkGYxpDB8ky9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.applications\n",
        "from keras import regularizers\n",
        "from keras.applications import VGG16\n",
        "from keras.applications import resnet50\n",
        "from keras import layers\n",
        "from tensorflow import keras\n",
        "def residual_block(filter, input, add=True):\n",
        "  with tf.device('/gpu:0'):\n",
        "    layer_1 = keras.layers.Conv2D(filters=filter//4, kernel_size=(1, 1), data_format=\"channels_first\")(input)\n",
        "    layer_2 = keras.layers.Conv2D(filters=filter//4, kernel_size=(3, 3), padding='same', data_format=\"channels_first\", kernel_regularizer=keras.regularizers.l2(0.001))(layer_1)\n",
        "    layer_2 = keras.layers.BatchNormalization()(layer_2)\n",
        "    layer_2 = keras.layers.ReLU()(layer_2)\n",
        "    layer_3 = keras.layers.Conv2D(filters=filter, kernel_size=(1, 1), data_format=\"channels_first\")(layer_2)\n",
        "    layer_3 = keras.layers.BatchNormalization()(layer_3)\n",
        "    if add:\n",
        "        layer_3 = keras.layers.add([input, layer_3])\n",
        "    layer_3 = keras.layers.ReLU()(layer_3)\n",
        "    return layer_3\n",
        "def make_model():\n",
        "  with tf.device('/gpu:0'):\n",
        "    input_layer = keras.Input(shape=(24, 24, 1))\n",
        "    layer_1 = keras.layers.Conv2D(filters=64, kernel_size=(7, 7), padding='same', data_format=\"channels_first\")(input_layer)\n",
        "    block_1 = residual_block(64, layer_1)\n",
        "    #block_2 = residual_block(64, block_1)\n",
        "    #block_3 = residual_block(64, block_2)\n",
        "    pooling_layer = keras.layers.MaxPool2D((2, 2),padding='same', data_format=\"channels_first\")(block_1)\n",
        "    block_4 = residual_block(128, pooling_layer, add=False)\n",
        "    block_5 = residual_block(128, block_4)\n",
        "    #block_6 = residual_block(128, block_5)\n",
        "    pooling_layer2 = keras.layers.MaxPool2D(padding='same',pool_size=(2, 2), data_format=\"channels_first\")(block_4)\n",
        "    block_7 = residual_block(256, pooling_layer2, add=False)\n",
        "    block_8 = residual_block(256, block_7)\n",
        "    block_9 = residual_block(256, block_8)\n",
        "    #pooling_layer4 = keras.layers.MaxPool2D(pool_size=(2, 2), data_format=\"channels_first\")(block_7)\n",
        "    #block_10 = residual_block(256, pooling_layer4)\n",
        "    #block_11 = residual_block(512, block_10)\n",
        "    pooling_layer3 = keras.layers.AvgPool2D(padding='same',pool_size=(8, 8), data_format=\"channels_first\")(block_7)\n",
        "    last_layer = keras.layers.Flatten()(pooling_layer3)\n",
        "    last_layer = keras.layers.Dropout(0.4)(last_layer)\n",
        "    last_layer = keras.layers.Dense(13, activation=\"sigmoid\")(last_layer)\n",
        "    return keras.models.Model(inputs=input_layer, outputs=last_layer)\n",
        "def make_classifier():\n",
        "  with tf.device('/gpu:0'):\n",
        "    classifier = keras.Sequential()\n",
        "    classifier.add(keras.layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1), padding='same',\n",
        "                  activation='relu',\n",
        "                  input_shape=(24,24,1)))\n",
        "    classifier.add(keras.layers.BatchNormalization())\n",
        "    classifier.add(keras.layers.LeakyReLU(alpha=0.01))\n",
        "    classifier.add(keras.layers.Conv2D(128, (2, 2), activation='relu', padding='same'))\n",
        "    classifier.add(keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    classifier.add(keras.layers.Conv2D(256, (2, 2), padding='same'))\n",
        "    classifier.add(keras.layers.LeakyReLU(alpha=0.01))\n",
        "    classifier.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    classifier.add(keras.layers.Flatten())\n",
        "    classifier.add(keras.layers.Dropout(0.25))\n",
        "    classifier.add(keras.layers.Dense(4, activation='sigmoid'))\n",
        "  return classifier"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn-X5lReh2ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "model_path = '/content/drive/My Drive/models/' + 'deeperppddbest.h5'\n",
        "\n",
        "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_acc',\n",
        "                                verbose=1, save_best_only=True)\n",
        "callbacks = [cb_checkpoint]\n",
        "\n",
        "model_path = '/content/drive/My Drive/models/' + 'updown.h5'\n",
        "\n",
        "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_acc',\n",
        "                                verbose=1, save_best_only=True)\n",
        "updown_callbacks=[cb_checkpoint]\n",
        "#tf.keras의 경우(make_model) monitor='val_acc'로, keras의 경우(make_classifier) monitor='val_accuracy'로 해야한다."
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf1jJnIxemG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "def recall(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
        "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
        "\n",
        "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
        "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
        "\n",
        "    # Precision = (True Positive) / (True Positive + False Positive)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1score(y_target, y_pred):\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
        "    \n",
        "    # return a single tensor value\n",
        "    return _f1score\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Nx8Ygs-I6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "c46ed1ee-d356-4923-a868-9ab5ce9a3afe"
      },
      "source": [
        "from tensorflow import keras\n",
        "classifier=make_model()\n",
        "classifier.compile(loss=keras.losses.BinaryCrossentropy(\n",
        "      from_logits=False, label_smoothing=0.1, \n",
        "      name='binary_crossentropy'\n",
        "  ), optimizer='adam', metrics=['accuracy',recall,precision,f1score])\n",
        "\"\"\"\n",
        "hist=classifier.fit(\n",
        "      train_matrix,train_label2,batch_size=256,\n",
        "      epochs=150,\n",
        "      validation_data=(valid_matrix,valid_label2),\n",
        "      callbacks=callbacks,\n",
        "  )\n",
        "\n",
        "\"\"\"\n",
        "#실제 학습\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nhist=classifier.fit(\\n      train_matrix,train_label2,batch_size=256,\\n      epochs=150,\\n      validation_data=(valid_matrix,valid_label2),\\n      callbacks=callbacks,\\n  )\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ8oiBIhX7kq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7ca0587f-fc92-478b-8c72-39e77d2529f1"
      },
      "source": [
        "#음의 흐름에 관한 분류기\n",
        "updown_classifier=make_classifier()\n",
        "updown_classifier.compile(loss=keras.losses.BinaryCrossentropy(\n",
        "      from_logits=False,\n",
        "      name='binary_crossentorpy',label_smoothing=0.1\n",
        "  ), optimizer='adam', metrics=['accuracy'])\n",
        "\"\"\"\n",
        "updown_hist=updown_classifier.fit(\n",
        "      train_matrix,train_updown_label,batch_size=256,\n",
        "      epochs=150,\n",
        "      validation_data=(valid_matrix,valid_updown_label),\n",
        "      callbacks=updown_callbacks,\n",
        "  )\n",
        "\"\"\""
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nupdown_hist=updown_classifier.fit(\\n      train_matrix,train_updown_label,batch_size=256,\\n      epochs=150,\\n      validation_data=(valid_matrix,valid_updown_label),\\n      callbacks=updown_callbacks,\\n  )\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76EtBzIJsfJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuracy')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T9qKNfNsuTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, recall_ax = plt.subplots()\n",
        "\n",
        "\n",
        "recall_ax.plot(hist.history['recall'], 'y', label='recall')\n",
        "recall_ax.plot(hist.history['val_recall'], 'r', label='valid recall')\n",
        "\n",
        "recall_ax.plot(hist.history['f1score'], 'b', label='f1score')\n",
        "recall_ax.plot(hist.history['val_f1score'], 'g', label='valid f1score')\n",
        "\n",
        "recall_ax.plot(hist.history['precision'], 'c', label='precision')\n",
        "recall_ax.plot(hist.history['val_precision'], 'k', label='valid precision')\n",
        "\n",
        "recall_ax.set_xlabel('epoch')\n",
        "recall_ax.set_ylabel('score')\n",
        "\n",
        "recall_ax.legend(loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ-K9Y2vFN2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  classifier.load_weights(\"/content/drive/My Drive/models/deeperppddbest.h5\")\n",
        "  testresult=classifier.predict(test_matrix)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2XAN8kzN3gp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "de2bf9c2-78bb-401e-a06f-cc4163d930f7"
      },
      "source": [
        "print(testresult)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.03725642 0.0403899  0.04788151 ... 0.26445758 0.08476987 0.11423087]\n",
            " [0.8777615  0.05382282 0.04724708 ... 0.07853436 0.06870478 0.03987336]\n",
            " [0.04623258 0.04350001 0.05003032 ... 0.20176557 0.04249719 0.05868679]\n",
            " ...\n",
            " [0.9727199  0.95933604 0.04430762 ... 0.04517195 0.0849402  0.0529477 ]\n",
            " [0.9459064  0.9236393  0.09470636 ... 0.04856196 0.86422026 0.04754475]\n",
            " [0.06226301 0.02835664 0.07952607 ... 0.07468611 0.22929463 0.06059822]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFucJZONFnqX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b81cb88f-19ac-4e1c-de7f-5486795478f4"
      },
      "source": [
        "def get_tag_results(testresult,test_label2):  \n",
        "  classnum={}\n",
        "  testnum={}\n",
        "  resultmat=[]\n",
        "  bestmat=[]\n",
        "  for i in range(len(testresult)):\n",
        "    eval_result=[0 for i in range(13)]\n",
        "    best_result=[0 for i in range(13)]\n",
        "    class_num=np.count_nonzero(test_label2[i]==1)+1\n",
        "    classidx=(-testresult[i]).argsort()[:class_num]\n",
        "    for k,j in enumerate(classidx):\n",
        "      if (k==0):\n",
        "        best_result[j]=1\n",
        "      eval_result[j]=1\n",
        "    resultmat.append(eval_result)\n",
        "    bestmat.append(best_result)\n",
        "    test_result2=copy.deepcopy(testresult)\n",
        "    test_result2[np.where(test_result2>0.30)]=1\n",
        "    test_result2[np.where(test_result2<=0.30)]=0\n",
        "  resultmat=np.array(resultmat)\n",
        "  bestmat=np.array(bestmat)\n",
        "  testidx=mlb.inverse_transform(resultmat)\n",
        "  classidx=mlb.inverse_transform(test_label2)\n",
        "  testidx2=mlb.inverse_transform(test_result2)\n",
        "  bestidx=mlb.inverse_transform(bestmat)\n",
        "  for i in range(len(testidx)):  \n",
        "    #print(bestidx[i],testidx2[i],testidx[i], classidx[i],i) # 값이 0.30 이상인 set, 원래 Label보다 1개 많이 보여주는 내림차순 set, 원래 label set 순서이다. \n",
        "    for classes in classidx[i]:\n",
        "      if (classes not in classnum):\n",
        "        classnum[classes]=1\n",
        "      else:\n",
        "        classnum[classes]+=1\n",
        "    for classes in testidx[i]:\n",
        "      if (classes not in testnum):\n",
        "        testnum[classes]=1\n",
        "      else:\n",
        "        testnum[classes]+=1\n",
        "  print(classnum, testnum)\n",
        "  return bestidx, testidx2, testidx, classidx#가장 높은거, 0.30이상인 set, 원래의 Label보다 1개 많이 보여주는 내림차순, 원래 Label\n",
        "best=get_tag_results(testresult,test_label2)[0]\n",
        "print(best)\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'fast_rhythm': 797, 'triplet': 1070, 'One_rhythm': 1405, 'leaping_twisting': 1903, 'resting': 666, 'down_steping': 267, 'repeating': 703, 'staccato': 1079, 'up_steping': 916, 'down_leaping': 939, 'up_leaping': 1099, 'steping_twisting': 357, 'continuing_rhythm': 1033} {'fast_rhythm': 1348, 'leaping_twisting': 2498, 'triplet': 1871, 'One_rhythm': 1806, 'down_steping': 512, 'staccato': 1928, 'resting': 811, 'repeating': 1047, 'up_leaping': 1537, 'up_steping': 1322, 'down_leaping': 1345, 'steping_twisting': 754, 'continuing_rhythm': 1312}\n",
            "[('fast_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('staccato',), ('One_rhythm',), ('resting',), ('down_steping',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('up_steping',), ('repeating',), ('staccato',), ('staccato',), ('repeating',), ('repeating',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('One_rhythm',), ('resting',), ('One_rhythm',), ('resting',), ('One_rhythm',), ('up_leaping',), ('One_rhythm',), ('up_leaping',), ('One_rhythm',), ('up_leaping',), ('One_rhythm',), ('up_leaping',), ('One_rhythm',), ('up_leaping',), ('triplet',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('up_leaping',), ('triplet',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('triplet',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('down_steping',), ('leaping_twisting',), ('down_steping',), ('leaping_twisting',), ('down_steping',), ('leaping_twisting',), ('down_steping',), ('leaping_twisting',), ('down_steping',), ('leaping_twisting',), ('down_steping',), ('leaping_twisting',), ('down_steping',), ('leaping_twisting',), ('down_steping',), ('triplet',), ('resting',), ('staccato',), ('resting',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('staccato',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('staccato',), ('up_leaping',), ('leaping_twisting',), ('resting',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('up_steping',), ('staccato',), ('triplet',), ('steping_twisting',), ('up_steping',), ('leaping_twisting',), ('fast_rhythm',), ('down_leaping',), ('up_steping',), ('fast_rhythm',), ('up_steping',), ('staccato',), ('up_steping',), ('leaping_twisting',), ('fast_rhythm',), ('down_leaping',), ('down_steping',), ('resting',), ('resting',), ('resting',), ('steping_twisting',), ('staccato',), ('One_rhythm',), ('up_leaping',), ('down_steping',), ('leaping_twisting',), ('resting',), ('resting',), ('continuing_rhythm',), ('steping_twisting',), ('steping_twisting',), ('staccato',), ('down_steping',), ('resting',), ('resting',), ('resting',), ('steping_twisting',), ('One_rhythm',), ('One_rhythm',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('down_leaping',), ('resting',), ('triplet',), ('resting',), ('triplet',), ('resting',), ('resting',), ('leaping_twisting',), ('down_steping',), ('up_steping',), ('resting',), ('down_steping',), ('resting',), ('leaping_twisting',), ('down_leaping',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('down_leaping',), ('down_leaping',), ('continuing_rhythm',), ('down_leaping',), ('down_leaping',), ('resting',), ('resting',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('continuing_rhythm',), ('down_leaping',), ('down_leaping',), ('resting',), ('resting',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('One_rhythm',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('continuing_rhythm',), ('up_leaping',), ('down_leaping',), ('down_leaping',), ('up_leaping',), ('triplet',), ('triplet',), ('triplet',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('continuing_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('resting',), ('leaping_twisting',), ('up_steping',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('resting',), ('up_steping',), ('repeating',), ('repeating',), ('repeating',), ('up_steping',), ('repeating',), ('repeating',), ('repeating',), ('down_steping',), ('repeating',), ('repeating',), ('down_steping',), ('down_steping',), ('repeating',), ('repeating',), ('down_steping',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('down_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('triplet',), ('triplet',), ('up_leaping',), ('leaping_twisting',), ('down_leaping',), ('up_steping',), ('One_rhythm',), ('triplet',), ('steping_twisting',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('down_leaping',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('down_leaping',), ('staccato',), ('staccato',), ('staccato',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('down_leaping',), ('staccato',), ('leaping_twisting',), ('staccato',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('up_steping',), ('repeating',), ('repeating',), ('One_rhythm',), ('One_rhythm',), ('repeating',), ('fast_rhythm',), ('repeating',), ('resting',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('One_rhythm',), ('resting',), ('up_steping',), ('resting',), ('up_steping',), ('resting',), ('triplet',), ('resting',), ('up_leaping',), ('resting',), ('continuing_rhythm',), ('up_leaping',), ('continuing_rhythm',), ('resting',), ('up_steping',), ('resting',), ('staccato',), ('resting',), ('staccato',), ('resting',), ('steping_twisting',), ('resting',), ('repeating',), ('up_leaping',), ('down_leaping',), ('steping_twisting',), ('down_leaping',), ('up_steping',), ('staccato',), ('repeating',), ('repeating',), ('triplet',), ('down_leaping',), ('resting',), ('repeating',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('up_steping',), ('down_steping',), ('up_steping',), ('triplet',), ('triplet',), ('triplet',), ('up_steping',), ('staccato',), ('staccato',), ('leaping_twisting',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('up_steping',), ('repeating',), ('repeating',), ('repeating',), ('One_rhythm',), ('repeating',), ('repeating',), ('leaping_twisting',), ('repeating',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('repeating',), ('repeating',), ('resting',), ('down_steping',), ('continuing_rhythm',), ('down_steping',), ('up_leaping',), ('repeating',), ('triplet',), ('down_steping',), ('up_leaping',), ('down_steping',), ('continuing_rhythm',), ('One_rhythm',), ('up_steping',), ('down_steping',), ('up_leaping',), ('triplet',), ('repeating',), ('continuing_rhythm',), ('up_steping',), ('One_rhythm',), ('triplet',), ('repeating',), ('resting',), ('triplet',), ('resting',), ('staccato',), ('resting',), ('down_leaping',), ('resting',), ('resting',), ('triplet',), ('resting',), ('up_leaping',), ('continuing_rhythm',), ('resting',), ('up_steping',), ('staccato',), ('up_steping',), ('up_steping',), ('resting',), ('resting',), ('steping_twisting',), ('resting',), ('up_leaping',), ('resting',), ('One_rhythm',), ('repeating',), ('repeating',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('up_leaping',), ('fast_rhythm',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('leaping_twisting',), ('triplet',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('down_steping',), ('down_steping',), ('up_steping',), ('up_steping',), ('triplet',), ('triplet',), ('up_steping',), ('triplet',), ('triplet',), ('up_steping',), ('up_steping',), ('up_steping',), ('down_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('triplet',), ('triplet',), ('triplet',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('resting',), ('staccato',), ('up_steping',), ('up_steping',), ('down_leaping',), ('fast_rhythm',), ('up_steping',), ('up_steping',), ('down_leaping',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('down_leaping',), ('staccato',), ('repeating',), ('up_steping',), ('repeating',), ('up_steping',), ('down_steping',), ('staccato',), ('repeating',), ('up_steping',), ('staccato',), ('up_steping',), ('repeating',), ('up_steping',), ('down_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('up_steping',), ('down_leaping',), ('staccato',), ('up_leaping',), ('leaping_twisting',), ('down_leaping',), ('staccato',), ('fast_rhythm',), ('down_steping',), ('leaping_twisting',), ('repeating',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('repeating',), ('staccato',), ('staccato',), ('down_leaping',), ('staccato',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('repeating',), ('repeating',), ('staccato',), ('staccato',), ('up_leaping',), ('down_leaping',), ('up_steping',), ('leaping_twisting',), ('resting',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('triplet',), ('staccato',), ('up_steping',), ('down_leaping',), ('staccato',), ('up_leaping',), ('up_steping',), ('repeating',), ('down_steping',), ('up_steping',), ('down_steping',), ('staccato',), ('continuing_rhythm',), ('fast_rhythm',), ('continuing_rhythm',), ('down_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('repeating',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('repeating',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('repeating',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('repeating',), ('resting',), ('leaping_twisting',), ('down_steping',), ('up_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('One_rhythm',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('staccato',), ('up_leaping',), ('resting',), ('down_leaping',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('resting',), ('One_rhythm',), ('resting',), ('up_steping',), ('up_steping',), ('up_steping',), ('down_leaping',), ('repeating',), ('repeating',), ('triplet',), ('resting',), ('repeating',), ('steping_twisting',), ('fast_rhythm',), ('resting',), ('One_rhythm',), ('resting',), ('up_steping',), ('leaping_twisting',), ('One_rhythm',), ('up_leaping',), ('up_steping',), ('down_steping',), ('up_leaping',), ('up_leaping',), ('down_steping',), ('staccato',), ('up_steping',), ('leaping_twisting',), ('resting',), ('up_steping',), ('down_steping',), ('repeating',), ('repeating',), ('steping_twisting',), ('down_leaping',), ('continuing_rhythm',), ('up_steping',), ('up_steping',), ('down_steping',), ('up_steping',), ('up_steping',), ('down_steping',), ('leaping_twisting',), ('down_steping',), ('up_leaping',), ('down_steping',), ('leaping_twisting',), ('down_steping',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('continuing_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('up_leaping',), ('staccato',), ('up_leaping',), ('staccato',), ('up_leaping',), ('staccato',), ('up_leaping',), ('staccato',), ('up_leaping',), ('staccato',), ('up_leaping',), ('staccato',), ('up_leaping',), ('staccato',), ('up_leaping',), ('down_leaping',), ('down_leaping',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('down_leaping',), ('down_leaping',), ('repeating',), ('down_leaping',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('down_leaping',), ('up_leaping',), ('up_steping',), ('resting',), ('down_leaping',), ('up_leaping',), ('up_steping',), ('leaping_twisting',), ('down_leaping',), ('up_leaping',), ('up_steping',), ('leaping_twisting',), ('up_leaping',), ('down_leaping',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('up_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('up_steping',), ('up_leaping',), ('up_steping',), ('down_steping',), ('up_steping',), ('resting',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('resting',), ('triplet',), ('down_leaping',), ('up_steping',), ('resting',), ('triplet',), ('triplet',), ('triplet',), ('fast_rhythm',), ('staccato',), ('down_leaping',), ('resting',), ('down_leaping',), ('up_leaping',), ('up_steping',), ('resting',), ('down_leaping',), ('resting',), ('leaping_twisting',), ('resting',), ('resting',), ('resting',), ('up_leaping',), ('resting',), ('leaping_twisting',), ('resting',), ('leaping_twisting',), ('resting',), ('leaping_twisting',), ('resting',), ('up_steping',), ('resting',), ('down_leaping',), ('resting',), ('resting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('resting',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('leaping_twisting',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('repeating',), ('up_leaping',), ('triplet',), ('up_leaping',), ('leaping_twisting',), ('resting',), ('leaping_twisting',), ('up_leaping',), ('triplet',), ('resting',), ('up_steping',), ('up_leaping',), ('resting',), ('up_leaping',), ('leaping_twisting',), ('resting',), ('down_leaping',), ('up_leaping',), ('leaping_twisting',), ('resting',), ('up_steping',), ('resting',), ('repeating',), ('resting',), ('staccato',), ('staccato',), ('down_steping',), ('resting',), ('resting',), ('up_steping',), ('staccato',), ('down_steping',), ('resting',), ('resting',), ('resting',), ('staccato',), ('up_steping',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('up_steping',), ('staccato',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('repeating',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('repeating',), ('repeating',), ('repeating',), ('resting',), ('down_leaping',), ('leaping_twisting',), ('up_leaping',), ('up_steping',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('up_leaping',), ('up_steping',), ('up_leaping',), ('resting',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('down_steping',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('up_steping',), ('up_leaping',), ('resting',), ('down_leaping',), ('up_steping',), ('up_leaping',), ('up_steping',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('triplet',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('down_leaping',), ('resting',), ('resting',), ('resting',), ('up_leaping',), ('resting',), ('down_leaping',), ('resting',), ('continuing_rhythm',), ('resting',), ('down_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('fast_rhythm',), ('fast_rhythm',), ('steping_twisting',), ('continuing_rhythm',), ('down_steping',), ('resting',), ('leaping_twisting',), ('fast_rhythm',), ('steping_twisting',), ('fast_rhythm',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('leaping_twisting',), ('staccato',), ('staccato',), ('resting',), ('staccato',), ('fast_rhythm',), ('staccato',), ('staccato',), ('triplet',), ('triplet',), ('up_steping',), ('fast_rhythm',), ('triplet',), ('leaping_twisting',), ('staccato',), ('up_steping',), ('down_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('up_steping',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('steping_twisting',), ('up_steping',), ('staccato',), ('down_steping',), ('fast_rhythm',), ('up_steping',), ('staccato',), ('down_steping',), ('fast_rhythm',), ('up_steping',), ('staccato',), ('down_steping',), ('fast_rhythm',), ('up_steping',), ('staccato',), ('up_steping',), ('down_steping',), ('up_steping',), ('steping_twisting',), ('up_steping',), ('One_rhythm',), ('down_leaping',), ('up_leaping',), ('up_steping',), ('triplet',), ('triplet',), ('up_steping',), ('up_steping',), ('One_rhythm',), ('One_rhythm',), ('up_steping',), ('repeating',), ('resting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('leaping_twisting',), ('down_steping',), ('triplet',), ('staccato',), ('down_leaping',), ('One_rhythm',), ('fast_rhythm',), ('resting',), ('One_rhythm',), ('triplet',), ('down_steping',), ('leaping_twisting',), ('One_rhythm',), ('down_leaping',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('resting',), ('staccato',), ('staccato',), ('staccato',), ('resting',), ('down_steping',), ('triplet',), ('staccato',), ('resting',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('leaping_twisting',), ('staccato',), ('up_leaping',), ('up_steping',), ('continuing_rhythm',), ('up_steping',), ('One_rhythm',), ('up_steping',), ('continuing_rhythm',), ('up_steping',), ('resting',), ('up_steping',), ('continuing_rhythm',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('continuing_rhythm',), ('up_steping',), ('up_leaping',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('resting',), ('up_steping',), ('staccato',), ('repeating',), ('up_steping',), ('triplet',), ('triplet',), ('up_steping',), ('fast_rhythm',), ('One_rhythm',), ('steping_twisting',), ('resting',), ('up_steping',), ('triplet',), ('triplet',), ('resting',), ('up_steping',), ('repeating',), ('resting',), ('resting',), ('staccato',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('up_leaping',), ('repeating',), ('up_steping',), ('repeating',), ('repeating',), ('resting',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('triplet',), ('staccato',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('up_leaping',), ('up_leaping',), ('up_steping',), ('resting',), ('fast_rhythm',), ('down_steping',), ('continuing_rhythm',), ('down_leaping',), ('fast_rhythm',), ('resting',), ('repeating',), ('up_leaping',), ('continuing_rhythm',), ('down_steping',), ('continuing_rhythm',), ('up_steping',), ('continuing_rhythm',), ('up_steping',), ('triplet',), ('down_leaping',), ('up_steping',), ('down_leaping',), ('up_steping',), ('down_leaping',), ('down_steping',), ('resting',), ('continuing_rhythm',), ('up_leaping',), ('continuing_rhythm',), ('repeating',), ('up_steping',), ('down_leaping',), ('continuing_rhythm',), ('up_leaping',), ('down_steping',), ('up_steping',), ('continuing_rhythm',), ('up_leaping',), ('continuing_rhythm',), ('resting',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('steping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('continuing_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('up_leaping',), ('staccato',), ('staccato',), ('leaping_twisting',), ('up_leaping',), ('down_steping',), ('continuing_rhythm',), ('down_leaping',), ('up_leaping',), ('up_steping',), ('staccato',), ('staccato',), ('resting',), ('resting',), ('down_leaping',), ('staccato',), ('repeating',), ('repeating',), ('repeating',), ('down_leaping',), ('repeating',), ('repeating',), ('resting',), ('resting',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('staccato',), ('triplet',), ('resting',), ('leaping_twisting',), ('up_leaping',), ('staccato',), ('up_leaping',), ('leaping_twisting',), ('staccato',), ('staccato',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('resting',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('repeating',), ('repeating',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('down_leaping',), ('down_steping',), ('staccato',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('staccato',), ('down_steping',), ('staccato',), ('resting',), ('triplet',), ('fast_rhythm',), ('leaping_twisting',), ('resting',), ('down_leaping',), ('One_rhythm',), ('down_leaping',), ('resting',), ('down_leaping',), ('resting',), ('resting',), ('resting',), ('down_leaping',), ('up_leaping',), ('One_rhythm',), ('leaping_twisting',), ('continuing_rhythm',), ('One_rhythm',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('One_rhythm',), ('leaping_twisting',), ('One_rhythm',), ('up_steping',), ('One_rhythm',), ('up_leaping',), ('down_leaping',), ('resting',), ('leaping_twisting',), ('repeating',), ('repeating',), ('up_leaping',), ('steping_twisting',), ('up_leaping',), ('steping_twisting',), ('leaping_twisting',), ('steping_twisting',), ('leaping_twisting',), ('steping_twisting',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('staccato',), ('fast_rhythm',), ('triplet',), ('triplet',), ('leaping_twisting',), ('leaping_twisting',), ('down_steping',), ('One_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('triplet',), ('fast_rhythm',), ('repeating',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('triplet',), ('resting',), ('resting',), ('repeating',), ('repeating',), ('fast_rhythm',), ('repeating',), ('triplet',), ('repeating',), ('fast_rhythm',), ('repeating',), ('triplet',), ('repeating',), ('down_leaping',), ('repeating',), ('repeating',), ('leaping_twisting',), ('triplet',), ('repeating',), ('triplet',), ('continuing_rhythm',), ('fast_rhythm',), ('repeating',), ('repeating',), ('repeating',), ('resting',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('repeating',), ('up_steping',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('repeating',), ('repeating',), ('up_leaping',), ('repeating',), ('repeating',), ('down_leaping',), ('down_leaping',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('down_leaping',), ('staccato',), ('staccato',), ('repeating',), ('up_steping',), ('resting',), ('steping_twisting',), ('steping_twisting',), ('down_steping',), ('One_rhythm',), ('down_steping',), ('steping_twisting',), ('down_steping',), ('up_steping',), ('One_rhythm',), ('resting',), ('up_leaping',), ('up_steping',), ('up_steping',), ('up_steping',), ('leaping_twisting',), ('triplet',), ('up_leaping',), ('repeating',), ('repeating',), ('repeating',), ('up_steping',), ('leaping_twisting',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('up_steping',), ('leaping_twisting',), ('repeating',), ('resting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('leaping_twisting',), ('fast_rhythm',), ('triplet',), ('fast_rhythm',), ('staccato',), ('fast_rhythm',), ('staccato',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('triplet',), ('fast_rhythm',), ('staccato',), ('fast_rhythm',), ('staccato',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('triplet',), ('staccato',), ('leaping_twisting',), ('staccato',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('staccato',), ('continuing_rhythm',), ('leaping_twisting',), ('continuing_rhythm',), ('up_steping',), ('continuing_rhythm',), ('up_leaping',), ('leaping_twisting',), ('triplet',), ('staccato',), ('down_leaping',), ('staccato',), ('continuing_rhythm',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('triplet',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('One_rhythm',), ('down_leaping',), ('staccato',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('up_steping',), ('One_rhythm',), ('down_leaping',), ('up_steping',), ('steping_twisting',), ('steping_twisting',), ('triplet',), ('triplet',), ('up_steping',), ('down_steping',), ('down_steping',), ('One_rhythm',), ('One_rhythm',), ('up_steping',), ('up_steping',), ('One_rhythm',), ('fast_rhythm',), ('triplet',), ('resting',), ('up_steping',), ('down_leaping',), ('down_steping',), ('repeating',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('staccato',), ('staccato',), ('staccato',), ('up_steping',), ('leaping_twisting',), ('staccato',), ('staccato',), ('up_steping',), ('staccato',), ('repeating',), ('repeating',), ('repeating',), ('up_steping',), ('leaping_twisting',), ('triplet',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('triplet',), ('leaping_twisting',), ('leaping_twisting',), ('triplet',), ('triplet',), ('resting',), ('resting',), ('repeating',), ('resting',), ('resting',), ('resting',), ('resting',), ('repeating',), ('down_steping',), ('staccato',), ('resting',), ('resting',), ('up_leaping',), ('resting',), ('resting',), ('resting',), ('up_leaping',), ('resting',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('resting',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('resting',), ('resting',), ('up_steping',), ('up_leaping',), ('resting',), ('resting',), ('resting',), ('up_leaping',), ('resting',), ('resting',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('triplet',), ('leaping_twisting',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('triplet',), ('staccato',), ('leaping_twisting',), ('up_steping',), ('One_rhythm',), ('up_steping',), ('repeating',), ('up_steping',), ('fast_rhythm',), ('up_steping',), ('repeating',), ('up_steping',), ('up_steping',), ('down_steping',), ('repeating',), ('up_steping',), ('triplet',), ('triplet',), ('repeating',), ('up_steping',), ('up_steping',), ('repeating',), ('repeating',), ('up_steping',), ('up_steping',), ('repeating',), ('steping_twisting',), ('triplet',), ('fast_rhythm',), ('steping_twisting',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('staccato',), ('triplet',), ('fast_rhythm',), ('triplet',), ('continuing_rhythm',), ('down_steping',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('up_leaping',), ('down_leaping',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('staccato',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('staccato',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('staccato',), ('staccato',), ('triplet',), ('staccato',), ('resting',), ('staccato',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('resting',), ('up_steping',), ('up_steping',), ('steping_twisting',), ('staccato',), ('up_steping',), ('up_steping',), ('staccato',), ('staccato',), ('up_steping',), ('up_steping',), ('steping_twisting',), ('staccato',), ('up_steping',), ('up_steping',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('fast_rhythm',), ('up_steping',), ('steping_twisting',), ('resting',), ('resting',), ('continuing_rhythm',), ('continuing_rhythm',), ('resting',), ('resting',), ('continuing_rhythm',), ('continuing_rhythm',), ('resting',), ('resting',), ('continuing_rhythm',), ('steping_twisting',), ('resting',), ('up_leaping',), ('steping_twisting',), ('steping_twisting',), ('down_leaping',), ('resting',), ('down_steping',), ('leaping_twisting',), ('fast_rhythm',), ('down_leaping',), ('leaping_twisting',), ('triplet',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('triplet',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('up_steping',), ('down_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('down_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('up_steping',), ('resting',), ('up_steping',), ('triplet',), ('triplet',), ('down_leaping',), ('resting',), ('resting',), ('up_steping',), ('up_steping',), ('up_steping',), ('resting',), ('up_leaping',), ('resting',), ('down_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('staccato',), ('staccato',), ('down_leaping',), ('resting',), ('resting',), ('down_leaping',), ('up_steping',), ('repeating',), ('up_steping',), ('resting',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('repeating',), ('leaping_twisting',), ('One_rhythm',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('One_rhythm',), ('repeating',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('staccato',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('staccato',), ('repeating',), ('repeating',), ('staccato',), ('repeating',), ('repeating',), ('One_rhythm',), ('One_rhythm',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('staccato',), ('resting',), ('fast_rhythm',), ('steping_twisting',), ('resting',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('triplet',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('down_leaping',), ('up_steping',), ('resting',), ('continuing_rhythm',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('resting',), ('down_leaping',), ('up_steping',), ('resting',), ('triplet',), ('resting',), ('up_leaping',), ('up_leaping',), ('continuing_rhythm',), ('One_rhythm',), ('down_leaping',), ('resting',), ('resting',), ('steping_twisting',), ('resting',), ('One_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('up_steping',), ('One_rhythm',), ('up_steping',), ('continuing_rhythm',), ('resting',), ('One_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('up_steping',), ('One_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('resting',), ('One_rhythm',), ('up_leaping',), ('resting',), ('resting',), ('One_rhythm',), ('up_leaping',), ('resting',), ('resting',), ('One_rhythm',), ('staccato',), ('One_rhythm',), ('staccato',), ('One_rhythm',), ('resting',), ('steping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('down_leaping',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('down_leaping',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('down_leaping',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('down_leaping',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('down_leaping',), ('leaping_twisting',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('leaping_twisting',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('triplet',), ('up_steping',), ('triplet',), ('triplet',), ('triplet',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('repeating',), ('triplet',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('leaping_twisting',), ('triplet',), ('One_rhythm',), ('triplet',), ('One_rhythm',), ('staccato',), ('One_rhythm',), ('staccato',), ('staccato',), ('leaping_twisting',), ('repeating',), ('down_steping',), ('leaping_twisting',), ('down_leaping',), ('fast_rhythm',), ('repeating',), ('leaping_twisting',), ('down_leaping',), ('fast_rhythm',), ('repeating',), ('leaping_twisting',), ('down_leaping',), ('fast_rhythm',), ('repeating',), ('leaping_twisting',), ('down_leaping',), ('fast_rhythm',), ('repeating',), ('resting',), ('continuing_rhythm',), ('continuing_rhythm',), ('steping_twisting',), ('up_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('steping_twisting',), ('One_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('up_leaping',), ('One_rhythm',), ('continuing_rhythm',), ('steping_twisting',), ('One_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('continuing_rhythm',), ('resting',), ('continuing_rhythm',), ('up_leaping',), ('leaping_twisting',), ('resting',), ('triplet',), ('down_leaping',), ('up_leaping',), ('triplet',), ('up_leaping',), ('leaping_twisting',), ('resting',), ('triplet',), ('resting',), ('up_steping',), ('triplet',), ('triplet',), ('up_leaping',), ('leaping_twisting',), ('resting',), ('One_rhythm',), ('down_steping',), ('resting',), ('steping_twisting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('resting',), ('resting',), ('continuing_rhythm',), ('resting',), ('continuing_rhythm',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('down_steping',), ('leaping_twisting',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('down_leaping',), ('resting',), ('down_leaping',), ('resting',), ('staccato',), ('resting',), ('down_leaping',), ('resting',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('resting',), ('down_leaping',), ('resting',), ('triplet',), ('up_steping',), ('fast_rhythm',), ('up_steping',), ('triplet',), ('up_steping',), ('repeating',), ('resting',), ('triplet',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('up_steping',), ('up_steping',), ('up_steping',), ('staccato',), ('resting',), ('up_steping',), ('staccato',), ('fast_rhythm',), ('resting',), ('up_steping',), ('resting',), ('steping_twisting',), ('up_steping',), ('up_steping',), ('triplet',), ('staccato',), ('resting',), ('resting',), ('resting',), ('up_steping',), ('up_leaping',), ('up_steping',), ('up_leaping',), ('up_steping',), ('up_leaping',), ('up_steping',), ('up_leaping',), ('up_steping',), ('up_leaping',), ('up_steping',), ('up_leaping',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('down_leaping',), ('One_rhythm',), ('up_leaping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('repeating',), ('fast_rhythm',), ('down_leaping',), ('triplet',), ('repeating',), ('triplet',), ('triplet',), ('triplet',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('repeating',), ('up_leaping',), ('up_leaping',), ('One_rhythm',), ('One_rhythm',), ('up_leaping',), ('up_leaping',), ('One_rhythm',), ('resting',), ('up_leaping',), ('up_leaping',), ('One_rhythm',), ('repeating',), ('up_leaping',), ('up_leaping',), ('One_rhythm',), ('resting',), ('up_leaping',), ('up_leaping',), ('One_rhythm',), ('One_rhythm',), ('up_leaping',), ('up_leaping',), ('One_rhythm',), ('resting',), ('up_leaping',), ('up_leaping',), ('One_rhythm',), ('repeating',), ('up_leaping',), ('resting',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('up_steping',), ('One_rhythm',), ('leaping_twisting',), ('One_rhythm',), ('resting',), ('up_steping',), ('resting',), ('up_steping',), ('resting',), ('up_steping',), ('up_leaping',), ('up_steping',), ('repeating',), ('repeating',), ('steping_twisting',), ('up_steping',), ('resting',), ('triplet',), ('continuing_rhythm',), ('resting',), ('repeating',), ('up_leaping',), ('repeating',), ('leaping_twisting',), ('down_steping',), ('leaping_twisting',), ('repeating',), ('up_leaping',), ('down_leaping',), ('repeating',), ('repeating',), ('down_steping',), ('up_steping',), ('triplet',), ('repeating',), ('repeating',), ('triplet',), ('repeating',), ('fast_rhythm',), ('fast_rhythm',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('triplet',), ('resting',), ('fast_rhythm',), ('fast_rhythm',), ('triplet',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('triplet',), ('fast_rhythm',), ('staccato',), ('continuing_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('continuing_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('One_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('leaping_twisting',), ('One_rhythm',), ('One_rhythm',), ('resting',), ('One_rhythm',), ('fast_rhythm',), ('steping_twisting',), ('fast_rhythm',), ('repeating',), ('resting',), ('resting',), ('fast_rhythm',), ('up_steping',), ('up_steping',), ('continuing_rhythm',), ('resting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('fast_rhythm',), ('up_steping',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('resting',), ('down_leaping',), ('resting',), ('resting',), ('up_leaping',), ('up_leaping',), ('resting',), ('resting',), ('repeating',), ('down_leaping',), ('up_steping',), ('staccato',), ('staccato',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('resting',), ('up_steping',), ('up_steping',), ('triplet',), ('triplet',), ('triplet',), ('up_steping',), ('triplet',), ('triplet',), ('triplet',), ('up_steping',), ('triplet',), ('triplet',), ('up_steping',), ('up_steping',), ('triplet',), ('triplet',), ('triplet',), ('up_steping',), ('resting',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('fast_rhythm',), ('resting',), ('up_steping',), ('down_steping',), ('up_steping',), ('resting',), ('up_steping',), ('triplet',), ('staccato',), ('resting',), ('resting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('repeating',), ('leaping_twisting',), ('repeating',), ('repeating',), ('up_steping',), ('staccato',), ('down_steping',), ('resting',), ('One_rhythm',), ('resting',), ('triplet',), ('down_steping',), ('staccato',), ('up_steping',), ('staccato',), ('up_steping',), ('up_steping',), ('up_steping',), ('resting',), ('repeating',), ('leaping_twisting',), ('up_steping',), ('up_steping',), ('repeating',), ('repeating',), ('fast_rhythm',), ('fast_rhythm',), ('repeating',), ('up_steping',), ('resting',), ('resting',), ('resting',), ('fast_rhythm',), ('down_steping',), ('resting',), ('resting',), ('down_leaping',), ('up_steping',), ('resting',), ('triplet',), ('resting',), ('triplet',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('resting',), ('up_leaping',), ('resting',), ('down_steping',), ('resting',), ('down_steping',), ('up_steping',), ('resting',), ('resting',), ('resting',), ('down_leaping',), ('down_leaping',), ('down_steping',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('down_leaping',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('down_steping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('One_rhythm',), ('leaping_twisting',), ('down_leaping',), ('steping_twisting',), ('continuing_rhythm',), ('resting',), ('up_leaping',), ('down_leaping',), ('up_steping',), ('down_steping',), ('resting',), ('repeating',), ('resting',), ('resting',), ('resting',), ('resting',), ('steping_twisting',), ('leaping_twisting',), ('resting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('resting',), ('staccato',), ('resting',), ('steping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('triplet',), ('triplet',), ('leaping_twisting',), ('triplet',), ('continuing_rhythm',), ('triplet',), ('leaping_twisting',), ('up_steping',), ('continuing_rhythm',), ('fast_rhythm',), ('up_steping',), ('up_leaping',), ('triplet',), ('One_rhythm',), ('down_leaping',), ('up_steping',), ('triplet',), ('up_leaping',), ('leaping_twisting',), ('repeating',), ('up_steping',), ('steping_twisting',), ('down_leaping',), ('steping_twisting',), ('up_steping',), ('steping_twisting',), ('down_leaping',), ('continuing_rhythm',), ('up_steping',), ('steping_twisting',), ('down_leaping',), ('steping_twisting',), ('up_steping',), ('steping_twisting',), ('down_leaping',), ('continuing_rhythm',), ('up_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('continuing_rhythm',), ('continuing_rhythm',), ('resting',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('resting',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('resting',), ('One_rhythm',), ('up_leaping',), ('fast_rhythm',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('triplet',), ('staccato',), ('fast_rhythm',), ('staccato',), ('resting',), ('repeating',), ('repeating',), ('staccato',), ('repeating',), ('down_leaping',), ('leaping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('down_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('triplet',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('fast_rhythm',), ('repeating',), ('up_steping',), ('steping_twisting',), ('fast_rhythm',), ('leaping_twisting',), ('fast_rhythm',), ('leaping_twisting',), ('fast_rhythm',), ('down_steping',), ('down_leaping',), ('repeating',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('One_rhythm',), ('continuing_rhythm',), ('up_steping',), ('continuing_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('up_steping',), ('up_steping',), ('One_rhythm',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('up_steping',), ('down_steping',), ('down_steping',), ('down_leaping',), ('staccato',), ('down_leaping',), ('up_steping',), ('leaping_twisting',), ('steping_twisting',), ('staccato',), ('leaping_twisting',), ('steping_twisting',), ('up_steping',), ('up_steping',), ('resting',), ('resting',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('fast_rhythm',), ('repeating',), ('repeating',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('down_leaping',), ('leaping_twisting',), ('One_rhythm',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('up_steping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('resting',), ('staccato',), ('triplet',), ('down_steping',), ('up_steping',), ('triplet',), ('triplet',), ('triplet',), ('triplet',), ('down_steping',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('staccato',), ('leaping_twisting',), ('resting',), ('steping_twisting',), ('up_steping',), ('steping_twisting',), ('up_steping',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('One_rhythm',), ('resting',), ('resting',), ('resting',), ('repeating',), ('One_rhythm',), ('up_steping',), ('repeating',), ('One_rhythm',), ('repeating',), ('One_rhythm',), ('One_rhythm',), ('up_steping',), ('down_leaping',), ('up_steping',), ('One_rhythm',), ('up_steping',), ('down_leaping',), ('One_rhythm',), ('repeating',), ('leaping_twisting',), ('repeating',), ('up_steping',), ('continuing_rhythm',), ('down_leaping',), ('repeating',), ('One_rhythm',), ('continuing_rhythm',), ('up_steping',), ('repeating',), ('up_steping',), ('resting',), ('up_leaping',), ('One_rhythm',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('up_steping',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('up_leaping',), ('resting',), ('down_steping',), ('leaping_twisting',), ('up_steping',), ('up_steping',), ('resting',), ('triplet',), ('leaping_twisting',), ('triplet',), ('up_steping',), ('up_steping',), ('down_steping',), ('triplet',), ('resting',), ('fast_rhythm',), ('down_leaping',), ('up_steping',), ('resting',), ('up_steping',), ('resting',), ('fast_rhythm',), ('down_leaping',), ('down_leaping',), ('steping_twisting',), ('steping_twisting',), ('up_leaping',), ('down_steping',), ('up_leaping',), ('triplet',), ('leaping_twisting',), ('up_leaping',), ('triplet',), ('down_steping',), ('up_steping',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('down_leaping',), ('triplet',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('triplet',), ('staccato',), ('staccato',), ('staccato',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('staccato',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_leaping',), ('triplet',), ('leaping_twisting',), ('up_leaping',), ('triplet',), ('resting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('down_leaping',), ('down_leaping',), ('leaping_twisting',), ('staccato',), ('down_leaping',), ('leaping_twisting',), ('down_leaping',), ('triplet',), ('leaping_twisting',), ('staccato',), ('resting',), ('resting',), ('resting',), ('triplet',), ('resting',), ('down_steping',), ('fast_rhythm',), ('up_steping',), ('resting',), ('resting',), ('triplet',), ('resting',), ('triplet',), ('triplet',), ('fast_rhythm',), ('leaping_twisting',), ('resting',), ('staccato',), ('steping_twisting',), ('triplet',), ('down_leaping',), ('staccato',), ('resting',), ('staccato',), ('resting',), ('triplet',), ('repeating',), ('fast_rhythm',), ('resting',), ('repeating',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('repeating',), ('resting',), ('resting',), ('resting',), ('down_steping',), ('One_rhythm',), ('up_steping',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('repeating',), ('One_rhythm',), ('up_steping',), ('One_rhythm',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('up_steping',), ('steping_twisting',), ('down_steping',), ('down_leaping',), ('up_steping',), ('resting',), ('resting',), ('steping_twisting',), ('up_steping',), ('steping_twisting',), ('down_steping',), ('down_leaping',), ('up_steping',), ('staccato',), ('resting',), ('staccato',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('up_leaping',), ('up_leaping',), ('staccato',), ('staccato',), ('staccato',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('staccato',), ('up_leaping',), ('repeating',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('repeating',), ('leaping_twisting',), ('resting',), ('repeating',), ('repeating',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('up_leaping',), ('staccato',), ('staccato',), ('up_leaping',), ('resting',), ('triplet',), ('leaping_twisting',), ('triplet',), ('up_leaping',), ('triplet',), ('triplet',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('triplet',), ('leaping_twisting',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('leaping_twisting',), ('up_leaping',), ('triplet',), ('resting',), ('triplet',), ('up_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('up_leaping',), ('resting',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('down_leaping',), ('resting',), ('up_steping',), ('up_steping',), ('triplet',), ('continuing_rhythm',), ('leaping_twisting',), ('up_steping',), ('triplet',), ('triplet',), ('continuing_rhythm',), ('down_steping',), ('triplet',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('steping_twisting',), ('down_steping',), ('down_steping',), ('resting',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('continuing_rhythm',), ('continuing_rhythm',), ('down_steping',), ('down_steping',), ('repeating',), ('down_steping',), ('down_steping',), ('triplet',), ('up_steping',), ('up_steping',), ('staccato',), ('staccato',), ('staccato',), ('up_leaping',), ('staccato',), ('up_leaping',), ('down_leaping',), ('staccato',), ('up_steping',), ('staccato',), ('staccato',), ('up_steping',), ('staccato',), ('steping_twisting',), ('down_steping',), ('fast_rhythm',), ('triplet',), ('up_steping',), ('triplet',), ('triplet',), ('down_leaping',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('staccato',), ('staccato',), ('leaping_twisting',), ('up_steping',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('triplet',), ('staccato',), ('resting',), ('fast_rhythm',), ('repeating',), ('down_steping',), ('resting',), ('fast_rhythm',), ('up_steping',), ('resting',), ('triplet',), ('resting',), ('triplet',), ('resting',), ('One_rhythm',), ('One_rhythm',), ('up_leaping',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('resting',), ('up_steping',), ('down_leaping',), ('resting',), ('resting',), ('One_rhythm',), ('steping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('resting',), ('resting',), ('continuing_rhythm',), ('repeating',), ('resting',), ('resting',), ('resting',), ('fast_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('triplet',), ('fast_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('triplet',), ('fast_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('triplet',), ('staccato',), ('staccato',), ('up_steping',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('resting',), ('resting',), ('up_leaping',), ('repeating',), ('repeating',), ('repeating',), ('up_steping',), ('leaping_twisting',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('resting',), ('triplet',), ('leaping_twisting',), ('staccato',), ('triplet',), ('triplet',), ('triplet',), ('up_steping',), ('steping_twisting',), ('resting',), ('staccato',), ('triplet',), ('down_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('down_steping',), ('up_steping',), ('up_steping',), ('staccato',), ('down_steping',), ('staccato',), ('repeating',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('steping_twisting',), ('down_steping',), ('up_steping',), ('up_steping',), ('staccato',), ('staccato',), ('fast_rhythm',), ('triplet',), ('triplet',), ('triplet',), ('down_leaping',), ('up_steping',), ('steping_twisting',), ('triplet',), ('up_steping',), ('staccato',), ('up_leaping',), ('One_rhythm',), ('down_leaping',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('down_leaping',), ('down_leaping',), ('up_leaping',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('continuing_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('down_leaping',), ('triplet',), ('up_leaping',), ('down_steping',), ('triplet',), ('triplet',), ('triplet',), ('fast_rhythm',), ('staccato',), ('up_steping',), ('up_steping',), ('staccato',), ('up_steping',), ('triplet',), ('staccato',), ('resting',), ('up_leaping',), ('repeating',), ('down_steping',), ('fast_rhythm',), ('triplet',), ('fast_rhythm',), ('triplet',), ('repeating',), ('leaping_twisting',), ('repeating',), ('leaping_twisting',), ('staccato',), ('repeating',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('up_steping',), ('staccato',), ('staccato',), ('resting',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('resting',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('up_steping',), ('repeating',), ('repeating',), ('up_steping',), ('down_leaping',), ('steping_twisting',), ('resting',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('leaping_twisting',), ('resting',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('repeating',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('One_rhythm',), ('up_steping',), ('repeating',), ('up_steping',), ('up_steping',), ('repeating',), ('up_steping',), ('leaping_twisting',), ('triplet',), ('up_steping',), ('staccato',), ('triplet',), ('staccato',), ('up_steping',), ('leaping_twisting',), ('resting',), ('staccato',), ('up_leaping',), ('fast_rhythm',), ('steping_twisting',), ('staccato',), ('up_steping',), ('triplet',), ('leaping_twisting',), ('triplet',), ('up_steping',), ('staccato',), ('triplet',), ('staccato',), ('resting',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('triplet',), ('fast_rhythm',), ('up_steping',), ('up_steping',), ('up_steping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('up_steping',), ('up_steping',), ('resting',), ('up_steping',), ('up_steping',), ('up_steping',), ('resting',), ('One_rhythm',), ('repeating',), ('staccato',), ('up_steping',), ('up_steping',), ('up_steping',), ('up_steping',), ('resting',), ('up_steping',), ('down_leaping',), ('staccato',), ('down_leaping',), ('staccato',), ('resting',), ('leaping_twisting',), ('resting',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('resting',), ('fast_rhythm',), ('steping_twisting',), ('down_steping',), ('fast_rhythm',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('One_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('continuing_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('steping_twisting',), ('fast_rhythm',), ('steping_twisting',), ('fast_rhythm',), ('steping_twisting',), ('fast_rhythm',), ('steping_twisting',), ('fast_rhythm',), ('leaping_twisting',), ('resting',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('up_leaping',), ('One_rhythm',), ('up_steping',), ('One_rhythm',), ('down_leaping',), ('One_rhythm',), ('leaping_twisting',), ('One_rhythm',), ('One_rhythm',), ('leaping_twisting',), ('up_leaping',), ('down_steping',), ('One_rhythm',), ('staccato',), ('down_steping',), ('up_leaping',), ('One_rhythm',), ('steping_twisting',), ('up_leaping',), ('fast_rhythm',), ('staccato',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('repeating',), ('fast_rhythm',), ('repeating',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('repeating',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('up_steping',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('up_steping',), ('triplet',), ('staccato',), ('fast_rhythm',), ('up_steping',), ('up_steping',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('resting',), ('fast_rhythm',), ('repeating',), ('repeating',), ('triplet',), ('triplet',), ('triplet',), ('leaping_twisting',), ('triplet',), ('staccato',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('repeating',), ('staccato',), ('staccato',), ('triplet',), ('triplet',), ('triplet',), ('triplet',), ('fast_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('fast_rhythm',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('One_rhythm',), ('leaping_twisting',), ('One_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('up_steping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('down_leaping',), ('staccato',), ('staccato',), ('staccato',), ('up_leaping',), ('staccato',), ('staccato',), ('staccato',), ('resting',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('up_leaping',), ('resting',), ('fast_rhythm',), ('up_steping',), ('triplet',), ('up_steping',), ('fast_rhythm',), ('continuing_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('repeating',), ('fast_rhythm',), ('repeating',), ('fast_rhythm',), ('staccato',), ('up_steping',), ('down_steping',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('down_steping',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('up_steping',), ('up_steping',), ('steping_twisting',), ('resting',), ('triplet',), ('resting',), ('up_steping',), ('repeating',), ('staccato',), ('down_steping',), ('up_leaping',), ('triplet',), ('down_leaping',), ('resting',), ('up_steping',), ('steping_twisting',), ('up_steping',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('staccato',), ('fast_rhythm',), ('staccato',), ('staccato',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('staccato',), ('staccato',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('triplet',), ('triplet',), ('staccato',), ('triplet',), ('staccato',), ('triplet',), ('triplet',), ('fast_rhythm',), ('staccato',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('up_steping',), ('resting',), ('up_steping',), ('resting',), ('up_steping',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('continuing_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('continuing_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('staccato',), ('triplet',), ('resting',), ('up_steping',), ('staccato',), ('triplet',), ('resting',), ('up_steping',), ('staccato',), ('triplet',), ('resting',), ('up_steping',), ('staccato',), ('triplet',), ('resting',), ('up_steping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('repeating',), ('staccato',), ('down_steping',), ('fast_rhythm',), ('fast_rhythm',), ('triplet',), ('fast_rhythm',), ('fast_rhythm',), ('triplet',), ('down_steping',), ('continuing_rhythm',), ('down_steping',), ('resting',), ('down_steping',), ('steping_twisting',), ('up_steping',), ('resting',), ('down_steping',), ('steping_twisting',), ('fast_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('up_steping',), ('leaping_twisting',), ('down_leaping',), ('up_steping',), ('resting',), ('up_steping',), ('resting',), ('fast_rhythm',), ('resting',), ('triplet',), ('up_steping',), ('One_rhythm',), ('leaping_twisting',), ('resting',), ('staccato',), ('resting',), ('down_leaping',), ('resting',), ('staccato',), ('staccato',), ('down_steping',), ('resting',), ('up_leaping',), ('down_leaping',), ('staccato',), ('up_leaping',), ('staccato',), ('staccato',), ('resting',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('resting',), ('repeating',), ('staccato',), ('down_leaping',), ('up_steping',), ('repeating',), ('staccato',), ('repeating',), ('repeating',), ('triplet',), ('down_steping',), ('repeating',), ('up_steping',), ('repeating',), ('fast_rhythm',), ('repeating',), ('repeating',), ('triplet',), ('down_steping',), ('repeating',), ('up_steping',), ('repeating',), ('fast_rhythm',), ('repeating',), ('repeating',), ('repeating',), ('fast_rhythm',), ('down_steping',), ('repeating',), ('down_leaping',), ('repeating',), ('up_steping',), ('repeating',), ('repeating',), ('down_steping',), ('down_leaping',), ('repeating',), ('repeating',), ('leaping_twisting',), ('repeating',), ('down_leaping',), ('repeating',), ('down_steping',), ('down_leaping',), ('repeating',), ('repeating',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('resting',), ('resting',), ('repeating',), ('resting',), ('up_steping',), ('resting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('resting',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('leaping_twisting',), ('staccato',), ('up_steping',), ('down_steping',), ('down_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('leaping_twisting',), ('staccato',), ('staccato',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('up_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('up_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('up_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('up_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('up_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('fast_rhythm',), ('resting',), ('resting',), ('resting',), ('One_rhythm',), ('resting',), ('resting',), ('resting',), ('resting',), ('down_leaping',), ('resting',), ('up_leaping',), ('steping_twisting',), ('down_leaping',), ('resting',), ('One_rhythm',), ('down_leaping',), ('repeating',), ('staccato',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('staccato',), ('repeating',), ('fast_rhythm',), ('repeating',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('repeating',), ('staccato',), ('repeating',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('resting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('staccato',), ('triplet',), ('staccato',), ('up_leaping',), ('resting',), ('leaping_twisting',), ('resting',), ('up_leaping',), ('resting',), ('leaping_twisting',), ('resting',), ('staccato',), ('resting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('triplet',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('triplet',), ('fast_rhythm',), ('triplet',), ('up_leaping',), ('One_rhythm',), ('up_leaping',), ('resting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('up_leaping',), ('resting',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('up_leaping',), ('leaping_twisting',), ('up_steping',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('down_leaping',), ('down_leaping',), ('leaping_twisting',), ('down_leaping',), ('down_leaping',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('staccato',), ('staccato',), ('resting',), ('resting',), ('steping_twisting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('staccato',), ('steping_twisting',), ('resting',), ('resting',), ('resting',), ('One_rhythm',), ('resting',), ('resting',), ('resting',), ('resting',), ('One_rhythm',), ('One_rhythm',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('staccato',), ('steping_twisting',), ('resting',), ('resting',), ('resting',), ('One_rhythm',), ('resting',), ('resting',), ('resting',), ('resting',), ('One_rhythm',), ('down_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('up_steping',), ('resting',), ('One_rhythm',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('down_leaping',), ('continuing_rhythm',), ('down_leaping',), ('continuing_rhythm',), ('continuing_rhythm',), ('resting',), ('down_leaping',), ('continuing_rhythm',), ('down_leaping',), ('continuing_rhythm',), ('resting',), ('continuing_rhythm',), ('up_leaping',), ('continuing_rhythm',), ('resting',), ('continuing_rhythm',), ('continuing_rhythm',), ('down_leaping',), ('repeating',), ('up_leaping',), ('continuing_rhythm',), ('down_leaping',), ('resting',), ('continuing_rhythm',), ('continuing_rhythm',), ('down_leaping',), ('resting',), ('up_leaping',), ('continuing_rhythm',), ('continuing_rhythm',), ('staccato',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('staccato',), ('up_steping',), ('down_leaping',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('staccato',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('leaping_twisting',), ('staccato',), ('up_steping',), ('repeating',), ('repeating',), ('repeating',), ('One_rhythm',), ('repeating',), ('repeating',), ('down_steping',), ('One_rhythm',), ('repeating',), ('repeating',), ('repeating',), ('fast_rhythm',), ('repeating',), ('repeating',), ('repeating',), ('triplet',), ('fast_rhythm',), ('triplet',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('triplet',), ('triplet',), ('fast_rhythm',), ('up_steping',), ('triplet',), ('staccato',), ('down_leaping',), ('repeating',), ('leaping_twisting',), ('repeating',), ('repeating',), ('repeating',), ('staccato',), ('repeating',), ('repeating',), ('repeating',), ('staccato',), ('up_steping',), ('up_steping',), ('repeating',), ('leaping_twisting',), ('repeating',), ('up_steping',), ('up_steping',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('triplet',), ('repeating',), ('repeating',), ('repeating',), ('up_steping',), ('triplet',), ('triplet',), ('leaping_twisting',), ('triplet',), ('leaping_twisting',), ('triplet',), ('triplet',), ('triplet',), ('triplet',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('triplet',), ('resting',), ('triplet',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('One_rhythm',), ('down_leaping',), ('staccato',), ('up_leaping',), ('resting',), ('resting',), ('down_leaping',), ('resting',), ('resting',), ('resting',), ('triplet',), ('up_steping',), ('up_steping',), ('triplet',), ('triplet',), ('triplet',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('up_steping',), ('up_steping',), ('up_steping',), ('triplet',), ('triplet',), ('up_steping',), ('steping_twisting',), ('up_steping',), ('triplet',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('resting',), ('resting',), ('leaping_twisting',), ('down_leaping',), ('staccato',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('down_leaping',), ('up_leaping',), ('leaping_twisting',), ('up_steping',), ('down_steping',), ('repeating',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('down_leaping',), ('down_steping',), ('resting',), ('triplet',), ('fast_rhythm',), ('continuing_rhythm',), ('fast_rhythm',), ('triplet',), ('triplet',), ('triplet',), ('triplet',), ('repeating',), ('continuing_rhythm',), ('continuing_rhythm',), ('down_leaping',), ('resting',), ('resting',), ('triplet',), ('down_steping',), ('leaping_twisting',), ('down_leaping',), ('steping_twisting',), ('leaping_twisting',), ('steping_twisting',), ('triplet',), ('resting',), ('staccato',), ('resting',), ('resting',), ('staccato',), ('up_leaping',), ('staccato',), ('down_leaping',), ('steping_twisting',), ('leaping_twisting',), ('steping_twisting',), ('triplet',), ('resting',), ('fast_rhythm',), ('resting',), ('resting',), ('triplet',), ('up_leaping',), ('staccato',), ('down_leaping',), ('up_steping',), ('continuing_rhythm',), ('down_leaping',), ('up_leaping',), ('continuing_rhythm',), ('One_rhythm',), ('resting',), ('continuing_rhythm',), ('continuing_rhythm',), ('up_leaping',), ('continuing_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('up_steping',), ('repeating',), ('up_leaping',), ('continuing_rhythm',), ('resting',), ('leaping_twisting',), ('down_leaping',), ('repeating',), ('up_leaping',), ('repeating',), ('continuing_rhythm',), ('leaping_twisting',), ('up_leaping',), ('repeating',), ('up_steping',), ('down_leaping',), ('One_rhythm',), ('triplet',), ('resting',), ('resting',), ('resting',), ('down_steping',), ('up_steping',), ('staccato',), ('repeating',), ('triplet',), ('One_rhythm',), ('down_steping',), ('One_rhythm',), ('down_steping',), ('leaping_twisting',), ('up_steping',), ('leaping_twisting',), ('staccato',), ('up_steping',), ('triplet',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('resting',), ('One_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('up_steping',), ('down_leaping',), ('resting',), ('resting',), ('leaping_twisting',), ('One_rhythm',), ('One_rhythm',), ('up_steping',), ('leaping_twisting',), ('One_rhythm',), ('One_rhythm',), ('staccato',), ('triplet',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('triplet',), ('up_leaping',), ('continuing_rhythm',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('continuing_rhythm',), ('up_leaping',), ('up_leaping',), ('continuing_rhythm',), ('up_leaping',), ('resting',), ('staccato',), ('resting',), ('up_leaping',), ('leaping_twisting',), ('resting',), ('staccato',), ('staccato',), ('resting',), ('resting',), ('resting',), ('up_leaping',), ('resting',), ('staccato',), ('staccato',), ('up_steping',), ('steping_twisting',), ('resting',), ('down_steping',), ('up_steping',), ('continuing_rhythm',), ('leaping_twisting',), ('down_steping',), ('One_rhythm',), ('resting',), ('leaping_twisting',), ('down_steping',), ('up_steping',), ('continuing_rhythm',), ('leaping_twisting',), ('down_steping',), ('One_rhythm',), ('resting',), ('leaping_twisting',), ('resting',), ('repeating',), ('resting',), ('repeating',), ('One_rhythm',), ('One_rhythm',), ('resting',), ('leaping_twisting',), ('down_steping',), ('up_steping',), ('continuing_rhythm',), ('leaping_twisting',), ('down_steping',), ('One_rhythm',), ('resting',), ('leaping_twisting',), ('resting',), ('repeating',), ('resting',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('resting',), ('continuing_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('down_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('down_steping',), ('One_rhythm',), ('leaping_twisting',), ('up_steping',), ('One_rhythm',), ('steping_twisting',), ('repeating',), ('up_leaping',), ('repeating',), ('up_steping',), ('leaping_twisting',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('up_leaping',), ('fast_rhythm',), ('staccato',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('leaping_twisting',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('triplet',), ('triplet',), ('up_steping',), ('fast_rhythm',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('triplet',), ('triplet',), ('down_steping',), ('fast_rhythm',), ('triplet',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('staccato',), ('staccato',), ('fast_rhythm',), ('triplet',), ('steping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('down_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('repeating',), ('triplet',), ('up_steping',), ('repeating',), ('steping_twisting',), ('triplet',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('triplet',), ('up_steping',), ('steping_twisting',), ('steping_twisting',), ('triplet',), ('triplet',), ('resting',), ('resting',), ('resting',), ('resting',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('leaping_twisting',), ('repeating',), ('up_steping',), ('repeating',), ('leaping_twisting',), ('repeating',), ('up_steping',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('continuing_rhythm',), ('up_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('up_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('up_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('up_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('up_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('up_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('up_steping',), ('continuing_rhythm',), ('resting',), ('leaping_twisting',), ('repeating',), ('down_leaping',), ('leaping_twisting',), ('repeating',), ('down_leaping',), ('One_rhythm',), ('leaping_twisting',), ('up_steping',), ('down_leaping',), ('leaping_twisting',), ('repeating',), ('One_rhythm',), ('repeating',), ('leaping_twisting',), ('repeating',), ('One_rhythm',), ('One_rhythm',), ('repeating',), ('down_leaping',), ('up_leaping',), ('triplet',), ('staccato',), ('triplet',), ('staccato',), ('triplet',), ('staccato',), ('triplet',), ('triplet',), ('resting',), ('staccato',), ('leaping_twisting',), ('staccato',), ('staccato',), ('staccato',), ('leaping_twisting',), ('staccato',), ('staccato',), ('staccato',), ('leaping_twisting',), ('staccato',), ('staccato',), ('staccato',), ('leaping_twisting',), ('staccato',), ('staccato',), ('continuing_rhythm',), ('resting',), ('One_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('up_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('up_steping',), ('steping_twisting',), ('down_leaping',), ('leaping_twisting',), ('continuing_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('continuing_rhythm',), ('continuing_rhythm',), ('repeating',), ('fast_rhythm',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('fast_rhythm',), ('repeating',), ('fast_rhythm',), ('repeating',), ('repeating',), ('fast_rhythm',), ('fast_rhythm',), ('repeating',), ('repeating',), ('repeating',), ('fast_rhythm',), ('repeating',), ('repeating',), ('resting',), ('continuing_rhythm',), ('up_steping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('fast_rhythm',), ('leaping_twisting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('continuing_rhythm',), ('resting',), ('up_steping',), ('up_steping',), ('One_rhythm',), ('up_steping',), ('steping_twisting',), ('staccato',), ('One_rhythm',), ('leaping_twisting',), ('up_steping',), ('down_leaping',), ('up_steping',), ('down_steping',), ('up_steping',), ('up_steping',), ('One_rhythm',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('steping_twisting',), ('resting',), ('resting',), ('up_leaping',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('fast_rhythm',), ('triplet',), ('triplet',), ('leaping_twisting',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('steping_twisting',), ('One_rhythm',), ('down_leaping',), ('up_leaping',), ('One_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('up_leaping',), ('triplet',), ('up_leaping',), ('continuing_rhythm',), ('up_leaping',), ('down_steping',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('down_steping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('up_leaping',), ('One_rhythm',), ('continuing_rhythm',), ('up_steping',), ('up_steping',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('up_steping',), ('fast_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('up_leaping',), ('resting',), ('staccato',), ('staccato',), ('repeating',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('repeating',), ('up_steping',), ('up_leaping',), ('repeating',), ('resting',), ('fast_rhythm',), ('up_steping',), ('leaping_twisting',), ('leaping_twisting',), ('up_steping',), ('up_leaping',), ('leaping_twisting',), ('up_steping',), ('One_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('up_steping',), ('down_leaping',), ('down_leaping',), ('up_steping',), ('up_steping',), ('One_rhythm',), ('down_steping',), ('down_leaping',), ('resting',), ('steping_twisting',), ('resting',), ('steping_twisting',), ('resting',), ('staccato',), ('steping_twisting',), ('up_leaping',), ('resting',), ('resting',), ('resting',), ('resting',), ('resting',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('continuing_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('up_steping',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('leaping_twisting',), ('leaping_twisting',), ('One_rhythm',), ('One_rhythm',), ('resting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('steping_twisting',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('One_rhythm',), ('One_rhythm',), ('leaping_twisting',), ('staccato',), ('staccato',), ('down_leaping',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('triplet',), ('resting',), ('staccato',), ('staccato',), ('staccato',), ('staccato',), ('down_leaping',), ('staccato',), ('staccato',), ('continuing_rhythm',), ('resting',), ('resting',), ('resting',), ('up_leaping',), ('down_leaping',), ('resting',), ('resting',), ('up_steping',), ('resting',), ('up_steping',), ('resting',), ('continuing_rhythm',), ('down_leaping',), ('resting',), ('down_leaping',), ('leaping_twisting',), ('up_steping',), ('up_steping',), ('up_leaping',), ('leaping_twisting',), ('up_steping',), ('up_steping',), ('up_steping',), ('leaping_twisting',), ('up_steping',), ('up_steping',), ('down_leaping',), ('leaping_twisting',), ('up_steping',), ('resting',), ('up_leaping',), ('continuing_rhythm',), ('down_leaping',), ('up_leaping',), ('down_leaping',), ('down_leaping',), ('up_leaping',), ('continuing_rhythm',), ('up_leaping',), ('continuing_rhythm',), ('down_leaping',), ('leaping_twisting',), ('One_rhythm',), ('triplet',), ('triplet',), ('repeating',), ('fast_rhythm',), ('fast_rhythm',), ('triplet',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('staccato',), ('staccato',), ('fast_rhythm',), ('fast_rhythm',), ('staccato',), ('triplet',), ('triplet',), ('triplet',), ('triplet',), ('repeating',), ('resting',), ('down_steping',), ('triplet',), ('up_leaping',), ('repeating',), ('triplet',), ('up_leaping',), ('steping_twisting',), ('resting',), ('up_steping',), ('down_steping',), ('down_leaping',), ('up_steping',), ('steping_twisting',), ('resting',), ('up_steping',), ('resting',), ('steping_twisting',), ('resting',), ('up_steping',), ('resting',), ('One_rhythm',), ('up_steping',), ('resting',), ('steping_twisting',), ('resting',), ('resting',), ('leaping_twisting',), ('staccato',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('continuing_rhythm',), ('up_steping',), ('up_leaping',), ('continuing_rhythm',), ('continuing_rhythm',), ('down_steping',), ('up_leaping',), ('up_leaping',), ('up_leaping',), ('continuing_rhythm',), ('continuing_rhythm',), ('steping_twisting',), ('continuing_rhythm',), ('continuing_rhythm',), ('One_rhythm',), ('up_steping',), ('up_steping',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('leaping_twisting',), ('up_leaping',), ('up_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('leaping_twisting',), ('repeating',), ('repeating',), ('repeating',), ('triplet',), ('resting',), ('resting',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('triplet',), ('repeating',), ('triplet',), ('repeating',), ('repeating',), ('repeating',), ('repeating',), ('triplet',), ('leaping_twisting',), ('staccato',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('leaping_twisting',), ('down_leaping',), ('leaping_twisting',), ('leaping_twisting',), ('staccato',), ('down_leaping',), ('triplet',), ('leaping_twisting',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('One_rhythm',), ('fast_rhythm',), ('fast_rhythm',), ('One_rhythm',), ('One_rhythm',), ('leaping_twisting',), ('fast_rhythm',), ('One_rhythm',), ('One_rhythm',), ('leaping_twisting',), ('leaping_twisting',)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKJiCvrXd5NJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "486c73c8-8f86-422a-df7d-384d8551f552"
      },
      "source": [
        "prediction=classifier.predict(test_matrix[500].reshape(1,24,24,1))\n",
        "categorical=np.zeros_like(prediction)\n",
        "num=np.argmax(prediction)\n",
        "categorical[0][num]=1\n",
        "label=mlb.inverse_transform(categorical)\n",
        "print(label)\n",
        "H = test_matrix[500].reshape(24,24) #위에 출력에서 나온 숫자 test_matrix['요기']에 적고 출력하면 나옴\n",
        "fig = plt.figure(figsize=(6, 3.2))\n",
        "\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_title('colorMap')\n",
        "plt.imshow(H)\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
        "cax.get_xaxis().set_visible(False)\n",
        "cax.get_yaxis().set_visible(False)\n",
        "cax.patch.set_alpha(0)\n",
        "cax.set_frame_on(False)\n",
        "plt.colorbar(orientation='vertical')\n",
        "plt.show()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('up_steping',)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x230.4 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1UAO1g3lELo",
        "colab_type": "text"
      },
      "source": [
        "여기부터 GAN의 구현을 해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgsQdsiqlC9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q imageio"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27spffKwmnaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGCuBMQI7sX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_best_results(testresult,test_label2):  #for optimizing.\n",
        "  classnum={}\n",
        "  testnum={}\n",
        "  resultmat=[]\n",
        "  bestmat=[]\n",
        "  for i in range(len(testresult)):\n",
        "    best_result=[0 for i in range(13)]\n",
        "    class_num=np.count_nonzero(test_label2[i]==1)+1\n",
        "    classidx=(-testresult[i]).argsort()[:class_num]\n",
        "    for k,j in enumerate(classidx):\n",
        "      if (k==0):\n",
        "        best_result[j]=1\n",
        "    bestmat.append(best_result)\n",
        "  bestmat=np.array(bestmat)\n",
        "  bestidx=mlb.inverse_transform(bestmat)\n",
        "  return bestidx"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V32sqv0MOOnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c3d7330b-cf42-41d6-a3e3-3da1e50ac1f5"
      },
      "source": [
        "trainX=train_matrix.reshape((150000,24,24))\n",
        "for i in tqdm(range(150),position=0):\n",
        "  if(i==0):\n",
        "    test_result=classifier.predict(train_matrix[1000*i:1000+1000*i])\n",
        "  else:\n",
        "    sub_testresult=classifier.predict(train_matrix[1000*i:1000+1000*i])\n",
        "    test_result=np.concatenate((test_result,sub_testresult))\n",
        "trainy=np.array(get_best_results(test_result,train_label2)).reshape((150000,))\n",
        "print('Train', trainX.shape, trainy.shape)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 150/150 [00:08<00:00, 18.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train (150000, 24, 24) (150000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCe1Clvjvz1-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52668a20-47e7-458d-b06d-5d6006fdab21"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# 라벨 인코더 생성\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# X_train데이터를 이용 피팅하고 라벨숫자로 변환한다\n",
        "encoder.fit(trainy)\n",
        "trainy = encoder.transform(trainy)\n",
        "print(trainy)\n",
        "trainy=np.array(trainy)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 3 10  6 ...  7  9  8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiRvfSRjmyEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.ndimage\n",
        "def blur_image(matrix):\n",
        "  sigma_y = 1.0\n",
        "  sigma_x = 1.0\n",
        "  inputmat=matrix\n",
        "  \"\"\"\n",
        "  # Plot input array\n",
        "  plt.imshow(inputmat, cmap='Blues', interpolation='nearest')\n",
        "  plt.xlabel(\"$x$\")\n",
        "  plt.ylabel(\"$y$\")\n",
        "  plt.savefig(\"array.png\")\n",
        "  \"\"\"\n",
        "  # Apply gaussian filter\n",
        "  sigma = [sigma_y, sigma_x]\n",
        "  y = sp.ndimage.filters.gaussian_filter(inputmat, sigma, mode='constant')\n",
        "  \"\"\"\n",
        "  # Display filtered array\n",
        "  plt.imshow(y, cmap='Blues', interpolation='nearest')\n",
        "  plt.xlabel(\"$x$\")\n",
        "  plt.ylabel(\"$y$\")\n",
        "  plt.title(\"$\\sigma_x = \" + str(sigma_x) + \"\\quad \\sigma_y = \" + str(sigma_y) + \"$\")\n",
        "  plt.savefig(\"smooth_array_\" + str(sigma_x) + \"_\" + str(sigma_y) + \".png\")\n",
        "  \"\"\"\n",
        "  return y\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHYRcDkY4GgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blur_trainX=copy.deepcopy(trainX)\n",
        "for i,matrix in enumerate(trainX):\n",
        "  blur_trainX[i]=blur_image(matrix)\n",
        "trainX=blur_trainX"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJr2T_F3wHVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/gaborvecsei/CDCGAN-Keras/tree/master/cdcgan\n",
        "#위 깃헙의 코드를 사용하는겁니다\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "from keras import models, layers\n",
        "\n",
        "ACTIVATION = layers.Activation(\"tanh\")\n",
        "\n",
        "\n",
        "def generator_model():\n",
        "  with tf.device('/gpu:0'):\n",
        "    # Prepare noise input\n",
        "    input_z = layers.Input((13,))\n",
        "    dense_z_1 = layers.Dense(1024)(input_z)\n",
        "    act_z_1 = ACTIVATION(dense_z_1)\n",
        "    dense_z_2 = layers.Dense(128 * 6* 6)(act_z_1)\n",
        "    bn_z_1 = layers.BatchNormalization()(dense_z_2)\n",
        "    reshape_z = layers.Reshape((6, 6, 128), input_shape=(128 * 6 * 6,))(bn_z_1)\n",
        "\n",
        "    # Prepare Conditional (label) input\n",
        "    input_c = layers.Input((13,))\n",
        "    dense_c_1 = layers.Dense(1024)(input_c)\n",
        "    act_c_1 = ACTIVATION(dense_c_1)\n",
        "    dense_c_2 = layers.Dense(128 * 6 * 6)(act_c_1)\n",
        "    bn_c_1 = layers.BatchNormalization()(dense_c_2)\n",
        "    reshape_c = layers.Reshape((6, 6, 128), input_shape=(128 * 6 * 6,))(bn_c_1)\n",
        "\n",
        "    # Combine input source\n",
        "    concat_z_c = layers.Concatenate()([reshape_z, reshape_c])\n",
        "\n",
        "    # Image generation with the concatenated inputs\n",
        "    up_1 = layers.UpSampling2D(size=(2, 2))(concat_z_c)\n",
        "    conv_1 = layers.Conv2D(64, (5, 5), padding='same')(up_1)\n",
        "    act_1 = ACTIVATION(conv_1)\n",
        "    up_2 = layers.UpSampling2D(size=(2, 2))(act_1)\n",
        "    conv_2 = layers.Conv2D(1, (5, 5), padding='same')(up_2)\n",
        "    act_2 = layers.Activation(\"tanh\")(conv_2)\n",
        "    model = models.Model(inputs=[input_z, input_c], outputs=act_2)\n",
        "    return model\n",
        "\n",
        "\n",
        "def discriminator_model():\n",
        "  with tf.device('/gpu:0'):\n",
        "    input_gen_image = layers.Input((24, 24, 1))\n",
        "    conv_1_image = layers.Conv2D(64, (5, 5), padding='same')(input_gen_image)\n",
        "    act_1_image = ACTIVATION(conv_1_image)\n",
        "    pool_1_image = layers.MaxPooling2D(pool_size=(2, 2))(act_1_image)\n",
        "    conv_2_image = layers.Conv2D(128, (5, 5))(pool_1_image)\n",
        "    act_2_image = ACTIVATION(conv_2_image)\n",
        "    pool_2_image = layers.MaxPooling2D(pool_size=(2, 2))(act_2_image)\n",
        "\n",
        "    input_c = layers.Input((13,))\n",
        "    dense_1_c = layers.Dense(1024)(input_c)\n",
        "    act_1_c = ACTIVATION(dense_1_c)\n",
        "    dense_2_c = layers.Dense(4 * 4 * 128)(act_1_c)\n",
        "    bn_c = layers.BatchNormalization()(dense_2_c)\n",
        "    reshaped_c = layers.Reshape((4, 4, 128))(bn_c)\n",
        "\n",
        "    concat = layers.Concatenate()([pool_2_image, reshaped_c])\n",
        "\n",
        "    flat = layers.Flatten()(concat)\n",
        "    dense_1 = layers.Dense(1024)(flat)\n",
        "    act_1 = ACTIVATION(dense_1)\n",
        "    dense_2 = layers.Dense(1)(act_1)\n",
        "    act_2 = layers.Activation('sigmoid')(dense_2)\n",
        "    model = models.Model(inputs=[input_gen_image, input_c], outputs=act_2)\n",
        "    return model\n",
        "\n",
        "\n",
        "def generator_containing_discriminator(g, d):\n",
        "  with tf.device('/gpu:0'):\n",
        "    input_z = layers.Input((13,))\n",
        "    input_c = layers.Input((13,))\n",
        "    gen_image = g([input_z, input_c])\n",
        "    d.trainable = False\n",
        "    is_real = d([gen_image, input_c])\n",
        "    model = models.Model(inputs=[input_z, input_c], outputs=is_real)\n",
        "    return model"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNHFwwLpm85B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "import cv2\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def combine_images(generated_images):\n",
        "    num_images = generated_images.shape[0]\n",
        "    new_width = int(math.sqrt(num_images))\n",
        "    new_height = int(math.ceil(float(num_images) / new_width))\n",
        "    grid_shape = generated_images.shape[1:3]\n",
        "    grid_image = np.zeros((new_height * grid_shape[0], new_width * grid_shape[1]), dtype=generated_images.dtype)\n",
        "    for index, img in enumerate(generated_images):\n",
        "        i = int(index / new_width)\n",
        "        j = index % new_width\n",
        "        grid_image[i * grid_shape[0]:(i + 1) * grid_shape[0], j * grid_shape[1]:(j + 1) * grid_shape[1]] = \\\n",
        "            img[:, :, 0]\n",
        "    return grid_image\n",
        "\n",
        "\n",
        "def generate_noise(shape: tuple):\n",
        "    noise = np.random.uniform(0, 1, size=shape)\n",
        "    return noise\n",
        "\n",
        "\n",
        "def generate_condition_embedding(label: int, nb_of_label_embeddings: int):\n",
        "    label_embeddings = np.zeros((nb_of_label_embeddings, 13))\n",
        "    label_embeddings[:, label] = 1\n",
        "    return label_embeddings\n",
        "\n",
        "\n",
        "def generate_images(generator, nb_images: int, label: int):\n",
        "    noise = generate_noise((nb_images, 13))\n",
        "    label_batch = generate_condition_embedding(label, nb_images)\n",
        "    generated_images = generator.predict([noise, label_batch], verbose=0)\n",
        "    return generated_images\n",
        "\n",
        "\n",
        "def generate_mnist_image_grid(generator, title: str = \"Generated images\"):\n",
        "    generated_images = []\n",
        "\n",
        "    for i in range(10):\n",
        "        noise = generate_noise((10, 13))\n",
        "        label_input = generate_condition_embedding(i, 10)\n",
        "        gen_images = generator.predict([noise, label_input], verbose=0)\n",
        "        generated_images.extend(gen_images)\n",
        "\n",
        "    generated_images = np.array(generated_images)\n",
        "    image_grid = combine_images(generated_images)\n",
        "    image_grid = inverse_transform_images(image_grid)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.axis(\"off\")\n",
        "    ax.imshow(image_grid, cmap=\"gray\")\n",
        "    ax.set_title(title)\n",
        "    fig.canvas.draw()\n",
        "\n",
        "    image = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
        "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def save_generated_image(image, epoch, iteration, folder_path):\n",
        "    if not os.path.isdir(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    file_path = \"{0}/{1}_{2}.png\".format(folder_path, epoch, iteration)\n",
        "    cv2.imwrite(file_path, image.astype(np.uint8))\n",
        "\n",
        "\n",
        "def transform_images(images: np.ndarray):\n",
        "    \"\"\"\n",
        "    [0,1]Transform images to [-1, 1]\n",
        "    \"\"\"\n",
        "    max_value=images.max()\n",
        "\n",
        "    images = (images.astype(np.float32) - (max_value/2)) / (max_value/2)\n",
        "    return images\n",
        "\n",
        "\n",
        "def inverse_transform_images(images: np.ndarray):\n",
        "    \"\"\"\n",
        "    From the [-1, 1] range transform the images back to [0, 255]\n",
        "    \"\"\"\n",
        "\n",
        "    images = images * 127.5 + 127.5\n",
        "    images = images.astype(np.uint8)\n",
        "    return images\n",
        "\n",
        "\n",
        "def convert_video_to_gif(input_video_path, output_gif_path, fps=24):\n",
        "    palette_image_path = \"palette.png\"\n",
        "    command_palette = 'ffmpeg -y -t 0 -i {0} -vf fps={1},scale=320:-1:flags=lanczos,palettegen {2}'.format(input_video_path,\n",
        "                                                                                                           fps,\n",
        "                                                                                                           palette_image_path)\n",
        "    command_convert = 'ffmpeg -y -t 0 -i {0} -i {1} -filter_complex \"fps={2},scale=320:-1:flags=lanczos[x];[x][1:v]paletteuse\" {3}'.format(input_video_path,palette_image_path, fps, output_gif_path)\n",
        "    \n",
        "    try:\n",
        "        subprocess.check_call(command_palette)\n",
        "        subprocess.check_call(command_convert)\n",
        "    except subprocess.CalledProcessError as exc:\n",
        "        print(exc.output)\n",
        "        raise\n",
        "    finally:\n",
        "        os.remove(palette_image_path)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xqJAsQDYM2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "cbdc2608-f9e2-4f64-90de-b5b3d66c1179"
      },
      "source": [
        "pip install git+https://github.com/gaborvecsei/Swiss-Army-Tensorboard.git"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/gaborvecsei/Swiss-Army-Tensorboard.git\n",
            "  Cloning https://github.com/gaborvecsei/Swiss-Army-Tensorboard.git to /tmp/pip-req-build-u8x_azl0\n",
            "  Running command git clone -q https://github.com/gaborvecsei/Swiss-Army-Tensorboard.git /tmp/pip-req-build-u8x_azl0\n",
            "Requirement already satisfied: numpy>=1.16.1 in /usr/local/lib/python3.6/dist-packages (from swiss-army-tensorboard==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: matplotlib>=3.1 in /usr/local/lib/python3.6/dist-packages (from swiss-army-tensorboard==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1->swiss-army-tensorboard==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1->swiss-army-tensorboard==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1->swiss-army-tensorboard==0.0.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1->swiss-army-tensorboard==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1->swiss-army-tensorboard==0.0.1) (1.15.0)\n",
            "Building wheels for collected packages: swiss-army-tensorboard\n",
            "  Building wheel for swiss-army-tensorboard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swiss-army-tensorboard: filename=swiss_army_tensorboard-0.0.1-cp36-none-any.whl size=5377 sha256=918f6444714a4dcc408717383342918406aaa1a5ff18d0267d3ee858fdb82928\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-txzdbq_o/wheels/ea/f7/db/67f8661f9c334ad8eda2d0dfbc21bcaea99f739c97f6008d41\n",
            "Successfully built swiss-army-tensorboard\n",
            "Installing collected packages: swiss-army-tensorboard\n",
            "Successfully installed swiss-army-tensorboard-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJjHn-Nngy8P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc696e2e-dee9-4d73-f552-1d08d0df54c4"
      },
      "source": [
        "import matplotlib\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras import utils as keras_utils\n",
        "from keras import optimizers\n",
        "from keras import datasets\n",
        "from swiss_army_tensorboard import tfboard_loggers\n",
        "from tqdm import tqdm\n",
        "#from cdcgan import cdcgan_models, cdcgan_utils\n",
        "#colab환경이라 안씁니다\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 200\n",
        "\n",
        "# Load & Prepare MNIST\n",
        "\n",
        "X_train=trainX\n",
        "y_train=trainy\n",
        "X_train = transform_images(X_train)\n",
        "X_train = X_train[:, :, :, None]\n",
        "\n",
        "y_train = keras_utils.to_categorical(y_train, 13) #원래는 13써야 하는데 추후 고려할 예정\n",
        "\n",
        "# Create the models\n",
        "\n",
        "print(\"Generator:\")\n",
        "G = generator_model()\n",
        "G.summary()\n",
        "\n",
        "print(\"Discriminator:\")\n",
        "D = discriminator_model()\n",
        "D.summary()\n",
        "\n",
        "print(\"Combined:\")\n",
        "GD = generator_containing_discriminator(G, D)\n",
        "GD.summary()\n",
        "\n",
        "optimizer = optimizers.Adam(0.0002, 0.5)\n",
        "\n",
        "\n",
        "G.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "GD.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "D.trainable = True\n",
        "D.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/swiss_army_tensorboard/tfboard_loggers/tfboard_loggers.py:47: The name tf.HistogramProto is deprecated. Please use tf.compat.v1.HistogramProto instead.\n",
            "\n",
            "Generator:\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 13)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 13)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         14336       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1024)         14336       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       multiple             0           dense_1[0][0]                    \n",
            "                                                                 dense_3[0][0]                    \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 4608)         4723200     activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 4608)         4723200     activation_1[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 4608)         18432       dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 4608)         18432       dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 6, 6, 128)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 6, 6, 128)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 6, 6, 256)    0           reshape_1[0][0]                  \n",
            "                                                                 reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 12, 12, 256)  0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 12, 12, 64)   409664      up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 24, 24, 64)   0           activation_1[2][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 24, 24, 1)    1601        up_sampling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 24, 24, 1)    0           conv2d_2[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 9,923,201\n",
            "Trainable params: 9,904,769\n",
            "Non-trainable params: 18,432\n",
            "__________________________________________________________________________________________________\n",
            "Discriminator:\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 24, 24, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 24, 24, 64)   1664        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       multiple             0           conv2d_3[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 dense_5[0][0]                    \n",
            "                                                                 dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 12, 12, 64)   0           activation_1[3][0]               \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 13)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 8, 8, 128)    204928      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1024)         14336       input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 2048)         2099200     activation_1[5][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 2048)         8192        dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 128)    0           activation_1[4][0]               \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 4, 4, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 4, 4, 256)    0           max_pooling2d_2[0][0]            \n",
            "                                                                 reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 4096)         0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1024)         4195328     flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1)            1025        activation_1[6][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 1)            0           dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 6,524,673\n",
            "Trainable params: 6,520,577\n",
            "Non-trainable params: 4,096\n",
            "__________________________________________________________________________________________________\n",
            "Combined:\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 13)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            (None, 13)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Model)                 (None, 24, 24, 1)    9923201     input_6[0][0]                    \n",
            "                                                                 input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "model_2 (Model)                 (None, 1)            6524673     model_1[1][0]                    \n",
            "                                                                 input_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 16,447,874\n",
            "Trainable params: 9,904,769\n",
            "Non-trainable params: 6,543,105\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiSLW8ipuBhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G.load_weights(\"/content/drive/My Drive/MARG/PPDDlist/GAN_result/generator.h5\")\n",
        "D.load_weights(\"/content/drive/My Drive/MARG/PPDDlist/GAN_result/discriminator.h5\")"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX3bvlrduAKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup Tensorboard loggers\n",
        "\n",
        "tfboard_loggers.TFBoardModelGraphLogger.log_graph(\"/content/drive/My Drive/MARG/PPDDlist/GAN_result/logs\", K.get_session())\n",
        "loss_logger = tfboard_loggers.TFBoardScalarLogger(\"/content/drive/My Drive/MARG/PPDDlist/GAN_result/logs/loss\")\n",
        "image_logger = tfboard_loggers.TFBoardImageLogger(\"/content/drive/My Drive/MARG/PPDDlist/GAN_result/logs/generated_images\")\n",
        "\n",
        "# Model Training\n",
        "\n",
        "iteration = 0\n",
        "\n",
        "nb_of_iterations_per_epoch = int(X_train.shape[0] / BATCH_SIZE)\n",
        "print(\"Number of iterations per epoch: {0}\".format(nb_of_iterations_per_epoch))\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    pbar = tqdm(desc=\"Epoch: {0}\".format(epoch), total=X_train.shape[0],position=0)\n",
        "\n",
        "    g_losses_for_epoch = []\n",
        "    d_losses_for_epoch = []\n",
        "\n",
        "    for i in range(nb_of_iterations_per_epoch):\n",
        "        noise = generate_noise((BATCH_SIZE, 13))\n",
        "\n",
        "        image_batch = X_train[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
        "        label_batch = y_train[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
        "\n",
        "        generated_images = G.predict([noise, label_batch], verbose=0)\n",
        "        X = np.concatenate((image_batch, generated_images))\n",
        "        y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
        "        label_batches_for_discriminator = np.concatenate((label_batch, label_batch))\n",
        "\n",
        "        D_loss = D.train_on_batch([X, label_batches_for_discriminator], y)\n",
        "        d_losses_for_epoch.append(D_loss)\n",
        "        loss_logger.log_scalar(\"discriminator_loss\", D_loss, iteration)\n",
        "\n",
        "        noise = generate_noise((BATCH_SIZE, 13))\n",
        "        D.trainable = False\n",
        "        G_loss = GD.train_on_batch([noise, label_batch], [1] * BATCH_SIZE)\n",
        "        D.trainable = True\n",
        "        g_losses_for_epoch.append(G_loss)\n",
        "        loss_logger.log_scalar(\"generator_loss\", G_loss, iteration)\n",
        "\n",
        "        pbar.update(BATCH_SIZE)\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    # Save a generated image for every epoch\n",
        "    image_grid = generate_mnist_image_grid(G, title=\"Epoch {0}\".format(epoch))\n",
        "    save_generated_image(image_grid, epoch, 0, \"/content/drive/My Drive/MARG/PPDDlist/GAN_result/generated_mnist_images_per_epoch\")\n",
        "    image_logger.log_images(\"/content/drive/My Drive/MARG/PPDDlist/GAN_result/generated_mnist_images_per_epoch\", [image_grid], epoch)\n",
        "\n",
        "    pbar.close()\n",
        "    print(\"D loss: {0}, G loss: {1}\".format(np.mean(d_losses_for_epoch), np.mean(g_losses_for_epoch)))\n",
        "\n",
        "    G.save_weights(\"/content/drive/My Drive/MARG/PPDDlist/GAN_result/generator.h5\")\n",
        "    D.save_weights(\"/content/drive/My Drive/MARG/PPDDlist/GAN_result/discriminator.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdEukvpITK_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(trainX[i], cmap='gray_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUyKRM_Vqd9Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "675292d4-7fb7-43fb-9bd5-90699c8a5d66"
      },
      "source": [
        "print(encoder.classes_)\n",
        "image=generate_images(G,10,12)\n",
        "H = image[0].reshape((24,24))\n",
        "\n",
        "fig = plt.figure(figsize=(14, 3))\n",
        "\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_title('colorMap')\n",
        "plt.imshow(H)\n",
        "ax.set_aspect('equal')\n",
        "\n",
        "cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
        "cax.get_xaxis().set_visible(False)\n",
        "cax.get_yaxis().set_visible(False)\n",
        "cax.patch.set_alpha(0)\n",
        "cax.set_frame_on(False)\n",
        "plt.colorbar(orientation='vertical')\n",
        "plt.show()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['One_rhythm' 'continuing_rhythm' 'down_leaping' 'down_steping'\n",
            " 'fast_rhythm' 'leaping_twisting' 'repeating' 'resting' 'staccato'\n",
            " 'steping_twisting' 'triplet' 'up_leaping' 'up_steping']\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x216 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exdtcOln9Qdq",
        "colab_type": "text"
      },
      "source": [
        "여기서부터 Simple RNN을 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNqr-1ZWLKdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RNNx_train=[]\n",
        "RNNy_train=[]\n",
        "RNNx_test=[]\n",
        "RNNy_test=[]\n",
        "RNN_kfoldx=[]\n",
        "RNN_kfoldy=[]\n",
        "for i in range(len(bar_matrix_list3)):\n",
        "  nowseq=[]\n",
        "  nowmat=np.array(bar_matrix_list3[i])\n",
        "  nowbars=classifier.predict(nowmat.reshape(len(nowmat),24,24,1) ) #이는 mlb를 통해 embedding 되어있다\n",
        "  for j,bars in enumerate(nowbars):\n",
        "    if(j==len(nowbars)-1):\n",
        "      if(i>=5000):\n",
        "        RNNy_test.append(np.argmax(bars))\n",
        "      else:\n",
        "        RNNy_train.append(np.argmax(bars))\n",
        "      RNN_kfoldy.append(np.argmax(bars))\n",
        "    else:\n",
        "      nowseq.append(np.argmax(bars))\n",
        "  if(i>=5000):\n",
        "    RNNx_test.append(nowseq)\n",
        "  else:\n",
        "    RNNx_train.append(nowseq)\n",
        "  RNN_kfoldx.append(nowseq)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oaBpBguS5j2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c39831c9-77ab-4a0d-d0e3-732ba470f978"
      },
      "source": [
        "print(len(RNNx_train),len(RNNx_train[0]))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbnUCrqw9O2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "RNNy_test=to_categorical(np.array(RNNy_test))\n",
        "RNNy_train=to_categorical(np.array(RNNy_train))\n",
        "RNN_kfoldy=to_categorical(np.array(RNN_kfoldy))\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "top_words=13\n",
        "# truncate and pad input sequences\n",
        "\n",
        "RNNx_train = sequence.pad_sequences(RNNx_train, maxlen=10)\n",
        "RNNx_test = sequence.pad_sequences(RNNx_test, maxlen=10)\n",
        "RNN_kfoldx = sequence.pad_sequences(RNN_kfoldx, maxlen=10)\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd4AzRMhPWyj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4ae3407-5167-46c4-f20b-016fc8d8d102"
      },
      "source": [
        "print(RNN_kfoldx.shape, RNN_kfoldy.shape)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 10) (10000, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmiN9lI_fbs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "model_path = '/content/drive/My Drive/models/' + 'RNN.h5'\n",
        "\n",
        "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_accuracy',\n",
        "                                verbose=1, save_best_only=True)\n",
        "callbacks = [cb_checkpoint]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtTnECWM9ooa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "954a8bcb-659a-4329-9457-ddf39632a7bc"
      },
      "source": [
        "from keras import regularizers\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "timesteps = 8\n",
        "\n",
        "RNNmodel = Sequential()\n",
        "RNNmodel.add(Embedding(top_words, embedding_vecor_length, input_length=10))\n",
        "RNNmodel.add(LSTM(100, return_sequences=True,\n",
        "               input_shape=(timesteps, 100)))  # returns a sequence of vectors of dimension 32\n",
        "RNNmodel.add(LSTM(100))  # return a single vector of dimension 32\n",
        "RNNmodel.add(Dropout(0.2))\n",
        "RNNmodel.add(Dense(13, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "RNNmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(RNNmodel.summary())\n",
        "\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 10, 32)            416       \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 10, 100)           53200     \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 13)                1313      \n",
            "=================================================================\n",
            "Total params: 135,329\n",
            "Trainable params: 135,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwd8KBt0SKZF",
        "colab_type": "text"
      },
      "source": [
        "아래거는 kfold방식. 더 아래에는 그냥 위에서 설정한 RNNmodel을 train : valid = 1:1로 학습하는 코드가 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oztvg4cPAxPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f449ad9-7391-4bac-b976-b72f35744b13"
      },
      "source": [
        "kfold = KFold(n_splits=5, shuffle=True, random_state=7)\n",
        "for i in range(3):\n",
        "  for train,valid in kfold.split(RNN_kfoldx,RNN_kfoldy):\n",
        "    RNNmodel.fit(RNN_kfoldx[train],RNN_kfoldy[train], validation_data=(RNN_kfoldx[valid],RNN_kfoldy[valid])\n",
        "    ,epochs=30, batch_size=64,callbacks=callbacks)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 3s 406us/step - loss: 2.3219 - accuracy: 0.3716 - val_loss: 2.1700 - val_accuracy: 0.3680\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 3s 320us/step - loss: 2.0962 - accuracy: 0.3736 - val_loss: 2.0473 - val_accuracy: 0.3680\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 3s 320us/step - loss: 2.0167 - accuracy: 0.3750 - val_loss: 2.0117 - val_accuracy: 0.3680\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 3s 321us/step - loss: 1.9765 - accuracy: 0.3787 - val_loss: 1.9810 - val_accuracy: 0.3805\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 3s 321us/step - loss: 1.9611 - accuracy: 0.3829 - val_loss: 1.9650 - val_accuracy: 0.3745\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 3s 320us/step - loss: 1.9371 - accuracy: 0.3871 - val_loss: 1.9509 - val_accuracy: 0.3785\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.9258 - accuracy: 0.3921 - val_loss: 1.9438 - val_accuracy: 0.3920\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.9116 - accuracy: 0.3923 - val_loss: 1.9407 - val_accuracy: 0.3835\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 1.9082 - accuracy: 0.3886 - val_loss: 1.9382 - val_accuracy: 0.3800\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 1.9016 - accuracy: 0.3988 - val_loss: 1.9330 - val_accuracy: 0.3880\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 3s 320us/step - loss: 1.8922 - accuracy: 0.3988 - val_loss: 1.9300 - val_accuracy: 0.3875\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 1.8882 - accuracy: 0.3991 - val_loss: 1.9238 - val_accuracy: 0.3795\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.8837 - accuracy: 0.4004 - val_loss: 1.9274 - val_accuracy: 0.3870\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.8789 - accuracy: 0.3997 - val_loss: 1.9317 - val_accuracy: 0.3940\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.8751 - accuracy: 0.4021 - val_loss: 1.9259 - val_accuracy: 0.3850\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 3s 342us/step - loss: 1.8717 - accuracy: 0.4014 - val_loss: 1.9253 - val_accuracy: 0.3860\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 3s 354us/step - loss: 1.8679 - accuracy: 0.3990 - val_loss: 1.9228 - val_accuracy: 0.3910\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 3s 351us/step - loss: 1.8628 - accuracy: 0.4069 - val_loss: 1.9214 - val_accuracy: 0.3895\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 3s 352us/step - loss: 1.8581 - accuracy: 0.4030 - val_loss: 1.9216 - val_accuracy: 0.3920\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 3s 318us/step - loss: 1.8554 - accuracy: 0.4049 - val_loss: 1.9261 - val_accuracy: 0.3925\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 1.8502 - accuracy: 0.4074 - val_loss: 1.9204 - val_accuracy: 0.3860\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.8446 - accuracy: 0.4042 - val_loss: 1.9140 - val_accuracy: 0.3895\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 3s 322us/step - loss: 1.8406 - accuracy: 0.4040 - val_loss: 1.9155 - val_accuracy: 0.3925\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 3s 318us/step - loss: 1.8388 - accuracy: 0.4075 - val_loss: 1.9202 - val_accuracy: 0.3935\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 1.8341 - accuracy: 0.4086 - val_loss: 1.9150 - val_accuracy: 0.3925\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 1.8284 - accuracy: 0.4083 - val_loss: 1.9139 - val_accuracy: 0.3915\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 1.8263 - accuracy: 0.4099 - val_loss: 1.9171 - val_accuracy: 0.3900\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 1.8210 - accuracy: 0.4078 - val_loss: 1.9212 - val_accuracy: 0.3925\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 1.8141 - accuracy: 0.4087 - val_loss: 1.9184 - val_accuracy: 0.3960\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 1.8074 - accuracy: 0.4095 - val_loss: 1.9179 - val_accuracy: 0.3935\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.8327 - accuracy: 0.4112 - val_loss: 1.8207 - val_accuracy: 0.4085\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 1.8207 - accuracy: 0.4091 - val_loss: 1.8268 - val_accuracy: 0.3980\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.8139 - accuracy: 0.4134 - val_loss: 1.8319 - val_accuracy: 0.4040\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 1.8073 - accuracy: 0.4085 - val_loss: 1.8346 - val_accuracy: 0.4060\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 3s 320us/step - loss: 1.7995 - accuracy: 0.4142 - val_loss: 1.8457 - val_accuracy: 0.3945\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 3s 327us/step - loss: 1.7921 - accuracy: 0.4148 - val_loss: 1.8371 - val_accuracy: 0.3985\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 3s 320us/step - loss: 1.7819 - accuracy: 0.4161 - val_loss: 1.8491 - val_accuracy: 0.3935\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.7739 - accuracy: 0.4199 - val_loss: 1.8449 - val_accuracy: 0.4015\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 1.7676 - accuracy: 0.4166 - val_loss: 1.8570 - val_accuracy: 0.4010\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 1.7601 - accuracy: 0.4155 - val_loss: 1.8595 - val_accuracy: 0.3980\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 1.7516 - accuracy: 0.4189 - val_loss: 1.8773 - val_accuracy: 0.3980\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 1.7403 - accuracy: 0.4254 - val_loss: 1.8750 - val_accuracy: 0.3895\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 1.7273 - accuracy: 0.4277 - val_loss: 1.8914 - val_accuracy: 0.3915\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 3s 318us/step - loss: 1.7163 - accuracy: 0.4310 - val_loss: 1.8838 - val_accuracy: 0.3980\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 1.7000 - accuracy: 0.4331 - val_loss: 1.8919 - val_accuracy: 0.3850\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 1.6927 - accuracy: 0.4379 - val_loss: 1.9002 - val_accuracy: 0.3845\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 1.6825 - accuracy: 0.4399 - val_loss: 1.9045 - val_accuracy: 0.3860\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 1.6713 - accuracy: 0.4424 - val_loss: 1.9040 - val_accuracy: 0.3880\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 3s 328us/step - loss: 1.6522 - accuracy: 0.4519 - val_loss: 1.9192 - val_accuracy: 0.3815\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 3s 328us/step - loss: 1.6433 - accuracy: 0.4541 - val_loss: 1.9242 - val_accuracy: 0.3890\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 1.6304 - accuracy: 0.4604 - val_loss: 1.9301 - val_accuracy: 0.3815\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.6180 - accuracy: 0.4641 - val_loss: 1.9400 - val_accuracy: 0.3835\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 1.6051 - accuracy: 0.4754 - val_loss: 1.9471 - val_accuracy: 0.3905\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 1.5881 - accuracy: 0.4766 - val_loss: 1.9609 - val_accuracy: 0.3615\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 1.5681 - accuracy: 0.4860 - val_loss: 1.9698 - val_accuracy: 0.3760\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 3s 321us/step - loss: 1.5542 - accuracy: 0.4958 - val_loss: 1.9830 - val_accuracy: 0.3630\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 1.5363 - accuracy: 0.5020 - val_loss: 1.9981 - val_accuracy: 0.3685\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 1.5209 - accuracy: 0.5138 - val_loss: 2.0089 - val_accuracy: 0.3785\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 1.5031 - accuracy: 0.5226 - val_loss: 2.0168 - val_accuracy: 0.3665\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 1.4895 - accuracy: 0.5264 - val_loss: 2.0381 - val_accuracy: 0.3710\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 1.6200 - accuracy: 0.4931 - val_loss: 1.4838 - val_accuracy: 0.5415\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 1.5832 - accuracy: 0.5015 - val_loss: 1.4967 - val_accuracy: 0.5055\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 1.5610 - accuracy: 0.5073 - val_loss: 1.5201 - val_accuracy: 0.5085\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 1.5410 - accuracy: 0.5190 - val_loss: 1.5252 - val_accuracy: 0.4975\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 1.5221 - accuracy: 0.5270 - val_loss: 1.5443 - val_accuracy: 0.4970\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 3s 321us/step - loss: 1.4920 - accuracy: 0.5385 - val_loss: 1.5526 - val_accuracy: 0.5020\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.4811 - accuracy: 0.5431 - val_loss: 1.5719 - val_accuracy: 0.4910\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 1.4604 - accuracy: 0.5505 - val_loss: 1.5751 - val_accuracy: 0.4955\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.4451 - accuracy: 0.5584 - val_loss: 1.6006 - val_accuracy: 0.4825\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 1.4196 - accuracy: 0.5698 - val_loss: 1.6185 - val_accuracy: 0.4880\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 1.4003 - accuracy: 0.5817 - val_loss: 1.6428 - val_accuracy: 0.4745\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 1.3725 - accuracy: 0.5940 - val_loss: 1.6381 - val_accuracy: 0.4805\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 1.3610 - accuracy: 0.5960 - val_loss: 1.6635 - val_accuracy: 0.4635\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 3s 321us/step - loss: 1.3435 - accuracy: 0.6054 - val_loss: 1.6836 - val_accuracy: 0.4630\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 1.3223 - accuracy: 0.6144 - val_loss: 1.6985 - val_accuracy: 0.4605\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 1.3000 - accuracy: 0.6286 - val_loss: 1.7264 - val_accuracy: 0.4575\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 1.2900 - accuracy: 0.6304 - val_loss: 1.7435 - val_accuracy: 0.4530\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 1.2725 - accuracy: 0.6375 - val_loss: 1.7595 - val_accuracy: 0.4645\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 1.2434 - accuracy: 0.6538 - val_loss: 1.7765 - val_accuracy: 0.4525\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 1.2341 - accuracy: 0.6539 - val_loss: 1.8054 - val_accuracy: 0.4365\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 1.2197 - accuracy: 0.6639 - val_loss: 1.8032 - val_accuracy: 0.4500\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 1.1938 - accuracy: 0.6771 - val_loss: 1.8341 - val_accuracy: 0.4400\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 1.1857 - accuracy: 0.6769 - val_loss: 1.8304 - val_accuracy: 0.4435\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 1.1739 - accuracy: 0.6842 - val_loss: 1.8609 - val_accuracy: 0.4340\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 1.1545 - accuracy: 0.6894 - val_loss: 1.8778 - val_accuracy: 0.4300\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 1.1489 - accuracy: 0.6938 - val_loss: 1.9138 - val_accuracy: 0.4250\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 1.1376 - accuracy: 0.6920 - val_loss: 1.8885 - val_accuracy: 0.4275\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 1.1148 - accuracy: 0.7075 - val_loss: 1.9047 - val_accuracy: 0.4315\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 1.0963 - accuracy: 0.7095 - val_loss: 1.9243 - val_accuracy: 0.4220\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 1.0844 - accuracy: 0.7204 - val_loss: 1.9585 - val_accuracy: 0.4275\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 1.3507 - accuracy: 0.6224 - val_loss: 1.0730 - val_accuracy: 0.7205\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 1.2761 - accuracy: 0.6447 - val_loss: 1.0929 - val_accuracy: 0.7090\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 1.2310 - accuracy: 0.6606 - val_loss: 1.0946 - val_accuracy: 0.7015\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 1.1936 - accuracy: 0.6815 - val_loss: 1.1029 - val_accuracy: 0.7030\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 1.1701 - accuracy: 0.6876 - val_loss: 1.1183 - val_accuracy: 0.6945\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 1.1485 - accuracy: 0.6971 - val_loss: 1.1542 - val_accuracy: 0.6790\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 1.1335 - accuracy: 0.7040 - val_loss: 1.1617 - val_accuracy: 0.6700\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 1.1112 - accuracy: 0.7116 - val_loss: 1.1711 - val_accuracy: 0.6760\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 1.0992 - accuracy: 0.7169 - val_loss: 1.1854 - val_accuracy: 0.6665\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 1.0835 - accuracy: 0.7243 - val_loss: 1.2011 - val_accuracy: 0.6555\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 1.0617 - accuracy: 0.7364 - val_loss: 1.2306 - val_accuracy: 0.6555\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 1.0495 - accuracy: 0.7355 - val_loss: 1.2630 - val_accuracy: 0.6290\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 1.0353 - accuracy: 0.7441 - val_loss: 1.2632 - val_accuracy: 0.6430\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 1.0240 - accuracy: 0.7511 - val_loss: 1.2834 - val_accuracy: 0.6275\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 1.0101 - accuracy: 0.7473 - val_loss: 1.2865 - val_accuracy: 0.6210\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 1.0119 - accuracy: 0.7531 - val_loss: 1.3110 - val_accuracy: 0.6180\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.9966 - accuracy: 0.7594 - val_loss: 1.3049 - val_accuracy: 0.6315\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.9890 - accuracy: 0.7625 - val_loss: 1.3425 - val_accuracy: 0.6155\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.9628 - accuracy: 0.7725 - val_loss: 1.3530 - val_accuracy: 0.6025\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.9554 - accuracy: 0.7761 - val_loss: 1.3664 - val_accuracy: 0.6005\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.9468 - accuracy: 0.7761 - val_loss: 1.3929 - val_accuracy: 0.5980\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.9568 - accuracy: 0.7747 - val_loss: 1.4027 - val_accuracy: 0.5900\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.9369 - accuracy: 0.7864 - val_loss: 1.4505 - val_accuracy: 0.5785\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.9192 - accuracy: 0.7916 - val_loss: 1.4439 - val_accuracy: 0.5855\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.9189 - accuracy: 0.7884 - val_loss: 1.4581 - val_accuracy: 0.5755\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.8998 - accuracy: 0.7993 - val_loss: 1.4382 - val_accuracy: 0.5780\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.8914 - accuracy: 0.8023 - val_loss: 1.4769 - val_accuracy: 0.5660\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.8807 - accuracy: 0.8048 - val_loss: 1.5131 - val_accuracy: 0.5585\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.8872 - accuracy: 0.8036 - val_loss: 1.5016 - val_accuracy: 0.5605\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.8727 - accuracy: 0.8076 - val_loss: 1.5161 - val_accuracy: 0.5540\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 1.1091 - accuracy: 0.7201 - val_loss: 0.9161 - val_accuracy: 0.7900\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 1.0457 - accuracy: 0.7352 - val_loss: 0.9041 - val_accuracy: 0.8060\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.9835 - accuracy: 0.7596 - val_loss: 0.8758 - val_accuracy: 0.7980\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 0.9344 - accuracy: 0.7826 - val_loss: 0.8897 - val_accuracy: 0.7880\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.9178 - accuracy: 0.7841 - val_loss: 0.8884 - val_accuracy: 0.7905\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.9105 - accuracy: 0.7910 - val_loss: 0.9072 - val_accuracy: 0.7815\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.8893 - accuracy: 0.8012 - val_loss: 0.9299 - val_accuracy: 0.7705\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.8774 - accuracy: 0.8016 - val_loss: 0.9343 - val_accuracy: 0.7725\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.8669 - accuracy: 0.8104 - val_loss: 0.9309 - val_accuracy: 0.7775\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.8527 - accuracy: 0.8086 - val_loss: 0.9554 - val_accuracy: 0.7690\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 0.8483 - accuracy: 0.8123 - val_loss: 0.9897 - val_accuracy: 0.7525\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.8349 - accuracy: 0.8199 - val_loss: 0.9822 - val_accuracy: 0.7470\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 3s 318us/step - loss: 0.8225 - accuracy: 0.8242 - val_loss: 0.9855 - val_accuracy: 0.7525\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.8314 - accuracy: 0.8198 - val_loss: 1.0197 - val_accuracy: 0.7400\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.8236 - accuracy: 0.8246 - val_loss: 1.0113 - val_accuracy: 0.7460\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 0.8132 - accuracy: 0.8279 - val_loss: 1.0604 - val_accuracy: 0.7175\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 3s 321us/step - loss: 0.8143 - accuracy: 0.8294 - val_loss: 1.0434 - val_accuracy: 0.7245\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.7974 - accuracy: 0.8347 - val_loss: 1.0839 - val_accuracy: 0.7165\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 3s 332us/step - loss: 0.7916 - accuracy: 0.8341 - val_loss: 1.0643 - val_accuracy: 0.7125\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 3s 346us/step - loss: 0.7836 - accuracy: 0.8380 - val_loss: 1.0819 - val_accuracy: 0.7010\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 3s 345us/step - loss: 0.7694 - accuracy: 0.8424 - val_loss: 1.1047 - val_accuracy: 0.7055\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 3s 344us/step - loss: 0.7804 - accuracy: 0.8386 - val_loss: 1.1177 - val_accuracy: 0.6945\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 3s 318us/step - loss: 0.7608 - accuracy: 0.8445 - val_loss: 1.1360 - val_accuracy: 0.6865\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.7628 - accuracy: 0.8443 - val_loss: 1.1490 - val_accuracy: 0.6905\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.7636 - accuracy: 0.8413 - val_loss: 1.1432 - val_accuracy: 0.6815\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.7466 - accuracy: 0.8485 - val_loss: 1.1608 - val_accuracy: 0.6760\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.7550 - accuracy: 0.8469 - val_loss: 1.1869 - val_accuracy: 0.6755\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.7597 - accuracy: 0.8479 - val_loss: 1.1982 - val_accuracy: 0.6660\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.7385 - accuracy: 0.8544 - val_loss: 1.1833 - val_accuracy: 0.6750\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.7299 - accuracy: 0.8530 - val_loss: 1.1996 - val_accuracy: 0.6620\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.9281 - accuracy: 0.7803 - val_loss: 0.7603 - val_accuracy: 0.8380\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.8721 - accuracy: 0.7977 - val_loss: 0.7720 - val_accuracy: 0.8345\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.8250 - accuracy: 0.8176 - val_loss: 0.7830 - val_accuracy: 0.8240\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.7894 - accuracy: 0.8328 - val_loss: 0.7450 - val_accuracy: 0.8485\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.7650 - accuracy: 0.8405 - val_loss: 0.7623 - val_accuracy: 0.8360\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.7471 - accuracy: 0.8460 - val_loss: 0.7641 - val_accuracy: 0.8365\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.7342 - accuracy: 0.8490 - val_loss: 0.7785 - val_accuracy: 0.8250\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.7220 - accuracy: 0.8550 - val_loss: 0.7901 - val_accuracy: 0.8210\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.7109 - accuracy: 0.8610 - val_loss: 0.7971 - val_accuracy: 0.8170\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.7114 - accuracy: 0.8625 - val_loss: 0.8208 - val_accuracy: 0.8155\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.7194 - accuracy: 0.8529 - val_loss: 0.8409 - val_accuracy: 0.8010\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 3s 320us/step - loss: 0.7091 - accuracy: 0.8631 - val_loss: 0.8497 - val_accuracy: 0.7915\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.6968 - accuracy: 0.8639 - val_loss: 0.8625 - val_accuracy: 0.7890\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.7036 - accuracy: 0.8641 - val_loss: 0.8723 - val_accuracy: 0.7870\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.6891 - accuracy: 0.8645 - val_loss: 0.8853 - val_accuracy: 0.7810\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.6888 - accuracy: 0.8661 - val_loss: 0.9007 - val_accuracy: 0.7795\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.6877 - accuracy: 0.8661 - val_loss: 0.9133 - val_accuracy: 0.7705\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.6820 - accuracy: 0.8686 - val_loss: 0.9631 - val_accuracy: 0.7490\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.6972 - accuracy: 0.8600 - val_loss: 0.9506 - val_accuracy: 0.7595\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.6966 - accuracy: 0.8597 - val_loss: 0.9831 - val_accuracy: 0.7490\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 3s 325us/step - loss: 0.6709 - accuracy: 0.8720 - val_loss: 0.9577 - val_accuracy: 0.7440\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 3s 329us/step - loss: 0.6717 - accuracy: 0.8730 - val_loss: 0.9694 - val_accuracy: 0.7490\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.6568 - accuracy: 0.8760 - val_loss: 0.9857 - val_accuracy: 0.7480\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.6444 - accuracy: 0.8794 - val_loss: 0.9875 - val_accuracy: 0.7440\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.6533 - accuracy: 0.8758 - val_loss: 1.0162 - val_accuracy: 0.7240\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.6807 - accuracy: 0.8649 - val_loss: 1.0658 - val_accuracy: 0.7155\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.6843 - accuracy: 0.8674 - val_loss: 1.0545 - val_accuracy: 0.7130\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.6666 - accuracy: 0.8704 - val_loss: 1.0604 - val_accuracy: 0.7160\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.6333 - accuracy: 0.8808 - val_loss: 1.0431 - val_accuracy: 0.7185\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.6272 - accuracy: 0.8844 - val_loss: 1.0384 - val_accuracy: 0.7150\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.8122 - accuracy: 0.8142 - val_loss: 0.7193 - val_accuracy: 0.8465\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.7643 - accuracy: 0.8336 - val_loss: 0.6789 - val_accuracy: 0.8610\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.7145 - accuracy: 0.8522 - val_loss: 0.6672 - val_accuracy: 0.8620\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.6828 - accuracy: 0.8679 - val_loss: 0.6697 - val_accuracy: 0.8615\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.6586 - accuracy: 0.8727 - val_loss: 0.6766 - val_accuracy: 0.8560\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.6433 - accuracy: 0.8773 - val_loss: 0.6740 - val_accuracy: 0.8580\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.6364 - accuracy: 0.8798 - val_loss: 0.6999 - val_accuracy: 0.8465\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.6338 - accuracy: 0.8792 - val_loss: 0.6928 - val_accuracy: 0.8490\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.6291 - accuracy: 0.8786 - val_loss: 0.6965 - val_accuracy: 0.8420\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.6238 - accuracy: 0.8831 - val_loss: 0.7276 - val_accuracy: 0.8320\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.6191 - accuracy: 0.8831 - val_loss: 0.7231 - val_accuracy: 0.8370\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.6166 - accuracy: 0.8873 - val_loss: 0.7353 - val_accuracy: 0.8330\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.6264 - accuracy: 0.8801 - val_loss: 0.7687 - val_accuracy: 0.8180\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.6483 - accuracy: 0.8720 - val_loss: 0.8805 - val_accuracy: 0.7800\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.6426 - accuracy: 0.8776 - val_loss: 0.8050 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 0.6065 - accuracy: 0.8894 - val_loss: 0.7978 - val_accuracy: 0.8010\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.6030 - accuracy: 0.8866 - val_loss: 0.8187 - val_accuracy: 0.8055\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.5919 - accuracy: 0.8919 - val_loss: 0.7937 - val_accuracy: 0.8170\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5873 - accuracy: 0.8904 - val_loss: 0.8119 - val_accuracy: 0.8035\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.5865 - accuracy: 0.8942 - val_loss: 0.8197 - val_accuracy: 0.7965\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.5884 - accuracy: 0.8924 - val_loss: 0.8491 - val_accuracy: 0.7850\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.5939 - accuracy: 0.8878 - val_loss: 0.8581 - val_accuracy: 0.7885\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.6173 - accuracy: 0.8816 - val_loss: 0.9136 - val_accuracy: 0.7710\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.6092 - accuracy: 0.8866 - val_loss: 0.8901 - val_accuracy: 0.7765\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.5816 - accuracy: 0.8935 - val_loss: 0.9023 - val_accuracy: 0.7710\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.5691 - accuracy: 0.8992 - val_loss: 0.9050 - val_accuracy: 0.7675\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.5860 - accuracy: 0.8923 - val_loss: 0.9507 - val_accuracy: 0.7530\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.6132 - accuracy: 0.8824 - val_loss: 0.9748 - val_accuracy: 0.7380\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.6077 - accuracy: 0.8846 - val_loss: 0.9988 - val_accuracy: 0.7340\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.5873 - accuracy: 0.8900 - val_loss: 0.9627 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.7436 - accuracy: 0.8326 - val_loss: 0.6570 - val_accuracy: 0.8665\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.6948 - accuracy: 0.8536 - val_loss: 0.6287 - val_accuracy: 0.8775\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.6422 - accuracy: 0.8720 - val_loss: 0.6105 - val_accuracy: 0.8735\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.6111 - accuracy: 0.8790 - val_loss: 0.6349 - val_accuracy: 0.8725\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.5968 - accuracy: 0.8838 - val_loss: 0.6051 - val_accuracy: 0.8785\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.5768 - accuracy: 0.8930 - val_loss: 0.6083 - val_accuracy: 0.8795\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.5649 - accuracy: 0.8974 - val_loss: 0.6182 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.5620 - accuracy: 0.8979 - val_loss: 0.6300 - val_accuracy: 0.8705\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5582 - accuracy: 0.8989 - val_loss: 0.6226 - val_accuracy: 0.8760\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5567 - accuracy: 0.8978 - val_loss: 0.6375 - val_accuracy: 0.8645\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.5499 - accuracy: 0.8988 - val_loss: 0.6395 - val_accuracy: 0.8645\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.5521 - accuracy: 0.9010 - val_loss: 0.6572 - val_accuracy: 0.8515\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.5701 - accuracy: 0.8914 - val_loss: 0.6976 - val_accuracy: 0.8485\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.5965 - accuracy: 0.8871 - val_loss: 0.7175 - val_accuracy: 0.8305\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.6064 - accuracy: 0.8838 - val_loss: 0.7764 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.6229 - accuracy: 0.8784 - val_loss: 0.7894 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.6054 - accuracy: 0.8823 - val_loss: 0.7479 - val_accuracy: 0.8290\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.5561 - accuracy: 0.8981 - val_loss: 0.7228 - val_accuracy: 0.8335\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.5326 - accuracy: 0.9040 - val_loss: 0.7207 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.5250 - accuracy: 0.9061 - val_loss: 0.7205 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.5246 - accuracy: 0.9064 - val_loss: 0.7146 - val_accuracy: 0.8360\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.5194 - accuracy: 0.9066 - val_loss: 0.7319 - val_accuracy: 0.8315\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.5250 - accuracy: 0.9025 - val_loss: 0.7673 - val_accuracy: 0.8175\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5510 - accuracy: 0.8960 - val_loss: 0.8125 - val_accuracy: 0.8055\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.5470 - accuracy: 0.8990 - val_loss: 0.7850 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.5608 - accuracy: 0.8941 - val_loss: 0.8817 - val_accuracy: 0.7745\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.5816 - accuracy: 0.8879 - val_loss: 0.8766 - val_accuracy: 0.7765\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.5755 - accuracy: 0.8885 - val_loss: 0.8526 - val_accuracy: 0.7770\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.5392 - accuracy: 0.8992 - val_loss: 0.7934 - val_accuracy: 0.8050\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5173 - accuracy: 0.9046 - val_loss: 0.8198 - val_accuracy: 0.7930\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.6647 - accuracy: 0.8580 - val_loss: 0.5401 - val_accuracy: 0.9030\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.6484 - accuracy: 0.8608 - val_loss: 0.5959 - val_accuracy: 0.8735\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.6161 - accuracy: 0.8726 - val_loss: 0.5655 - val_accuracy: 0.8925\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.5776 - accuracy: 0.8888 - val_loss: 0.5556 - val_accuracy: 0.8880\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.5501 - accuracy: 0.8981 - val_loss: 0.5333 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.5365 - accuracy: 0.9010 - val_loss: 0.5381 - val_accuracy: 0.8920\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.5290 - accuracy: 0.9032 - val_loss: 0.5395 - val_accuracy: 0.8895\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.5226 - accuracy: 0.9043 - val_loss: 0.5449 - val_accuracy: 0.8900\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.5232 - accuracy: 0.9056 - val_loss: 0.5631 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 2s 303us/step - loss: 0.5273 - accuracy: 0.9016 - val_loss: 0.5640 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.5206 - accuracy: 0.9013 - val_loss: 0.5561 - val_accuracy: 0.8900\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5159 - accuracy: 0.9056 - val_loss: 0.5670 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5101 - accuracy: 0.9055 - val_loss: 0.5757 - val_accuracy: 0.8775\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5147 - accuracy: 0.9054 - val_loss: 0.5733 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.5087 - accuracy: 0.9084 - val_loss: 0.5908 - val_accuracy: 0.8775\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.5091 - accuracy: 0.9071 - val_loss: 0.5986 - val_accuracy: 0.8765\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.5063 - accuracy: 0.9084 - val_loss: 0.6002 - val_accuracy: 0.8760\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5328 - accuracy: 0.8992 - val_loss: 0.6637 - val_accuracy: 0.8490\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.5927 - accuracy: 0.8825 - val_loss: 0.7259 - val_accuracy: 0.8255\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 0.5914 - accuracy: 0.8823 - val_loss: 0.6974 - val_accuracy: 0.8435\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 0.5633 - accuracy: 0.8921 - val_loss: 0.6914 - val_accuracy: 0.8380\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.5385 - accuracy: 0.9000 - val_loss: 0.6682 - val_accuracy: 0.8425\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 0.5243 - accuracy: 0.8994 - val_loss: 0.6883 - val_accuracy: 0.8405\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 3s 337us/step - loss: 0.5073 - accuracy: 0.9062 - val_loss: 0.6758 - val_accuracy: 0.8480\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 3s 342us/step - loss: 0.5002 - accuracy: 0.9081 - val_loss: 0.6489 - val_accuracy: 0.8600\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 3s 341us/step - loss: 0.4860 - accuracy: 0.9139 - val_loss: 0.6467 - val_accuracy: 0.8555\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 3s 343us/step - loss: 0.4853 - accuracy: 0.9100 - val_loss: 0.6580 - val_accuracy: 0.8555\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4841 - accuracy: 0.9096 - val_loss: 0.6561 - val_accuracy: 0.8495\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4822 - accuracy: 0.9106 - val_loss: 0.6698 - val_accuracy: 0.8470\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4881 - accuracy: 0.9072 - val_loss: 0.6910 - val_accuracy: 0.8430\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.6561 - accuracy: 0.8514 - val_loss: 0.6506 - val_accuracy: 0.8585\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.6306 - accuracy: 0.8639 - val_loss: 0.5662 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5565 - accuracy: 0.8885 - val_loss: 0.5323 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.5267 - accuracy: 0.8999 - val_loss: 0.5340 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.5131 - accuracy: 0.9035 - val_loss: 0.5197 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.4962 - accuracy: 0.9074 - val_loss: 0.5200 - val_accuracy: 0.8995\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4922 - accuracy: 0.9093 - val_loss: 0.5230 - val_accuracy: 0.8945\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4847 - accuracy: 0.9109 - val_loss: 0.5252 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4827 - accuracy: 0.9070 - val_loss: 0.5277 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4831 - accuracy: 0.9074 - val_loss: 0.5342 - val_accuracy: 0.8925\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4837 - accuracy: 0.9090 - val_loss: 0.5499 - val_accuracy: 0.8885\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.5029 - accuracy: 0.9029 - val_loss: 0.5643 - val_accuracy: 0.8815\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.4955 - accuracy: 0.9072 - val_loss: 0.5806 - val_accuracy: 0.8775\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5133 - accuracy: 0.9014 - val_loss: 0.6112 - val_accuracy: 0.8675\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.5232 - accuracy: 0.8971 - val_loss: 0.6168 - val_accuracy: 0.8660\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.5076 - accuracy: 0.9043 - val_loss: 0.6223 - val_accuracy: 0.8635\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.5082 - accuracy: 0.9054 - val_loss: 0.6321 - val_accuracy: 0.8595\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.4998 - accuracy: 0.9025 - val_loss: 0.6212 - val_accuracy: 0.8675\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4876 - accuracy: 0.9089 - val_loss: 0.6012 - val_accuracy: 0.8720\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4795 - accuracy: 0.9080 - val_loss: 0.5930 - val_accuracy: 0.8755\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.4683 - accuracy: 0.9095 - val_loss: 0.5902 - val_accuracy: 0.8740\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.4639 - accuracy: 0.9107 - val_loss: 0.5939 - val_accuracy: 0.8740\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.4595 - accuracy: 0.9143 - val_loss: 0.6047 - val_accuracy: 0.8685\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 3s 330us/step - loss: 0.4621 - accuracy: 0.9104 - val_loss: 0.6017 - val_accuracy: 0.8685\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 3s 325us/step - loss: 0.4589 - accuracy: 0.9128 - val_loss: 0.6125 - val_accuracy: 0.8630\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4549 - accuracy: 0.9140 - val_loss: 0.6351 - val_accuracy: 0.8565\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4707 - accuracy: 0.9074 - val_loss: 0.7167 - val_accuracy: 0.8310\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.5351 - accuracy: 0.8935 - val_loss: 0.8310 - val_accuracy: 0.7860\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.5934 - accuracy: 0.8724 - val_loss: 0.7706 - val_accuracy: 0.8090\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 2s 303us/step - loss: 0.5294 - accuracy: 0.8945 - val_loss: 0.7433 - val_accuracy: 0.8145\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.5840 - accuracy: 0.8763 - val_loss: 0.5439 - val_accuracy: 0.8870\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.5590 - accuracy: 0.8865 - val_loss: 0.5122 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5044 - accuracy: 0.9029 - val_loss: 0.5026 - val_accuracy: 0.9030\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5045 - accuracy: 0.9034 - val_loss: 0.5031 - val_accuracy: 0.9020\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4848 - accuracy: 0.9074 - val_loss: 0.5200 - val_accuracy: 0.8930\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4758 - accuracy: 0.9106 - val_loss: 0.5042 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4639 - accuracy: 0.9115 - val_loss: 0.5066 - val_accuracy: 0.8960\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4581 - accuracy: 0.9143 - val_loss: 0.5134 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4555 - accuracy: 0.9125 - val_loss: 0.5182 - val_accuracy: 0.8945\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4518 - accuracy: 0.9134 - val_loss: 0.5167 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.4519 - accuracy: 0.9144 - val_loss: 0.5275 - val_accuracy: 0.8905\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4499 - accuracy: 0.9143 - val_loss: 0.5249 - val_accuracy: 0.8900\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4513 - accuracy: 0.9137 - val_loss: 0.5346 - val_accuracy: 0.8920\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4522 - accuracy: 0.9147 - val_loss: 0.5362 - val_accuracy: 0.8900\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4503 - accuracy: 0.9119 - val_loss: 0.5791 - val_accuracy: 0.8715\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4957 - accuracy: 0.9001 - val_loss: 0.6199 - val_accuracy: 0.8570\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.5020 - accuracy: 0.9020 - val_loss: 0.6492 - val_accuracy: 0.8495\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.5441 - accuracy: 0.8886 - val_loss: 0.6906 - val_accuracy: 0.8330\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.5324 - accuracy: 0.8919 - val_loss: 0.6802 - val_accuracy: 0.8430\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.5029 - accuracy: 0.9007 - val_loss: 0.6389 - val_accuracy: 0.8580\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4768 - accuracy: 0.9093 - val_loss: 0.6011 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4566 - accuracy: 0.9114 - val_loss: 0.6149 - val_accuracy: 0.8635\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4460 - accuracy: 0.9145 - val_loss: 0.5990 - val_accuracy: 0.8700\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.4381 - accuracy: 0.9160 - val_loss: 0.5929 - val_accuracy: 0.8715\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.4346 - accuracy: 0.9161 - val_loss: 0.5921 - val_accuracy: 0.8775\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.4360 - accuracy: 0.9140 - val_loss: 0.6088 - val_accuracy: 0.8720\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.4289 - accuracy: 0.9160 - val_loss: 0.6110 - val_accuracy: 0.8650\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.4309 - accuracy: 0.9175 - val_loss: 0.6223 - val_accuracy: 0.8585\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4296 - accuracy: 0.9165 - val_loss: 0.6176 - val_accuracy: 0.8650\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4288 - accuracy: 0.9165 - val_loss: 0.6241 - val_accuracy: 0.8605\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.5751 - accuracy: 0.8759 - val_loss: 0.5931 - val_accuracy: 0.8680\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.6089 - accuracy: 0.8645 - val_loss: 0.5404 - val_accuracy: 0.8900\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.5436 - accuracy: 0.8888 - val_loss: 0.5464 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4999 - accuracy: 0.9009 - val_loss: 0.5241 - val_accuracy: 0.8845\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.4738 - accuracy: 0.9055 - val_loss: 0.4913 - val_accuracy: 0.8945\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4536 - accuracy: 0.9106 - val_loss: 0.4835 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4477 - accuracy: 0.9149 - val_loss: 0.4922 - val_accuracy: 0.8925\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4438 - accuracy: 0.9135 - val_loss: 0.4932 - val_accuracy: 0.8920\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4397 - accuracy: 0.9137 - val_loss: 0.5017 - val_accuracy: 0.8890\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.4360 - accuracy: 0.9164 - val_loss: 0.5037 - val_accuracy: 0.8925\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4338 - accuracy: 0.9145 - val_loss: 0.5100 - val_accuracy: 0.8915\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4376 - accuracy: 0.9158 - val_loss: 0.5141 - val_accuracy: 0.8905\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4352 - accuracy: 0.9153 - val_loss: 0.5253 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.4538 - accuracy: 0.9134 - val_loss: 0.5609 - val_accuracy: 0.8735\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4607 - accuracy: 0.9115 - val_loss: 0.5958 - val_accuracy: 0.8650\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4606 - accuracy: 0.9096 - val_loss: 0.5927 - val_accuracy: 0.8635\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4657 - accuracy: 0.9082 - val_loss: 0.5781 - val_accuracy: 0.8650\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4458 - accuracy: 0.9120 - val_loss: 0.5627 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4465 - accuracy: 0.9122 - val_loss: 0.6042 - val_accuracy: 0.8580\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4725 - accuracy: 0.9056 - val_loss: 0.6009 - val_accuracy: 0.8620\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 2s 303us/step - loss: 0.4523 - accuracy: 0.9114 - val_loss: 0.6215 - val_accuracy: 0.8600\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4556 - accuracy: 0.9080 - val_loss: 0.6223 - val_accuracy: 0.8550\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4474 - accuracy: 0.9116 - val_loss: 0.6281 - val_accuracy: 0.8500\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4433 - accuracy: 0.9140 - val_loss: 0.6238 - val_accuracy: 0.8570\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4698 - accuracy: 0.9070 - val_loss: 0.6969 - val_accuracy: 0.8260\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.4803 - accuracy: 0.9055 - val_loss: 0.6912 - val_accuracy: 0.8245\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.4624 - accuracy: 0.9084 - val_loss: 0.6499 - val_accuracy: 0.8460\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.4410 - accuracy: 0.9129 - val_loss: 0.6192 - val_accuracy: 0.8570\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.4238 - accuracy: 0.9195 - val_loss: 0.6185 - val_accuracy: 0.8595\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.4193 - accuracy: 0.9187 - val_loss: 0.6200 - val_accuracy: 0.8610\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.5499 - accuracy: 0.8813 - val_loss: 0.5606 - val_accuracy: 0.8795\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.5554 - accuracy: 0.8794 - val_loss: 0.5170 - val_accuracy: 0.8865\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.5205 - accuracy: 0.8935 - val_loss: 0.5212 - val_accuracy: 0.8830\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.4865 - accuracy: 0.8999 - val_loss: 0.5129 - val_accuracy: 0.8900\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.4606 - accuracy: 0.9107 - val_loss: 0.4842 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.4418 - accuracy: 0.9121 - val_loss: 0.4790 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4300 - accuracy: 0.9161 - val_loss: 0.4808 - val_accuracy: 0.8935\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4288 - accuracy: 0.9160 - val_loss: 0.4846 - val_accuracy: 0.8935\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4279 - accuracy: 0.9141 - val_loss: 0.4793 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4220 - accuracy: 0.9161 - val_loss: 0.4905 - val_accuracy: 0.8935\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4189 - accuracy: 0.9186 - val_loss: 0.4914 - val_accuracy: 0.8945\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4214 - accuracy: 0.9164 - val_loss: 0.4950 - val_accuracy: 0.8920\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4144 - accuracy: 0.9193 - val_loss: 0.5001 - val_accuracy: 0.8895\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4158 - accuracy: 0.9158 - val_loss: 0.5015 - val_accuracy: 0.8895\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4165 - accuracy: 0.9181 - val_loss: 0.5023 - val_accuracy: 0.8915\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4148 - accuracy: 0.9178 - val_loss: 0.5133 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4190 - accuracy: 0.9141 - val_loss: 0.5298 - val_accuracy: 0.8840\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.4528 - accuracy: 0.9081 - val_loss: 0.6248 - val_accuracy: 0.8480\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.4840 - accuracy: 0.8984 - val_loss: 0.6420 - val_accuracy: 0.8500\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.5240 - accuracy: 0.8906 - val_loss: 0.6462 - val_accuracy: 0.8510\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4881 - accuracy: 0.8995 - val_loss: 0.6083 - val_accuracy: 0.8625\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4518 - accuracy: 0.9114 - val_loss: 0.5846 - val_accuracy: 0.8565\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4238 - accuracy: 0.9168 - val_loss: 0.5619 - val_accuracy: 0.8730\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4179 - accuracy: 0.9174 - val_loss: 0.5595 - val_accuracy: 0.8690\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4095 - accuracy: 0.9180 - val_loss: 0.5584 - val_accuracy: 0.8720\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4074 - accuracy: 0.9210 - val_loss: 0.5732 - val_accuracy: 0.8700\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4091 - accuracy: 0.9174 - val_loss: 0.5728 - val_accuracy: 0.8700\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4056 - accuracy: 0.9204 - val_loss: 0.5942 - val_accuracy: 0.8635\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 0.4075 - accuracy: 0.9204 - val_loss: 0.5764 - val_accuracy: 0.8680\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 3s 345us/step - loss: 0.4021 - accuracy: 0.9191 - val_loss: 0.5823 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 3s 346us/step - loss: 0.5111 - accuracy: 0.8926 - val_loss: 0.5027 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 3s 350us/step - loss: 0.5616 - accuracy: 0.8751 - val_loss: 0.5010 - val_accuracy: 0.8890\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 3s 331us/step - loss: 0.5077 - accuracy: 0.8946 - val_loss: 0.4772 - val_accuracy: 0.9000\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4899 - accuracy: 0.9009 - val_loss: 0.4921 - val_accuracy: 0.8895\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4597 - accuracy: 0.9105 - val_loss: 0.4543 - val_accuracy: 0.8985\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4314 - accuracy: 0.9165 - val_loss: 0.4408 - val_accuracy: 0.9030\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90950\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4236 - accuracy: 0.9160 - val_loss: 0.4386 - val_accuracy: 0.9035\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90950\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.4168 - accuracy: 0.9180 - val_loss: 0.4447 - val_accuracy: 0.9020\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90950\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4167 - accuracy: 0.9160 - val_loss: 0.4445 - val_accuracy: 0.9010\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90950\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4147 - accuracy: 0.9159 - val_loss: 0.4476 - val_accuracy: 0.9035\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90950\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4114 - accuracy: 0.9195 - val_loss: 0.4586 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90950\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.4107 - accuracy: 0.9161 - val_loss: 0.4592 - val_accuracy: 0.8995\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90950\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4086 - accuracy: 0.9179 - val_loss: 0.4623 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90950\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 0.4129 - accuracy: 0.9153 - val_loss: 0.4717 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90950\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4121 - accuracy: 0.9180 - val_loss: 0.4799 - val_accuracy: 0.8945\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90950\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4100 - accuracy: 0.9156 - val_loss: 0.4756 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90950\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4109 - accuracy: 0.9181 - val_loss: 0.4812 - val_accuracy: 0.8915\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90950\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4209 - accuracy: 0.9159 - val_loss: 0.5229 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90950\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4435 - accuracy: 0.9081 - val_loss: 0.5866 - val_accuracy: 0.8520\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90950\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.5207 - accuracy: 0.8884 - val_loss: 0.7084 - val_accuracy: 0.8130\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90950\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.5525 - accuracy: 0.8776 - val_loss: 0.6267 - val_accuracy: 0.8380\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90950\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4853 - accuracy: 0.9013 - val_loss: 0.6086 - val_accuracy: 0.8505\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90950\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 2s 303us/step - loss: 0.4452 - accuracy: 0.9133 - val_loss: 0.5970 - val_accuracy: 0.8550\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90950\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4303 - accuracy: 0.9141 - val_loss: 0.5505 - val_accuracy: 0.8720\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90950\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4140 - accuracy: 0.9164 - val_loss: 0.5467 - val_accuracy: 0.8740\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90950\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4068 - accuracy: 0.9191 - val_loss: 0.5440 - val_accuracy: 0.8775\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90950\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 3s 318us/step - loss: 0.4020 - accuracy: 0.9209 - val_loss: 0.5441 - val_accuracy: 0.8760\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90950\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 3s 327us/step - loss: 0.3991 - accuracy: 0.9216 - val_loss: 0.5454 - val_accuracy: 0.8700\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90950\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.3966 - accuracy: 0.9224 - val_loss: 0.5448 - val_accuracy: 0.8800\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90950\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4010 - accuracy: 0.9184 - val_loss: 0.5585 - val_accuracy: 0.8720\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90950\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/30\n",
            "8000/8000 [==============================] - 2s 304us/step - loss: 0.4958 - accuracy: 0.8903 - val_loss: 0.4756 - val_accuracy: 0.9010\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.90950\n",
            "Epoch 2/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.5307 - accuracy: 0.8825 - val_loss: 0.4865 - val_accuracy: 0.8960\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.90950\n",
            "Epoch 3/30\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.4799 - accuracy: 0.8980 - val_loss: 0.4675 - val_accuracy: 0.9030\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.90950\n",
            "Epoch 4/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.4652 - accuracy: 0.9050 - val_loss: 0.4585 - val_accuracy: 0.9060\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90950\n",
            "Epoch 5/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.4340 - accuracy: 0.9106 - val_loss: 0.4491 - val_accuracy: 0.9040\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90950\n",
            "Epoch 6/30\n",
            "8000/8000 [==============================] - 2s 310us/step - loss: 0.4179 - accuracy: 0.9134 - val_loss: 0.4406 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90950 to 0.91000, saving model to /content/drive/My Drive/models/RNN.h5\n",
            "Epoch 7/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4069 - accuracy: 0.9180 - val_loss: 0.4395 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.91000 to 0.91100, saving model to /content/drive/My Drive/models/RNN.h5\n",
            "Epoch 8/30\n",
            "8000/8000 [==============================] - 3s 320us/step - loss: 0.4028 - accuracy: 0.9193 - val_loss: 0.4483 - val_accuracy: 0.9065\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.91100\n",
            "Epoch 9/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.4015 - accuracy: 0.9175 - val_loss: 0.4467 - val_accuracy: 0.9035\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.91100\n",
            "Epoch 10/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.3976 - accuracy: 0.9175 - val_loss: 0.4451 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.91100\n",
            "Epoch 11/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.3979 - accuracy: 0.9186 - val_loss: 0.4553 - val_accuracy: 0.9000\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.91100\n",
            "Epoch 12/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.3965 - accuracy: 0.9158 - val_loss: 0.4484 - val_accuracy: 0.9035\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.91100\n",
            "Epoch 13/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.3959 - accuracy: 0.9171 - val_loss: 0.4554 - val_accuracy: 0.9025\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.91100\n",
            "Epoch 14/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.3952 - accuracy: 0.9178 - val_loss: 0.4658 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.91100\n",
            "Epoch 15/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.3951 - accuracy: 0.9190 - val_loss: 0.4640 - val_accuracy: 0.9005\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.91100\n",
            "Epoch 16/30\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.4217 - accuracy: 0.9116 - val_loss: 0.5365 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.91100\n",
            "Epoch 17/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4825 - accuracy: 0.8940 - val_loss: 0.6136 - val_accuracy: 0.8540\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.91100\n",
            "Epoch 18/30\n",
            "8000/8000 [==============================] - 2s 305us/step - loss: 0.4937 - accuracy: 0.8914 - val_loss: 0.5399 - val_accuracy: 0.8765\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.91100\n",
            "Epoch 19/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4426 - accuracy: 0.9064 - val_loss: 0.5375 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.91100\n",
            "Epoch 20/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4188 - accuracy: 0.9151 - val_loss: 0.5078 - val_accuracy: 0.8880\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91100\n",
            "Epoch 21/30\n",
            "8000/8000 [==============================] - 3s 318us/step - loss: 0.4199 - accuracy: 0.9137 - val_loss: 0.5059 - val_accuracy: 0.8875\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.91100\n",
            "Epoch 22/30\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.3993 - accuracy: 0.9186 - val_loss: 0.5121 - val_accuracy: 0.8865\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.91100\n",
            "Epoch 23/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.3940 - accuracy: 0.9214 - val_loss: 0.5022 - val_accuracy: 0.8865\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91100\n",
            "Epoch 24/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.3908 - accuracy: 0.9178 - val_loss: 0.5060 - val_accuracy: 0.8845\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91100\n",
            "Epoch 25/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.3878 - accuracy: 0.9202 - val_loss: 0.5025 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91100\n",
            "Epoch 26/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.3881 - accuracy: 0.9214 - val_loss: 0.5083 - val_accuracy: 0.8875\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91100\n",
            "Epoch 27/30\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.3849 - accuracy: 0.9214 - val_loss: 0.5064 - val_accuracy: 0.8890\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91100\n",
            "Epoch 28/30\n",
            "8000/8000 [==============================] - 2s 309us/step - loss: 0.3862 - accuracy: 0.9206 - val_loss: 0.5154 - val_accuracy: 0.8840\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91100\n",
            "Epoch 29/30\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.3884 - accuracy: 0.9206 - val_loss: 0.5386 - val_accuracy: 0.8740\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91100\n",
            "Epoch 30/30\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.3973 - accuracy: 0.9172 - val_loss: 0.5456 - val_accuracy: 0.8760\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dxd0_4Judth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RNNmodel=RNNmodel.load_weights('/content/drive/My Drive/models/RNN.h5')"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8-5W7XTuaVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist=RNNmodel.fit(RNNx_train, RNNy_train, validation_data=(RNNx_test, RNNy_test), epochs=300, batch_size=64,callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuOfuuWDZuW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuracy')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upW2vW-k91lu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39936c78-7716-4598-c555-a73b00838f8f"
      },
      "source": [
        "scores = RNNmodel.evaluate(RNNx_test, RNNy_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 91.50%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0nHEDZOYxdL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "78d08659-314e-4275-fbf3-da7a19ba3a70"
      },
      "source": [
        "print(RNNmodel.predict(RNNx_test))# (size,10)의 RNNxtest를 넣으면 (size,13)이 나온다."
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.5431981e-04 3.3956766e-04 8.4123015e-04 ... 2.3465455e-03\n",
            "  8.0505013e-04 2.8437674e-03]\n",
            " [2.2640824e-04 4.1365623e-04 1.2001693e-03 ... 2.3826957e-04\n",
            "  8.7004900e-04 8.0549717e-04]\n",
            " [1.2156367e-04 2.8854609e-04 8.4158778e-04 ... 9.1424584e-04\n",
            "  2.1554232e-03 1.7278492e-03]\n",
            " ...\n",
            " [7.8558922e-05 1.5974045e-04 1.8977821e-03 ... 1.9258973e-01\n",
            "  1.4996231e-03 2.2495985e-03]\n",
            " [3.5855174e-04 1.7565489e-04 6.7970157e-04 ... 8.3813071e-04\n",
            "  6.3174069e-03 7.0127845e-04]\n",
            " [5.8162212e-04 1.8227100e-04 8.6811185e-04 ... 6.7040324e-04\n",
            "  1.2475550e-03 1.5582442e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CP7tkoSCIm2",
        "colab_type": "text"
      },
      "source": [
        "classifier : 24*24 bar input을 기반하여 Skill을 분류한다. 학습은 Multilabel Classifier로써 진행된다.\n",
        "\n",
        "updown_classifier : 24*24 bar input을 기반하여 다음 bar의 pitch change를 분류한다. 학습은 up, down, final, meanless 4가지 기반하여 진행되나 분류 자체는 up, down만 유의미하다.\n",
        "\n",
        "RNNmodel : bar skill sequence를 기반하여 다음 bar가 어떤 스킬을 가지는지를 분류한다. 학습은 k-fold기반의 RNN으로 진행된다\n",
        "\n",
        "G : GAN기반한 모델의 Generator이다. image_generator(G,갯수,encoded skill num)과 같은 형태로 사용하여 이미지를 생성할 수 있다. 원할한 생성을 위해 Matrix Image에 Bluring을 사용한다.\n",
        "\n",
        "이제 G가 생성한 Matrix이미지를 0과1로 이루어진 Matrix로 정리하고, Chord를 Match시켜준 뒤 MIDI로 Decoding하면 끝이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1Q__y8tCG-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "e1d5c942-e782-4f1f-d7e0-0df8d775e16d"
      },
      "source": [
        "#Model Loading Only\n",
        "RNNmodel = Sequential()\n",
        "RNNmodel.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "RNNmodel.add(LSTM(100, return_sequences=True,\n",
        "               input_shape=(timesteps, 100)))  # returns a sequence of vectors of dimension 32\n",
        "RNNmodel.add(LSTM(100))  # return a single vector of dimension 32\n",
        "RNNmodel.add(Dropout(0.2))\n",
        "RNNmodel.add(Dense(13, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n",
        "RNNmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "RNNmodel.load_weights('/content/drive/My Drive/models/RNN.h5')\n",
        "\n",
        "G = generator_model()\n",
        "G.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "G.load_weights(\"/content/drive/My Drive/MARG/PPDDlist/GAN_result/generator.h5\")\n",
        "\n",
        "updown_classifier=make_classifier()\n",
        "updown_classifier.compile(loss=keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=False, label_smoothing=0.1, \n",
        "      name='categorical_crossentropy'\n",
        "  ), optimizer='adam', metrics=['accuracy'])\n",
        "updown_classifier.load_weights(\"/content/drive/My Drive/models/updown.h5\")\n",
        "\n",
        "classifier=make_model()\n",
        "classifier.compile(loss=keras.losses.BinaryCrossentropy(\n",
        "      from_logits=False, label_smoothing=0.1, \n",
        "      name='binary_crossentropy'\n",
        "  ), optimizer='adam', metrics=['accuracy',recall,precision,f1score])\n",
        "classifier.load_weights(\"/content/drive/My Drive/models/deeperppddbest.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-d52597458d61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Model Loading Only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mRNNmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mRNNmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_vecor_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_review_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m RNNmodel.add(LSTM(100, return_sequences=True,\n\u001b[1;32m      5\u001b[0m                input_shape=(timesteps, 100)))  # returns a sequence of vectors of dimension 32\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Embedding' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6htKIS4Lcb7h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "c66641f7-0143-4f7b-d3eb-fa5964485457"
      },
      "source": [
        "import cv2\n",
        "H=generate_images(G,4,4)[0]\n",
        "def matrix_cleaner(matrix):\n",
        "  #matrix should be size of 24*24\n",
        "  #make matrix's value of [0,1]  \n",
        "  matrix=np.matrix(matrix)\n",
        "  maximum_value=matrix.max()\n",
        "  minimum_value=matrix.min()\n",
        "  matrix=(matrix-minimum_value)/(maximum_value-minimum_value)\n",
        "  flat=matrix.flatten()\n",
        "  flat.sort()\n",
        "  flat=flat.reshape((576,1))\n",
        "  hundred_val=flat[-70]\n",
        "  matrix=np.where(matrix<hundred_val,0,matrix)\n",
        "  matrix=np.where(matrix<0.3,0,matrix)\n",
        "  return matrix\n",
        "plt.imshow(matrix_cleaner(H))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0252f231d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAALlElEQVR4nO3da4hc9RnH8d/PuMniqm1W4zbV2EQbWtNCY93GS21RvBCFNvpG9IUNrbAWlGoRJApFoZWKoBaLWFaMhuIFQW3yIrTKIljaNBg1aLzVECJmXXe12zZpwEuSpy/2BDZxd89k5swlPt8PhJ0588+ch9Fv5nZ2xhEhAF98R7R7AACtQexAEsQOJEHsQBLEDiRxZCt3Nttzols9rdwlkMrH2q1P4xNPdVlLY+9Wj870Ba3cJZDKxhia9rKGHsbbXm77bdtbba9q5LoANFfdsdueJel+SZdIWiLpKttLqhoMQLUauWdfJmlrRGyLiE8lPSFpRTVjAahaI7GfKOm9Sed3FNsOYHvA9ibbmz7TJw3sDkAjmv7WW0QMRkR/RPR3aU6zdwdgGo3EPixpwaTzJxXbAHSgRmJ/UdJi24tsz5Z0paR11YwFoGp1v88eEXtsXy/pL5JmSVodEa9XNhmASjV0UE1ErJe0vqJZADQRx8YDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQxJGN/GXb2yXtkrRX0p6I6K9iKADVayj2wvkR8VEF1wOgiXgYDyTRaOwh6VnbL9kemGqB7QHbm2xv+kyfNLg7APVq9GH8uRExbPsESc/ZfisiXpi8ICIGJQ1K0rHujQb3B6BODd2zR8Rw8XNM0jOSllUxFIDq1R277R7bx+w/LeliSVuqGgxAtRp5GN8n6Rnb+6/nsYj4cyVTVWD8p2eXrul9eEMLJgE6Q92xR8Q2Sd+pcBYATcRbb0ASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kUcXvs3ckjo4DDsQ9O5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0kQO5AEsQNJEDuQRGnstlfbHrO9ZdK2XtvP2X6n+Dm3uWMCaFQt9+yPSFp+0LZVkoYiYrGkoeI8gA5WGntEvCBp/KDNKyStKU6vkXRZxXMBqFi9X9ncFxEjxekPJPVNt9D2gKQBSerWUXXuDkCjGn6BLiJCUsxw+WBE9EdEf5fmNLo7AHWqN/ZR2/Mlqfg5Vt1IAJqh3tjXSVpZnF4paW014wBollreentc0gZJ37C9w/Y1ku6UdJHtdyRdWJwH0MFKX6CLiKumueiCimcB0EQcQQckQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRR70dJI4ntvzm7dE3MKr+eo0Zcuqbvvr/XMhLqxD07kASxA0kQO5AEsQNJEDuQBLEDSRA7kASxA0lwUE1i2+4qP2DmW2duK10zr/t/pWs2DC8sXfPuceeUrvnabRx4Uy/u2YEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IwhHRsp11n7QgFlz3yxnXRA3//Cy6ZUNFE6HM9l+XH3jz6XF7S9d09X5cxTjaM3pU6ZrFN/yjkn0djjbGkHbG+JQfC8Q9O5BEaey2V9ses71l0rbbbQ/b3lz8ubS5YwJoVC337I9IWj7F9nsjYmnxZ321YwGoWmnsEfGCpPEWzAKgiRp5zn697VeLh/lzp1tke8D2Jtub9u7e3cDuADSi3tgfkHSqpKWSRiTdPd3CiBiMiP6I6J/V01Pn7gA0qq7YI2I0IvZGxD5JD0paVu1YAKpWV+y25086e7mkLdOtBdAZSj+pxvbjks6TdLztHZJuk3Se7aWSQtJ2SdfWsjN371XXaTtnXHPEEftKr2f8Z+UHevSu5sCbKiz8VTW3445byj+F5owfl99nnPzNf5euWXvzD0rXfPWufJ94Uxp7RFw1xeaHmjALgCbiCDogCWIHkiB2IAliB5IgdiAJYgeSIHYgidZ+/dPuWYoXvzTjkiNq+F2Z3tX5Dog43J302/L/Zn874azSNXPOea10zZ7v7Spd8+HPyw/MkqR5f6jmoKL//KR8f3u6y6/n+MH65+GeHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkWvr1T8e6N870BS3bH754Rn9R/ok3sy76qHTNoi/X9unoL729sHzRnvL7zB/1v1K65ts9w6Vr7n38shkvf3fwHn38/nt8/ROQGbEDSRA7kASxA0kQO5AEsQNJEDuQBLEDSRA7kARH0OEL558Pn1G6ZuOF99V0Xf/aO+XBaAe44/1LS9eMf3JU6ZqRnceWrjlhxVszXr4xhrQzxjmCDsiM2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkWvtdb0ALdI3MLl2ze19tB5OdNrundE0tB8y8//TC0jV9v2/udxiW3rPbXmD7edtv2H7d9g3F9l7bz9l+p/g5t6mTAmhILQ/j90i6KSKWSDpL0nW2l0haJWkoIhZLGirOA+hQpbFHxEhEvFyc3iXpTUknSlohaU2xbI2kmT/2EkBbHdJzdtsLJZ0uaaOkvogYKS76QFLfNH9nQNKAJHWr/LkNgOao+dV420dLekrSjRGxc/JlMfGrc1O+4hERgxHRHxH9XZrT0LAA6ldT7La7NBH6oxHxdLF51Pb84vL5ksaaMyKAKtTyarwlPSTpzYi4Z9JF6yStLE6vlLS2+vEAVKWW5+zfl3S1pNdsby623SrpTklP2r5G0ruSrmjOiACqwCfVIKWtfzy9pnVfmfff0jVjr0752vQBTrl5Q037axSfVAOA2IEsiB1IgtiBJIgdSILYgSSIHUiC2IEk+KQapPT1q1+p7LqO1rbKrquZuGcHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSaOnXP9n+UBPfC7ff8ZI+atkA1Tkc52bm1mnn3F+LiHlTXdDS2D+3c3tTRPS3bYA6HY5zM3PrdOrcPIwHkiB2IIl2xz7Y5v3X63Ccm5lbpyPnbutzdgCt0+57dgAtQuxAEm2L3fZy22/b3mp7VbvmOBS2t9t+zfZm25vaPc90bK+2PWZ7y6Rtvbafs/1O8XNuO2c82DQz3257uLi9N9u+tJ0zHsz2AtvP237D9uu2byi2d+Rt3ZbYbc+SdL+kSyQtkXSV7SXtmKUO50fE0k58H3WSRyQtP2jbKklDEbFY0lBxvpM8os/PLEn3Frf30ohY3+KZyuyRdFNELJF0lqTriv+PO/K2btc9+zJJWyNiW0R8KukJSSvaNMsXTkS8IGn8oM0rJK0pTq+RdFlLhyoxzcwdLSJGIuLl4vQuSW9KOlEdelu3K/YTJb036fyOYlunC0nP2n7J9kC7hzlEfRExUpz+QFJfO4c5BNfbfrV4mN8RD4enYnuhpNMlbVSH3ta8QHdozo2I72ri6cd1tn/Y7oHqERPvtx4O77k+IOlUSUsljUi6u73jTM320ZKeknRjROycfFkn3dbtin1Y0oJJ508qtnW0iBgufo5JekYTT0cOF6O250tS8XOszfOUiojRiNgbEfskPagOvL1td2ki9Ecj4ulic0fe1u2K/UVJi20vsj1b0pWS1rVplprY7rF9zP7Tki6WtGXmv9VR1klaWZxeKWltG2epyf5gCperw25v25b0kKQ3I+KeSRd15G3dtiPoirdRfidplqTVEXFHWwapke1TNHFvLklHSnqsU2e2/bik8zTxq5ajkm6T9CdJT0o6WRO/ZnxFRHTMC2LTzHyeJh7Ch6Ttkq6d9Fy47WyfK+mvkl6TtK/YfKsmnrd33G3N4bJAErxAByRB7EASxA4kQexAEsQOJEHsQBLEDiTxf8pVy78/evMeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRSy54CpFDz0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "c17db74b-b28e-4a02-962f-49aa82d1f847"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage.filters import maximum_filter\n",
        "from scipy.ndimage.morphology import generate_binary_structure, binary_erosion\n",
        "import matplotlib.pyplot as pp\n",
        "\n",
        "#getting a list of images\n",
        "\n",
        "paws = [matrix_cleaner(H)]\n",
        "\n",
        "\n",
        "def detect_peaks(image):\n",
        "    \"\"\"\n",
        "    Takes an image and detect the peaks usingthe local maximum filter.\n",
        "    Returns a boolean mask of the peaks (i.e. 1 when\n",
        "    the pixel's value is the neighborhood maximum, 0 otherwise)\n",
        "    \"\"\"\n",
        "\n",
        "    # define an 8-connected neighborhood\n",
        "    neighborhood = generate_binary_structure(2,2)\n",
        "\n",
        "    #apply the local maximum filter; all pixel of maximal value \n",
        "    #in their neighborhood are set to 1\n",
        "    local_max = maximum_filter(image, footprint=neighborhood)==image\n",
        "    #local_max is a mask that contains the peaks we are \n",
        "    #looking for, but also the background.\n",
        "    #In order to isolate the peaks we must remove the background from the mask.\n",
        "\n",
        "    #we create the mask of the background\n",
        "    background = (image==0)\n",
        "\n",
        "    #a little technicality: we must erode the background in order to \n",
        "    #successfully subtract it form local_max, otherwise a line will \n",
        "    #appear along the background border (artifact of the local maximum filter)\n",
        "    eroded_background = binary_erosion(background, structure=neighborhood, border_value=1)\n",
        "\n",
        "    #we obtain the final mask, containing only peaks, \n",
        "    #by removing the background from the local_max mask (xor operation)\n",
        "    detected_peaks = local_max ^ eroded_background\n",
        "    detected_peaks=np.where(detected_peaks==True,1,0)\n",
        "\n",
        "    return detected_peaks\n",
        "\n",
        "\n",
        "#applying the detection and plotting results\n",
        "for i, paw in enumerate(paws):\n",
        "    detected_peaks = detect_peaks(paw)\n",
        "    pp.subplot(1,2,(2*i+1))\n",
        "    pp.imshow(paw)\n",
        "    pp.subplot(1,2,(2*i+2) )\n",
        "    pp.imshow(detected_peaks)\n",
        "\n",
        "pp.show()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPVdL9UXO-0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "cb4157b2-06db-423c-8f51-499b42bbf858"
      },
      "source": [
        "def left_shifting(matrix,detected_peak):\n",
        "  #input은 detected_peak가 들어와야한다.\n",
        "  dots=[]\n",
        "  for j in range(len(detected_peak)):\n",
        "    pos=[]\n",
        "    for i in range(len(detected_peak[0])):#for 문의 순서를 이렇게 지정해야 Handle이 가능하다.\n",
        "      if(detected_peak[i][j]==1):\n",
        "        if(j%2!=0 and j%3!=0):\n",
        "          j=j-1\n",
        "        if(len(pos)==0):\n",
        "          pos=[i,j]\n",
        "        else:\n",
        "          if (matrix[pos[0]][pos[1]]<matrix[i][j]):\n",
        "            pos=[i,j]\n",
        "    if(len(pos)!=0):\n",
        "      dots.append(pos)\n",
        "  dots_with_length=[]\n",
        "  starting_points=[]\n",
        "  durations=[]\n",
        "  velocities=[]\n",
        "  for position in dots:\n",
        "    velocities.append(matrix[position[0]][position[1]])\n",
        "    length_val=0\n",
        "    while True:\n",
        "      if(length_val==0):\n",
        "        starting_points.append([position[0],position[1]])\n",
        "      if(position[1]+length_val>23):\n",
        "        break\n",
        "      elif(matrix[position[0]][position[1]+length_val]!=0):\n",
        "        dots_with_length.append([position[0],position[1]+length_val])\n",
        "      else:\n",
        "        break\n",
        "      length_val+=1\n",
        "    durations.append(length_val)\n",
        "  result=np.zeros_like(matrix)\n",
        "  for position in dots_with_length:\n",
        "    result[position[0]][position[1]]=1\n",
        "  return result,np.array(starting_points),np.array(durations),np.array(velocities)\n",
        "\n",
        "plt.imshow(left_shifting(matrix_cleaner(H),detect_peaks(matrix_cleaner(H)))[0])\n",
        "print(left_shifting(matrix_cleaner(H),detected_peaks)[1],left_shifting(matrix_cleaner(H),detected_peaks)[2],left_shifting(matrix_cleaner(H),detected_peaks)[3])"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[16  0]\n",
            " [ 6  3]\n",
            " [14 12]\n",
            " [16 16]\n",
            " [20 18]\n",
            " [18 22]] [3 1 4 2 4 2] [0.6297505  0.31638747 0.7212378  0.8000499  0.94594955 0.8467015 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hmNRrgvCTpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C_chord=[1,0,1,0,1,1,0,1,0,1,0,1]\n",
        "chords=['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']\n",
        "def chord_matching(shifted_matrix,chord,last_pitch,direction):\n",
        "  #input은 left_shifting의 output을 그대로 넣고, chord(C,C#,D,D#....,B), 마지막 리듬의 pitch를 숫자로, 그리고 진행 방향을 direction(1,0)으로 받는다.\n",
        "  #output은 MIDI기반의 decoding이 되도록 정보 기반으로, 그리고 last_pitch까지 넣어준다.\n",
        "  #또한 출력 그림과 상관없이, 그냥 Note Pitch를 그냥 Matrix에 넣어준다.\n",
        "  before_matrix=shifted_matrix[0]\n",
        "  if chord not in chords:\n",
        "    now_chord='C'\n",
        "  now_chord=chord\n",
        "  chord_diff=chords.index(now_chord)\n",
        "  now_chord_list=np.roll(C_chord,chord_diff)#[1,0,1,0,1...] 이런 set인데 여기에 맞추면 함수에 넣은 chord에 맞게 된다.\n",
        "  #나중에 그냥 chord뿐만 아니라 화음의 종류까지 고민하게 된다면 이걸 잘 쓰면 된다.\n",
        "  pitch_set=[]\n",
        "  for pitchs in shifted_matrix[1]:\n",
        "    pitch_set.append(23-pitchs[0])#위 그림 기준 7,17,9,7,3,5 이렇게 들감\n",
        "  pitch_set=pitch_set-pitch_set[0]#이러면 0, 10, -1, 0 ,-4, -2, 이렇게 들간다. 상대적인 위치를 다루는게 쉽다.\n",
        "  if (direction==1):\n",
        "    possible_set=[]\n",
        "    for i in range(12):\n",
        "      possible_set.append(pitch_set+i+last_pitch)\n",
        "  else:\n",
        "    possible_set=[]\n",
        "    for i in range(12):\n",
        "      possible_set.append(pitch_set-i+last_pitch)\n",
        "  possible_set_score=[]\n",
        "  for sets in possible_set:\n",
        "    score=0\n",
        "    for pitchs in sets:\n",
        "      score+=now_chord_list[pitchs%12]\n",
        "    possible_set_score.append(score)\n",
        "  final_set=possible_set[np.argmax(possible_set_score)]\n",
        "  return_val=[]\n",
        "  for i,sets in enumerate(final_set):\n",
        "    final=[]\n",
        "    final.append(sets)\n",
        "    final.append(shifted_matrix[1][i][1])\n",
        "    final.append(shifted_matrix[2][i])\n",
        "    final.append(shifted_matrix[3][i])\n",
        "    return_val.append(final)\n",
        "  return return_val, sets\n"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7chdoz0aQzCm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "11fe87aa-db2c-4f6f-ed7e-cfcf362becdc"
      },
      "source": [
        "shifted=left_shifting(matrix_cleaner(H),detect_peaks(matrix_cleaner(H)))\n",
        "print(chord_matching(shifted,'C',48,1))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([[57, 0, 3, 0.6297505], [67, 3, 1, 0.31638747], [59, 12, 4, 0.7212378], [57, 16, 2, 0.8000499], [53, 18, 4, 0.94594955], [55, 22, 2, 0.8467015]], 55)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf-UJL5SarLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMvW8smJXDen",
        "colab_type": "text"
      },
      "source": [
        "H를 생성한다. Concatenate된 matrix를 사용해도 될 것이라고 보인다.\n",
        "\n",
        "이를 matrix_cleaner함수에 넣은 뒤, return 값을 받고\n",
        "\n",
        "이를 paw=[matrix_cleaner(H)]와 같은 형태로 넣는다.\n",
        "\n",
        "그 후 detect_peaks(paw)를 사용해주고,\n",
        "\n",
        "마무리로 left_shifting(H,detected_peaks)와 같은 형태로 사용하면\n",
        "일단 나온다. "
      ]
    }
  ]
}